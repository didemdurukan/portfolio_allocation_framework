{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/doganparlak/Desktop/Master_2.2/Master_Project/uniFi_github/uniFi/AgentLayer', '/usr/local/Cellar/python@3.8/3.8.8_1/Frameworks/Python.framework/Versions/3.8/lib/python38.zip', '/usr/local/Cellar/python@3.8/3.8.8_1/Frameworks/Python.framework/Versions/3.8/lib/python3.8', '/usr/local/Cellar/python@3.8/3.8.8_1/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload', '', '/usr/local/lib/python3.8/site-packages', '/usr/local/lib/python3.8/site-packages/selenium-3.141.0-py3.8.egg', '/usr/local/lib/python3.8/site-packages/urllib3-1.26.4-py3.8.egg', '/Users/doganparlak/Desktop/Master_2.2/Master_Project/uniFi_github/uniFi']\n"
     ]
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import A2C as sb_A2C\n",
    "from stable_baselines3 import PPO as sb_PPO\n",
    "from stable_baselines3 import DDPG as sb_DDPG\n",
    "from stable_baselines3 import TD3 as sb_TD3\n",
    "from stable_baselines3.common.noise import ActionNoise\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "from stable_baselines3.common.type_aliases import Schedule\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch as th\n",
    "import yaml\n",
    "import gym\n",
    "from datetime import datetime\n",
    "from sys import path\n",
    "from os.path import dirname as dir\n",
    "from typing import Any, Dict, Optional, Tuple, Type, Union\n",
    "path.append(dir(path[0]))\n",
    "print(path)\n",
    "#__package__ = \"examples\"\n",
    "\n",
    "from FinancialDataLayer.DataCollection.DataDownloader import DataDownloader\n",
    "from FinancialDataLayer.DataProcessing.DefaultFeatureEngineer import DefaultFeatureEngineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(gym.Env, ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def step(self, action):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def render(self, mode=\"human\"):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_env(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax_normalization(actions):\n",
    "        numerator = np.exp(actions)\n",
    "        denominator = np.sum(np.exp(actions))\n",
    "        softmax_output = numerator / denominator\n",
    "        return softmax_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_model():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save_model():\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def load_model():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConventionalAgent(Agent, ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_model():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save_model():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_model():\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _return_predict():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _weight_optimization():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLAgent(Agent, ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_model():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict():\n",
    "        \n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save_model():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_model():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortfolioEnv(Environment):\n",
    "\n",
    "    def __init__(self,\n",
    "                 df: pd.DataFrame,  # input data\n",
    "                 stock_dim: int,  # number of unique securities in the investment universe\n",
    "                 hmax: float,  # maximum number of shares to trade\n",
    "                 initial_amount: float,  # initial cash value\n",
    "                 transaction_cost_pct: float,  # transaction cost percentage per trade\n",
    "                 reward_scaling: float,  # scaling factor for reward as training progresses\n",
    "                 state_space: int,  # the dimension of input features (state space)\n",
    "                 action_space: int,  # number of actions, which is equal to portfolio dimension\n",
    "                 tech_indicator_list: list,  # a list of technical indicator names\n",
    "                 turbulence_threshold=None,  # a threshold to control risk aversion\n",
    "                 lookback=252,  #\n",
    "                 day=0):  # an increment number to control date\n",
    "\n",
    "        self.df = df\n",
    "        self.day = day\n",
    "        self.lookback = lookback\n",
    "        self.stock_dim = stock_dim\n",
    "        self.hmax = hmax\n",
    "        self.initial_amount = initial_amount\n",
    "        self.transaction_cost_pct = transaction_cost_pct\n",
    "        self.reward_scaling = reward_scaling\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.tech_indicator_list = tech_indicator_list\n",
    "\n",
    "        # action_space normalization and shape is self.stock_dim\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(self.action_space,))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf,\n",
    "                                            shape=(self.state_space + len(self.tech_indicator_list), self.state_space))\n",
    "        \n",
    "\n",
    "        # load data from a pandas dataframe\n",
    "        \n",
    "        ##FINRL APPROACH\n",
    "        #self.df.set_index(\"date\", drop = True, inplace=True)\n",
    "        \n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        self.covs = self.data['cov_list'].values[0]\n",
    "        self.state =  np.append(np.array(self.covs), [self.data[tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)\n",
    "        self.terminal = False\n",
    "        #self.turbulence_threshold = turbulence_threshold\n",
    "        # initalize state: initial portfolio return + individual stock return + individual weights\n",
    "        self.portfolio_value = self.initial_amount\n",
    "\n",
    "        # memorize portfolio value each step\n",
    "        self.asset_memory = [self.initial_amount]\n",
    "        # memorize portfolio return each step\n",
    "        self.portfolio_return_memory = [0]\n",
    "        self.actions_memory = [[1 / self.stock_dim] * self.stock_dim]\n",
    "        self.date_memory=[self.data.date.unique()[0]]\n",
    "\n",
    "    def reset(self):\n",
    "        self.asset_memory = [self.initial_amount]\n",
    "        self.day = 0\n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        # load states\n",
    "        self.covs = self.data['cov_list'].values[0]\n",
    "        self.state =  np.append(np.array(self.covs), [self.data[tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)\n",
    "                \n",
    "        self.portfolio_value = self.initial_amount\n",
    "        #self.cost = 0\n",
    "        #self.trades = 0\n",
    "        self.terminal = False \n",
    "        self.portfolio_return_memory = [0]\n",
    "        self.actions_memory=[[1/self.stock_dim] * self.stock_dim]\n",
    "        self.date_memory=[self.data.date.unique()[0]] \n",
    "        return self.state\n",
    "\n",
    "    def step(self, actions):\n",
    "        self.terminal = self.day >= len(self.df.index.unique()) - 1\n",
    "        if self.terminal:\n",
    "            df = pd.DataFrame(self.portfolio_return_memory)\n",
    "            print(df)\n",
    "            df.columns = ['daily_return']\n",
    "            plt.plot(df.daily_return.cumsum(), 'r')\n",
    "           # plt.savefig('results/cumulative_reward.png')\n",
    "            plt.close()\n",
    "\n",
    "            plt.plot(self.portfolio_return_memory, 'r')\n",
    "           #plt.savefig('results/rewards.png')\n",
    "            plt.close()\n",
    "\n",
    "            print(\"=================================\")\n",
    "            print(\"begin_total_asset:{}\".format(self.asset_memory[0]))\n",
    "            print(\"end_total_asset:{}\".format(self.portfolio_value))\n",
    "\n",
    "            df_daily_return = pd.DataFrame(self.portfolio_return_memory)\n",
    "            df_daily_return.columns = ['daily_return']\n",
    "            if df_daily_return['daily_return'].std() != 0:\n",
    "                sharpe = (252 ** 0.5) * df_daily_return['daily_return'].mean() / \\\n",
    "                         df_daily_return['daily_return'].std()\n",
    "                print(\"Sharpe: \", sharpe)\n",
    "            print(\"=================================\")\n",
    "\n",
    "            return self.state, self.reward, self.terminal, {}\n",
    "\n",
    "        else:\n",
    "            weights = Environment.softmax_normalization(actions)\n",
    "            self.actions_memory.append(weights)\n",
    "            last_day_memory = self.data\n",
    "\n",
    "            # load next state\n",
    "            self.day += 1\n",
    "            self.data = self.df.loc[self.day, :]\n",
    "            self.covs = self.data['cov_list'].values[0]\n",
    "            self.state =  np.append(np.array(self.covs), [self.data[tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)         \n",
    "            portfolio_return = sum(((self.data.close.values / last_day_memory.close.values) - 1) * weights)\n",
    "            log_portfolio_return = np.log(sum((self.data.close.values / last_day_memory.close.values) * weights))\n",
    "            # update portfolio value\n",
    "            new_portfolio_value = self.portfolio_value * (1 + portfolio_return)\n",
    "            self.portfolio_value = new_portfolio_value\n",
    "\n",
    "            # save into memory\n",
    "            self.portfolio_return_memory.append(portfolio_return)\n",
    "            self.date_memory.append(self.data[\"date\"].unique()[0])\n",
    "            self.asset_memory.append(new_portfolio_value)\n",
    "\n",
    "            # the reward is the new portfolio value or end portfolo value\n",
    "            self.reward = new_portfolio_value\n",
    "\n",
    "        return self.state, self.reward, self.terminal, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        return self.state\n",
    "\n",
    "    def save_asset_memory(self):\n",
    "        date_list = self.date_memory\n",
    "        portfolio_return = self.portfolio_return_memory\n",
    "        # print(len(date_list))\n",
    "        # print(len(asset_list))\n",
    "        df_account_value = pd.DataFrame({'date': date_list, 'daily_return': portfolio_return})\n",
    "        return df_account_value\n",
    "\n",
    "    def save_action_memory(self):\n",
    "        # date and close price length must match actions length\n",
    "        date_list = self.date_memory\n",
    "        df_date = pd.DataFrame(date_list)\n",
    "        df_date.columns = ['date']\n",
    "\n",
    "        action_list = self.actions_memory\n",
    "        df_actions = pd.DataFrame(action_list)\n",
    "        df_actions.columns = self.data.tic.values\n",
    "        df_actions.index = df_date.date\n",
    "        # df_actions = pd.DataFrame({'date':date_list,'actions':action_list})\n",
    "        return df_actions\n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def get_env(self):\n",
    "        e = DummyVecEnv([lambda: self])\n",
    "        obs = e.reset()\n",
    "        return e, obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C(RLAgent):\n",
    "\n",
    "    def __init__(self,\n",
    "                 policy: \"MlpPolicy\",\n",
    "                 env: None,\n",
    "                 learning_rate: float = 7e-4,\n",
    "                 n_steps: int = 5,\n",
    "                 gamma: float = 0.99,\n",
    "                 gae_lambda: float = 1.0,\n",
    "                 ent_coef: float = 0.0,\n",
    "                 vf_coef: float = 0.5,\n",
    "                 max_grad_norm: float = 0.5,\n",
    "                 rms_prop_eps: float = 1e-5,\n",
    "                 use_rms_prop: bool = True,\n",
    "                 use_sde: bool = False,\n",
    "                 sde_sample_freq: int = -1,\n",
    "                 normalize_advantage: bool = False,\n",
    "                 tensorboard_log: Optional[str] = None,\n",
    "                 create_eval_env: bool = False,\n",
    "                 policy_kwargs: Optional[Dict[str, Any]] = None,\n",
    "                 verbose: int = 0,\n",
    "                 seed: Optional[int] = None,\n",
    "                 device: Union[th.device, str] = \"auto\",\n",
    "                 _init_setup_model: bool = True):\n",
    "\n",
    "        self.env = env\n",
    "\n",
    "        self.model = sb_A2C(policy = policy,\n",
    "                            env=self.env,\n",
    "                            learning_rate = learning_rate,\n",
    "                            n_steps = n_steps,\n",
    "                            gamma = gamma,\n",
    "                            gae_lambda= gae_lambda,\n",
    "                            ent_coef = ent_coef,\n",
    "                            vf_coef = vf_coef,\n",
    "                            max_grad_norm = max_grad_norm,\n",
    "                            rms_prop_eps= rms_prop_eps,\n",
    "                            use_rms_prop= use_rms_prop,\n",
    "                            use_sde= use_sde,\n",
    "                            sde_sample_freq= sde_sample_freq,\n",
    "                            normalize_advantage= normalize_advantage,\n",
    "                            tensorboard_log=tensorboard_log,  \n",
    "                            create_eval_env= create_eval_env,\n",
    "                            policy_kwargs=policy_kwargs,\n",
    "                            verbose=verbose,\n",
    "                            seed=seed,\n",
    "                            device= device,\n",
    "                            _init_setup_model = _init_setup_model)\n",
    "\n",
    "    def train_model(self, **train_params):\n",
    "        self.model = self.model.learn(**train_params)\n",
    "        return self.model\n",
    "\n",
    "    def predict(self, environment, **test_params):\n",
    "\n",
    "        env_test, obs_test = environment.get_env()\n",
    "        \"\"\"make a prediction\"\"\"\n",
    "        account_memory = []\n",
    "        actions_memory = []\n",
    "\n",
    "        env_test.reset()\n",
    "        for i in range(len(environment.df.index.unique())):\n",
    "            action, _states = self.model.predict(obs_test, **test_params)\n",
    "            obs_test, rewards, dones, info = env_test.step(action)\n",
    "            if i == (len(environment.df.index.unique()) - 2):\n",
    "                account_memory = env_test.env_method(method_name=\"save_asset_memory\")\n",
    "                actions_memory = env_test.env_method(method_name=\"save_action_memory\")\n",
    "            if dones[0]:\n",
    "                print(\"hit end!\")\n",
    "                break\n",
    "\n",
    "        return account_memory[0], actions_memory[0]\n",
    "\n",
    "    def save_model(self, path):\n",
    "        self.model.save(path)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        self.model = self.model.load(path)\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO(RLAgent):\n",
    "    def __init__(self,\n",
    "                policy: \"MlpPolicy\",\n",
    "                env: None,\n",
    "                learning_rate:  3e-4,\n",
    "                n_steps: int = 2048,\n",
    "                batch_size: int = 64,\n",
    "                n_epochs: int = 10,\n",
    "                gamma: float = 0.99,\n",
    "                gae_lambda: float = 0.95,\n",
    "                clip_range: Union[float, Schedule] = 0.2,\n",
    "                clip_range_vf: Union[None, float, Schedule] = None,\n",
    "                normalize_advantage: bool = True,\n",
    "                ent_coef: float = 0.0,\n",
    "                vf_coef: float = 0.5,\n",
    "                max_grad_norm: float = 0.5,\n",
    "                use_sde: bool = False,\n",
    "                sde_sample_freq: int = -1,\n",
    "                target_kl: Optional[float] = None,\n",
    "                tensorboard_log: Optional[str] = None,\n",
    "                create_eval_env: bool = False,\n",
    "                policy_kwargs: Optional[Dict[str, Any]] = None,\n",
    "                verbose: int = 0,\n",
    "                seed: Optional[int] = None,\n",
    "                device: Union[th.device, str] = \"auto\",\n",
    "                _init_setup_model: bool = True):\n",
    "\n",
    "        self.env = env\n",
    "        self.model = sb_PPO(policy = policy,\n",
    "                            env=self.env,\n",
    "                            learning_rate = learning_rate,\n",
    "                            n_steps = n_steps,\n",
    "                            gamma = gamma,\n",
    "                            batch_size = batch_size,\n",
    "                            n_epochs = n_epochs,\n",
    "                            gae_lambda=gae_lambda,\n",
    "                            clip_range = clip_range,\n",
    "                            clip_range_vf = clip_range_vf,\n",
    "                            normalize_advantage=normalize_advantage,\n",
    "                            ent_coef=ent_coef,\n",
    "                            vf_coef=vf_coef,\n",
    "                            max_grad_norm=max_grad_norm,\n",
    "                            use_sde=use_sde,\n",
    "                            sde_sample_freq=sde_sample_freq,\n",
    "                            target_kl=target_kl,\n",
    "                            tensorboard_log=tensorboard_log,\n",
    "                            create_eval_env=create_eval_env,\n",
    "                            policy_kwargs=policy_kwargs,\n",
    "                            verbose=verbose,\n",
    "                            seed=seed,\n",
    "                            device=device,\n",
    "                            _init_setup_model = _init_setup_model)\n",
    "    \n",
    "    def train_model(self, **train_params):\n",
    "        self.model = self.model.learn(**train_params)\n",
    "        return self.model\n",
    "    \n",
    "    def predict(self, environment, **test_params):\n",
    "        env_test, obs_test = environment.get_env()\n",
    "        \"\"\"make a prediction\"\"\"\n",
    "        account_memory = []\n",
    "        actions_memory = []\n",
    "\n",
    "        env_test.reset()\n",
    "        for i in range(len(environment.df.index.unique())):\n",
    "            action, _states = self.model.predict(obs_test, **test_params)\n",
    "            obs_test, rewards, dones, info = env_test.step(action)\n",
    "            if i == (len(environment.df.index.unique()) - 2):\n",
    "                account_memory = env_test.env_method(method_name=\"save_asset_memory\")\n",
    "                actions_memory = env_test.env_method(method_name=\"save_action_memory\")\n",
    "            if dones[0]:\n",
    "                print(\"hit end!\")\n",
    "                break\n",
    "\n",
    "        return account_memory[0], actions_memory[0]\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        self.model = self.model.load(path)\n",
    "        return self.model\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        self.model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG(RLAgent):\n",
    "    def __init__(self,\n",
    "                policy: \"MlpPolicy\",\n",
    "                env: None,\n",
    "                learning_rate : 1e-3,\n",
    "                buffer_size: 1_000_000,  # 1e6\n",
    "                learning_starts: 100,\n",
    "                batch_size:  100,\n",
    "                tau:  0.005,\n",
    "                gamma:  0.99,\n",
    "                train_freq:  1,\n",
    "                gradient_steps: int = -1,\n",
    "                action_noise: Optional[ActionNoise] = None,\n",
    "                replay_buffer_class: Optional[ReplayBuffer] = None,\n",
    "                replay_buffer_kwargs: Optional[Dict[str, Any]] = None,\n",
    "                optimize_memory_usage: bool = False,\n",
    "                tensorboard_log: Optional[str] = None,\n",
    "                create_eval_env: bool = False,\n",
    "                policy_kwargs: Optional[Dict[str, Any]] = None,\n",
    "                verbose: int = 0,\n",
    "                seed: Optional[int] = None,\n",
    "                device: Union[th.device, str] = \"auto\",\n",
    "                _init_setup_model: bool = True):\n",
    "                \n",
    "        self.env = env\n",
    "    \n",
    "        self.model = sb_DDPG(policy = policy,\n",
    "                            env=self.env,\n",
    "                            learning_rate = learning_rate,\n",
    "                            buffer_size = buffer_size,\n",
    "                            learning_starts= learning_starts,\n",
    "                            batch_size = batch_size,\n",
    "                            tau = tau,\n",
    "                            gamma= gamma,\n",
    "                            train_freq = train_freq,\n",
    "                            gradient_steps = gradient_steps,\n",
    "                            action_noise= action_noise,\n",
    "                            replay_buffer_class= replay_buffer_class,\n",
    "                            replay_buffer_kwargs= replay_buffer_kwargs,\n",
    "                            optimize_memory_usage=optimize_memory_usage,\n",
    "                            tensorboard_log=tensorboard_log,\n",
    "                            create_eval_env=create_eval_env,\n",
    "                            policy_kwargs=policy_kwargs,\n",
    "                            verbose=verbose,\n",
    "                            seed=seed,\n",
    "                            device=device,\n",
    "                            _init_setup_model = _init_setup_model)\n",
    "\n",
    "    def train_model(self, **train_params):\n",
    "        self.model = self.model.learn(**train_params)\n",
    "        return self.model\n",
    "\n",
    "    def predict(self, environment, **test_params):\n",
    "        env_test, obs_test = environment.get_env()\n",
    "        \"\"\"make a prediction\"\"\"\n",
    "        account_memory = []\n",
    "        actions_memory = []\n",
    "\n",
    "        env_test.reset()\n",
    "        for i in range(len(environment.df.index.unique())):\n",
    "            action, _states = self.model.predict(obs_test, **test_params)\n",
    "            obs_test, rewards, dones, info = env_test.step(action)\n",
    "            if i == (len(environment.df.index.unique()) - 2):\n",
    "                account_memory = env_test.env_method(method_name=\"save_asset_memory\")\n",
    "                actions_memory = env_test.env_method(method_name=\"save_action_memory\")\n",
    "            if dones[0]:\n",
    "                print(\"hit end!\")\n",
    "                break\n",
    "\n",
    "        return account_memory[0], actions_memory[0]\n",
    "        \n",
    "    def load_model(self, path):\n",
    "        self.model = self.model.load(path)\n",
    "        return self.model\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        self.model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3(RLAgent):\n",
    " \n",
    "    def __init__(self,\n",
    "                policy: \"MlpPolicy\",\n",
    "                env: None,\n",
    "                learning_rate: float =  1e-3,\n",
    "                buffer_size: int = 1_000_000,  # 1e6\n",
    "                learning_starts: int = 100,\n",
    "                batch_size: int = 100,\n",
    "                tau: float = 0.005,\n",
    "                gamma: float = 0.99,\n",
    "                train_freq: int = 1,\n",
    "                gradient_steps: int = -1,\n",
    "                action_noise: Optional[ActionNoise] = None,\n",
    "                replay_buffer_class: Optional[ReplayBuffer] = None,\n",
    "                replay_buffer_kwargs: Optional[Dict[str, Any]] = None,\n",
    "                optimize_memory_usage: bool = False,\n",
    "                tensorboard_log: Optional[str] = None,\n",
    "                create_eval_env: bool = False,\n",
    "                policy_kwargs: Optional[Dict[str, Any]] = None,\n",
    "                verbose: int = 0,\n",
    "                seed: Optional[int] = None,\n",
    "                device: Union[th.device, str] = \"auto\",\n",
    "                _init_setup_model: bool = True):\n",
    "                \n",
    "        self.env = env\n",
    "    \n",
    "        self.model = sb_TD3(policy = policy,\n",
    "                            env=self.env,\n",
    "                            learning_rate = learning_rate,\n",
    "                            buffer_size = buffer_size,\n",
    "                            learning_starts= learning_starts,\n",
    "                            batch_size = batch_size,\n",
    "                            tau = tau,\n",
    "                            gamma= gamma,\n",
    "                            train_freq = train_freq,\n",
    "                            gradient_steps = gradient_steps,\n",
    "                            action_noise= action_noise,\n",
    "                            replay_buffer_class= replay_buffer_class,\n",
    "                            replay_buffer_kwargs= replay_buffer_kwargs,\n",
    "                            optimize_memory_usage=optimize_memory_usage,\n",
    "                            tensorboard_log=tensorboard_log,\n",
    "                            create_eval_env=create_eval_env,\n",
    "                            policy_kwargs=policy_kwargs,\n",
    "                            verbose=verbose,\n",
    "                            seed=seed,\n",
    "                            device=device,\n",
    "                            _init_setup_model = _init_setup_model)\n",
    "\n",
    "    def train_model(self, **train_params):\n",
    "        self.model = self.model.learn(**train_params)\n",
    "        return self.model\n",
    "\n",
    "    def predict(self, environment, **test_params):\n",
    "        env_test, obs_test = environment.get_env()\n",
    "        \"\"\"make a prediction\"\"\"\n",
    "        account_memory = []\n",
    "        actions_memory = []\n",
    "\n",
    "        env_test.reset()\n",
    "        for i in range(len(environment.df.index.unique())):\n",
    "            action, _states = self.model.predict(obs_test, **test_params)\n",
    "            obs_test, rewards, dones, info = env_test.step(action)\n",
    "            if i == (len(environment.df.index.unique()) - 2):\n",
    "                account_memory = env_test.env_method(method_name=\"save_asset_memory\")\n",
    "                actions_memory = env_test.env_method(method_name=\"save_action_memory\")\n",
    "            if dones[0]:\n",
    "                print(\"hit end!\")\n",
    "                break\n",
    "\n",
    "        return account_memory[0], actions_memory[0]\n",
    "        \n",
    "    def load_model(self, path):\n",
    "        self.model = self.model.load(path)\n",
    "        return self.model\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        self.model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gather user parameters\n",
    "with open(\"../user_params.yaml\", \"r\") as stream:\n",
    "    try:\n",
    "        user_params = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = user_params[\"tickers\"]\n",
    "env_kwargs = user_params[\"env_params\"]\n",
    "train_params = user_params[\"train_params\"]\n",
    "policy_params = user_params[\"policy_params\"]\n",
    "test_params = user_params[\"test_params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 3: Downloading from Yahoo.........\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Shape of DataFrame:  (12924, 8)\n",
      "        date       open       high        low      close     volume   tic  day\n",
      "0 2008-12-31   3.070357   3.133571   3.047857   2.606278  607541200  AAPL    2\n",
      "1 2008-12-31  41.590000  43.049999  41.500000  32.005886    5443100    BA    2\n",
      "2 2008-12-31  43.700001  45.099998  43.700001  30.628826    6277400   CAT    2\n",
      "3 2008-12-31  72.900002  74.629997  72.900002  43.670765    9964300   CVX    2\n",
      "4 2009-01-02   3.067143   3.251429   3.041429   2.771174  746015200  AAPL    4\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTest 3: Downloading from Yahoo.........\")\n",
    "downloaded_df = DataDownloader(start_date='2009-01-01',\n",
    "                                end_date='2021-10-31',\n",
    "                                ticker_list= tickers).download_from_yahoo()\n",
    "print(downloaded_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 4: Feature engineer.........\n",
      "Successfully added technical indicators\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Shape of DataFrame:  (3231, 8)\n",
      "Successfully added vix\n",
      "Successfully added turbulence index\n",
      "Successfully added covariances\n",
      "        date  ...                                        return_list\n",
      "0 2009-12-31  ...  tic             AAPL        BA       CAT      ...\n",
      "0 2009-12-31  ...  tic             AAPL        BA       CAT      ...\n",
      "0 2009-12-31  ...  tic             AAPL        BA       CAT      ...\n",
      "0 2009-12-31  ...  tic             AAPL        BA       CAT      ...\n",
      "1 2010-01-04  ...  tic             AAPL        BA       CAT      ...\n",
      "\n",
      "[5 rows x 20 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTest 4: Feature engineer.........\")\n",
    "\n",
    "df_processed = DefaultFeatureEngineer( use_default= False,\n",
    "                                       tech_indicator_list= env_kwargs[\"tech_indicator_list\"],\n",
    "                                       use_vix=True,\n",
    "                                       use_turbulence=True,\n",
    "                                       use_covar=True).extend_data(downloaded_df)  # included technical indicators as features\n",
    "\n",
    "print(df_processed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PortfolioEnv(df=df_processed, **env_kwargs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_train, _ = env.get_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "td3 = TD3(env = env_train, **policy_params[\"TD3_PARAMS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.td3.td3.TD3 at 0x122f8da60>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td3.train_model(**train_params[\"TD3_PARAMS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test = PortfolioEnv(df=df_processed, **env_kwargs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0\n",
      "0     0.000000\n",
      "1     0.024403\n",
      "2     0.010359\n",
      "3    -0.000608\n",
      "4     0.005737\n",
      "...        ...\n",
      "2973 -0.002207\n",
      "2974  0.003908\n",
      "2975 -0.004611\n",
      "2976 -0.012020\n",
      "2977  0.026037\n",
      "\n",
      "[2978 rows x 1 columns]\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:9641012.870825065\n",
      "Sharpe:  0.9484473772200268\n",
      "=================================\n",
      "hit end!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(           date  daily_return\n",
       " 0    2009-12-31      0.000000\n",
       " 1    2010-01-04      0.024403\n",
       " 2    2010-01-05      0.010359\n",
       " 3    2010-01-06     -0.000608\n",
       " 4    2010-01-07      0.005737\n",
       " ...         ...           ...\n",
       " 2973 2021-10-22     -0.002207\n",
       " 2974 2021-10-25      0.003908\n",
       " 2975 2021-10-26     -0.004611\n",
       " 2976 2021-10-27     -0.012020\n",
       " 2977 2021-10-28      0.026037\n",
       " \n",
       " [2978 rows x 2 columns],\n",
       "                 AAPL        BA       CAT       CVX\n",
       " date                                              \n",
       " 2009-12-31  0.250000  0.250000  0.250000  0.250000\n",
       " 2010-01-04  0.365529  0.134471  0.365529  0.134471\n",
       " 2010-01-05  0.365529  0.134471  0.365529  0.134471\n",
       " 2010-01-06  0.365529  0.134471  0.365529  0.134471\n",
       " 2010-01-07  0.365529  0.134471  0.365529  0.134471\n",
       " ...              ...       ...       ...       ...\n",
       " 2021-10-22  0.365529  0.134471  0.365529  0.134471\n",
       " 2021-10-25  0.365529  0.134471  0.365529  0.134471\n",
       " 2021-10-26  0.365529  0.134471  0.365529  0.134471\n",
       " 2021-10-27  0.365529  0.134471  0.365529  0.134471\n",
       " 2021-10-28  0.365529  0.134471  0.365529  0.134471\n",
       " \n",
       " [2978 rows x 4 columns])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td3.predict(environment = env_test, **test_params[\"TD3_PARAMS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/stable_baselines3/common/save_util.py:276: UserWarning: Path 'AgentLayer/RLAgents' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"
     ]
    }
   ],
   "source": [
    "td3.save_model(\"AgentLayer/RLAgents/td3_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_td3_model = td3.load_model(\"AgentLayer/RLAgents/td3_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.td3.td3.TD3 at 0x12265bdf0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_td3_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1ee38ef4a5a9feb55287fd749643f13d043cb0a7addaab2a9c224cbe137c0062"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
