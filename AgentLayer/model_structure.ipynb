{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "class Environment:\n",
    "    \n",
    "    def __init__(self, data, params):\n",
    "        self.data = data\n",
    "        self.params = params\n",
    "        self.portfilio_env = PortfolioEnvironmentClass(self.data, **self.params)\n",
    "\n",
    "    def environment(self):\n",
    "        e = DummyVecEnv([lambda: self.portfilio_env])\n",
    "        obs = e.reset()\n",
    "        return e, obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortfolioEnvironmentClass(Environment):\n",
    "    def __init__(self, \n",
    "                df,\n",
    "                stock_dim,\n",
    "                hmax,\n",
    "                initial_amount,\n",
    "                transaction_cost_pct,\n",
    "                reward_scaling,\n",
    "                state_space,\n",
    "                action_space,\n",
    "                tech_indicator_list,\n",
    "                turbulence_threshold=None,\n",
    "                lookback=252,\n",
    "                day = 0):\n",
    "\n",
    "        self.day = day\n",
    "        self.lookback=lookback\n",
    "        self.df = df\n",
    "        self.stock_dim = stock_dim\n",
    "        self.hmax = hmax\n",
    "        self.initial_amount = initial_amount\n",
    "        self.transaction_cost_pct =transaction_cost_pct\n",
    "        self.reward_scaling = reward_scaling\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.tech_indicator_list = tech_indicator_list\n",
    "    \n",
    "        # action_space normalization and shape is self.stock_dim\n",
    "        self.action_space = spaces.Box(low = 0, high = 1,shape = (self.action_space,)) \n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape = (self.state_space+len(self.tech_indicator_list),self.state_space))\n",
    "\n",
    "        # load data from a pandas dataframe\n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        self.covs = self.data['cov_list'].values[0]\n",
    "        self.state =  np.append(np.array(self.covs), [self.data[tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)\n",
    "        self.terminal = False     \n",
    "        self.turbulence_threshold = turbulence_threshold        \n",
    "        # initalize state: inital portfolio return + individual stock return + individual weights\n",
    "        self.portfolio_value = self.initial_amount\n",
    "\n",
    "        # memorize portfolio value each step\n",
    "        self.asset_memory = [self.initial_amount]\n",
    "        # memorize portfolio return each step\n",
    "        self.portfolio_return_memory = [0]\n",
    "        self.actions_memory=[[1/self.stock_dim]*self.stock_dim]\n",
    "        self.date_memory=[self.data.date.unique()[0]]\n",
    "\n",
    "    def step(self, actions):\n",
    "        self.terminal = self.day >= len(self.df.index.unique())-1\n",
    "\n",
    "        if self.terminal:\n",
    "            df = pd.DataFrame(self.portfolio_return_memory)\n",
    "            df.columns = ['daily_return']\n",
    "            plt.plot(df.daily_return.cumsum(),'r')\n",
    "            plt.savefig('results/cumulative_reward.png')\n",
    "            plt.close()\n",
    "            \n",
    "            plt.plot(self.portfolio_return_memory,'r')\n",
    "            plt.savefig('results/rewards.png')\n",
    "            plt.close()\n",
    "\n",
    "            print(\"=================================\")\n",
    "            print(\"begin_total_asset:{}\".format(self.asset_memory[0]))           \n",
    "            print(\"end_total_asset:{}\".format(self.portfolio_value))\n",
    "\n",
    "            df_daily_return = pd.DataFrame(self.portfolio_return_memory)\n",
    "            df_daily_return.columns = ['daily_return']\n",
    "            if df_daily_return['daily_return'].std() !=0:\n",
    "              sharpe = (252**0.5)*df_daily_return['daily_return'].mean()/ \\\n",
    "                       df_daily_return['daily_return'].std()\n",
    "              print(\"Sharpe: \",sharpe)\n",
    "            print(\"=================================\")\n",
    "            \n",
    "            return self.state, self.reward, self.terminal,{}\n",
    "\n",
    "        else:\n",
    "            weights = self.softmax_normalization(actions) \n",
    "            self.actions_memory.append(weights)\n",
    "            last_day_memory = self.data\n",
    "\n",
    "            #load next state\n",
    "            self.day += 1\n",
    "            self.data = self.df.loc[self.day,:]\n",
    "            self.covs = self.data['cov_list'].values[0]\n",
    "            self.state =  np.append(np.array(self.covs), [self.data[tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)\n",
    "            portfolio_return = sum(((self.data.close.values / last_day_memory.close.values)-1)*weights)\n",
    "            log_portfolio_return = np.log(sum((self.data.close.values / last_day_memory.close.values)*weights))\n",
    "            # update portfolio value\n",
    "            new_portfolio_value = self.portfolio_value*(1+portfolio_return)\n",
    "            self.portfolio_value = new_portfolio_value\n",
    "\n",
    "            # save into memory\n",
    "            self.portfolio_return_memory.append(portfolio_return)\n",
    "            self.date_memory.append(self.data.date.unique()[0])            \n",
    "            self.asset_memory.append(new_portfolio_value)\n",
    "\n",
    "            # the reward is the new portfolio value or end portfolo value\n",
    "            self.reward = new_portfolio_value\n",
    "            \n",
    "\n",
    "        return self.state, self.reward, self.terminal, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        return self.state\n",
    "\n",
    "    def softmax_normalization(self, actions):\n",
    "        numerator = np.exp(actions)\n",
    "        denominator = np.sum(np.exp(actions))\n",
    "        softmax_output = numerator/denominator\n",
    "        return softmax_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_model():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save_model():\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def load_model():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConventionalAgent(Agent, ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_model():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save_model():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_model():\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _return_predict():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _weight_optimization():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLAgent(Agent, ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_model():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict():\n",
    "        \n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save_model():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_model():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C(RLAgent):\n",
    "\n",
    "    def __init__(self, model_params):\n",
    "        self.model_params = model_params\n",
    "        self.model = A2C(self.model_params[\"policy\"], self.model_params[\"environment\"], self.model_params[\"verbose\"])\n",
    "        #self.model = A2C(self.model_params[\"policy\"], self.model_params[\"environment\"], self.model_params[\"verbose\"], self.model_params[\"n_steps\"],\n",
    "        #                 self.model_params[\"gamma\"], self.model_params[\"vf_coef\"], self.model_params[\"alpha\"], self.model_params[\"momentum\"], \n",
    "        #                self.model_params[\"lr_schedule\"], self.model_params[\"epsilon\"]) \n",
    "\n",
    "    def train_model(self, train_params):\n",
    "        self.model =  self.model.learn(total_timesteps = train_params[\"total_timesteps\"], log_interval = train_params[\"log_interval\"])\n",
    "        return self.model\n",
    "        \n",
    "    def predict(self, test_params):\n",
    "\n",
    "        test_env, test_obs = test_params[\"environment\"].environment()\n",
    "        \"\"\"make a prediction\"\"\"\n",
    "        account_memory = []\n",
    "        actions_memory = []\n",
    "\n",
    "        test_env.reset()\n",
    "        for i in range(len(test_params[\"environment\"].df.index.unique())):\n",
    "            action, _states = self.model.predict(test_obs, deterministic = test_params[\"deterministic\"])\n",
    "            test_obs, rewards, dones, info = test_env.step(action)\n",
    "            if i == (len(test_params[\"environment\"].df.index.unique()) - 2):\n",
    "                account_memory = test_env.env_method(method_name=\"save_asset_memory\")\n",
    "                actions_memory = test_env.env_method(method_name=\"save_action_memory\")\n",
    "            if dones[0]:\n",
    "                print(\"hit end!\")\n",
    "                break\n",
    "\n",
    "        return account_memory[0], actions_memory[0]\n",
    "\n",
    "    def save_model(self, model_name):\n",
    "        self.model.save(model_name)\n",
    "\n",
    "    def load_model(self, model_name):\n",
    "        self.model = self.model.load(model_name)\n",
    "        return self.model\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TECHNICAL_INDICATORS_LIST = [\n",
    "    \"macd\",\n",
    "    \"boll_ub\",\n",
    "    \"boll_lb\",\n",
    "    \"rsi_30\",\n",
    "    \"cci_30\",\n",
    "    \"dx_30\",\n",
    "    \"close_30_sma\",\n",
    "    \"close_60_sma\",\n",
    "]\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100, #maximum number of shares to trade\n",
    "    \"initial_amount\": 1000000, # initial cash\n",
    "    \"transaction_cost_pct\": 0.001, # transaction cost percentage\n",
    "    \"state_space\": 29, # number of unique stocks \n",
    "    \"stock_dim\": 29, # number of unique stocks\n",
    "    \"tech_indicator_list\": TECHNICAL_INDICATORS_LIST, # technical indicators\n",
    "    \"action_space\": 29, # number of stocks in training data\n",
    "    \"reward_scaling\": 1e-1  #hyperparameter\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_train_gym = Environment(data = train, params = env_kwargs) #train parametresi training data olucak (dataframe) daha koyulmadi \n",
    "env_train, _ = e_train_gym.environment()\n",
    "\n",
    "e_trade_gym = Environment(data = trade, params = env_kwargs) #trade parametresi test data olucak (dataframe) daha koyulmadi\n",
    "env_train, _ = e_train_gym.environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2c_params = {\"policy\" : \"MlpPolicy\", \n",
    "                \"environment\" : e_train_gym,\n",
    "                \"verbose\" : 1}\n",
    "\n",
    "train_params = {\"total_timesteps\": 25000,\n",
    "                \"log_interval\": 100}\n",
    "\n",
    "test_params = {\"environment\" : e_train_gym,\n",
    "               \"deterministic\" : True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#object creation\n",
    "a2c = A2C(a2c_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "a2c.train_model(train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting\n",
    "a2c.predict(test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving \n",
    "a2c.save_model(\"a2c_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading\n",
    "loaded_a2c_model = a2c.load_model(\"a2c_model\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1ee38ef4a5a9feb55287fd749643f13d043cb0a7addaab2a9c224cbe137c0062"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
