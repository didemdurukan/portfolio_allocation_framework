{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/doganparlak/Desktop/Master_2.2/Master_Project/uniFi_github/uniFi/AgentLayer', '/usr/local/Cellar/python@3.8/3.8.8_1/Frameworks/Python.framework/Versions/3.8/lib/python38.zip', '/usr/local/Cellar/python@3.8/3.8.8_1/Frameworks/Python.framework/Versions/3.8/lib/python3.8', '/usr/local/Cellar/python@3.8/3.8.8_1/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload', '', '/Users/doganparlak/Library/Python/3.8/lib/python/site-packages', '/usr/local/lib/python3.8/site-packages', '/usr/local/lib/python3.8/site-packages/selenium-3.141.0-py3.8.egg', '/usr/local/lib/python3.8/site-packages/urllib3-1.26.4-py3.8.egg', '/Users/doganparlak/Desktop/Master_2.2/Master_Project/uniFi_github/uniFi', '/Users/doganparlak/Desktop/Master_2.2/Master_Project/uniFi_github/uniFi']\n"
     ]
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import A2C as sb_A2C\n",
    "from stable_baselines3 import PPO as sb_PPO\n",
    "from stable_baselines3 import DDPG as sb_DDPG\n",
    "from stable_baselines3 import TD3 as sb_TD3\n",
    "from stable_baselines3.common.noise import ActionNoise\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "from stable_baselines3.common.type_aliases import Schedule\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch as th\n",
    "import yaml\n",
    "import gym\n",
    "from datetime import datetime\n",
    "from sys import path\n",
    "from os.path import dirname as dir\n",
    "from typing import Any, Dict, Optional, Tuple, Type, Union\n",
    "path.append(dir(path[0]))\n",
    "print(path)\n",
    "#__package__ = \"examples\"\n",
    "\n",
    "from FinancialDataLayer.DataCollection.DataDownloader import DataDownloader\n",
    "from FinancialDataLayer.DataProcessing.DefaultFeatureEngineer import DefaultFeatureEngineer\n",
    "from AgentLayer.DataSplitter.TimeSeriesSplitter import TimeSeriesSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pypfopt\n",
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "from pypfopt import risk_models\n",
    "from pypfopt import EfficientFrontier\n",
    "from pypfopt import risk_models\n",
    "from pypfopt import expected_returns\n",
    "from pypfopt import objective_functions\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(gym.Env, ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def step(self, action):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def render(self, mode=\"human\"):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_env(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax_normalization(actions):\n",
    "        numerator = np.exp(actions)\n",
    "        denominator = np.sum(np.exp(actions))\n",
    "        softmax_output = numerator / denominator\n",
    "        return softmax_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_model():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save_model():\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def load_model():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConventionalAgent(Agent, ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_model():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save_model():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_model():\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _return_predict():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _weight_optimization():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLAgent(Agent, ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_model():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict():\n",
    "        \n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save_model():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_model():\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_weights(rl_actions_list):\n",
    "        agent_weight_df = {'date': [], 'weights': []}\n",
    "        for i in range(len(rl_actions_list)):\n",
    "            date = rl_actions_list.index[i]\n",
    "            tic_list = list(rl_actions_list.columns)\n",
    "            weights_list = rl_actions_list.reset_index()[list(rl_actions_list.columns)].iloc[i].values\n",
    "            weight_dict = {'tic': [], 'weight': []}\n",
    "            for j in range(len(tic_list)):\n",
    "                weight_dict['tic'] += [tic_list[j]]\n",
    "                weight_dict['weight'] += [weights_list[j]]\n",
    "\n",
    "            agent_weight_df['date'] += [date]\n",
    "            agent_weight_df['weights'] += [pd.DataFrame(weight_dict)]\n",
    "\n",
    "        agent_weights = pd.DataFrame(agent_weight_df)\n",
    "        return agent_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RFAgent(ConventionalAgent):\n",
    "\n",
    "    def __init__(self,\n",
    "                n_estimators = 100,\n",
    "                criterion = \"squared_error\",\n",
    "                max_depth = None,\n",
    "                min_samples_split = 2,\n",
    "                min_samples_leaf = 1,\n",
    "                min_weight_fraction_leaf = 0,\n",
    "                max_features = 1,\n",
    "                max_leaf_nodes = None,\n",
    "                min_impurity_decrease = 0,\n",
    "                bootstrap = True,\n",
    "                oob_score = False,\n",
    "                n_jobs = None,\n",
    "                random_state = None,\n",
    "                verbose = 0,\n",
    "                warm_start = False,\n",
    "                ccp_alpha = 0,\n",
    "                max_samples = None):\n",
    "\n",
    "        self.model = RandomForestRegressor(n_estimators= n_estimators,\n",
    "                            criterion= criterion,\n",
    "                            max_depth=max_depth,\n",
    "                            min_samples_split=min_samples_split,\n",
    "                            min_samples_leaf=min_samples_leaf,\n",
    "                            min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
    "                            max_features=max_features,\n",
    "                            max_leaf_nodes= max_leaf_nodes,\n",
    "                            min_impurity_decrease=min_impurity_decrease,\n",
    "                            bootstrap=bootstrap,\n",
    "                            oob_score=oob_score,\n",
    "                            n_jobs=n_jobs,\n",
    "                            random_state= random_state,\n",
    "                            verbose=verbose,\n",
    "                            warm_start=warm_start,\n",
    "                            ccp_alpha=ccp_alpha,\n",
    "                            max_samples=max_samples)\n",
    "    \n",
    "    def train_model(self, train_x, train_y, **train_params):\n",
    "        '''\n",
    "        *Trains the model*\n",
    "        Input: Train data x and train data y\n",
    "        Output: Linear Regression Model\n",
    "        '''\n",
    "        try:\n",
    "            trained = self.model.fit(train_x, train_y.ravel(), **train_params)\n",
    "            print(\"Model trained succesfully\")\n",
    "            return trained\n",
    "        except Exception as e:\n",
    "            print(\"training unsuccessful\")    \n",
    "    \n",
    "    def predict(self,\n",
    "                test_data, \n",
    "                initial_capital = 0,\n",
    "                tech_indicator_list = [\n",
    "                        \"macd\",\n",
    "                        \"boll_ub\",\n",
    "                        \"boll_lb\",\n",
    "                        \"rsi_30\",\n",
    "                        \"cci_30\",\n",
    "                        \"dx_30\",\n",
    "                        \"close_30_sma\",\n",
    "                        \"close_60_sma\",\n",
    "                    ]):\n",
    "\n",
    "            meta_coefficient = {\"date\": [], \"weights\": []}\n",
    "            unique_trade_date = test_data.date.unique()\n",
    "            portfolio = pd.DataFrame(index=range(1), columns=unique_trade_date)\n",
    "            portfolio.loc[0, unique_trade_date[0]] = initial_capital\n",
    "\n",
    "            for i in range(len(unique_trade_date) - 1):\n",
    "                mu, sigma, tics, df_current, df_next = self._return_predict(\n",
    "                    unique_trade_date, test_data, i, tech_indicator_list)\n",
    "\n",
    "                portfolio_value = self._weight_optimization(\n",
    "                    i, unique_trade_date, meta_coefficient, mu, sigma, tics, portfolio, df_current, df_next)\n",
    "        \n",
    "            portfolio = portfolio_value\n",
    "            portfolio = portfolio.T\n",
    "            portfolio.columns = ['account_value']\n",
    "            portfolio = portfolio.reset_index()\n",
    "            portfolio.columns = ['date', 'account_value']\n",
    "\n",
    "            '''Backtest hasn't been implemented yet, hence commented.'''\n",
    "            #stats = backtest_stats(portfolio, value_col_name='account_value')\n",
    "            \n",
    "            portfolio_cumprod = (\n",
    "                portfolio.account_value.pct_change()+1).cumprod()-1\n",
    "\n",
    "            return portfolio, portfolio_cumprod, pd.DataFrame(meta_coefficient)\n",
    "    \n",
    "    def _return_predict(self, unique_trade_date, test_data, i, tech_indicator_list):\n",
    "\n",
    "        current_date = unique_trade_date[i]\n",
    "        next_date = unique_trade_date[i+1]\n",
    "\n",
    "        df_current = test_data[test_data.date ==\n",
    "                                  current_date].reset_index(drop=True)\n",
    "        df_next = test_data[test_data.date ==\n",
    "                               next_date].reset_index(drop=True)\n",
    "\n",
    "        tics = df_current['tic'].values\n",
    "        features = df_current[tech_indicator_list].values\n",
    "\n",
    "        predicted_y = self.model.predict(features)\n",
    "        mu = predicted_y\n",
    "        sigma = risk_models.sample_cov(\n",
    "            df_current.return_list[0], returns_data=True)\n",
    "\n",
    "        return mu, sigma, tics, df_current, df_next\n",
    "    \n",
    "    def _weight_optimization(self, i, unique_trade_date, meta_coefficient, mu, sigma, tics, portfolio, df_current, df_next):\n",
    "\n",
    "        current_date = unique_trade_date[i]\n",
    "        predicted_y_df = pd.DataFrame(\n",
    "            {\"tic\": tics.reshape(-1,), \"predicted_y\": mu.reshape(-1,)})\n",
    "        min_weight, max_weight = 0, 1\n",
    "\n",
    "        ef = EfficientFrontier(mu, sigma)\n",
    "        weights = ef.nonconvex_objective(\n",
    "            objective_functions.sharpe_ratio,\n",
    "            objective_args=(ef.expected_returns, ef.cov_matrix),\n",
    "            weights_sum_to_one=True,\n",
    "            constraints=[\n",
    "                # greater than min_weight\n",
    "                {\"type\": \"ineq\", \"fun\": lambda w: w - min_weight},\n",
    "                # less than max_weight\n",
    "                {\"type\": \"ineq\", \"fun\": lambda w: max_weight - w},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        weight_df = {\"tic\": [], \"weight\": []}\n",
    "        meta_coefficient[\"date\"] += [current_date]\n",
    "\n",
    "        for item in weights:\n",
    "            weight_df['tic'] += [item]\n",
    "            weight_df['weight'] += [weights[item]]\n",
    "\n",
    "        weight_df = pd.DataFrame(weight_df).merge(predicted_y_df, on=['tic'])\n",
    "        meta_coefficient[\"weights\"] += [weight_df]\n",
    "        cap = portfolio.iloc[0, i]\n",
    "        # current cash invested for each stock\n",
    "        current_cash = [element * cap for element in list(weights.values())]\n",
    "        # current held shares\n",
    "        current_shares = list(np.array(current_cash) / np.array(df_current.close))\n",
    "        # next time period price\n",
    "        next_price = np.array(df_next.close)\n",
    "        portfolio.iloc[0, i+1] = np.dot(current_shares, next_price)\n",
    "\n",
    "        return portfolio \n",
    "\n",
    "    def save_model(self,  file_name):\n",
    "        with open(file_name, 'wb') as files:\n",
    "            pickle.dump(self.model, files)\n",
    "        print(\"Model saved succesfully.\")\n",
    "\n",
    "\n",
    "    def load_model(self, file_name):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            self.model = pickle.load(f)\n",
    "        print(\"Model loaded succesfully.\")\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTAgent(ConventionalAgent):\n",
    "    \n",
    "    def __init__(self,\n",
    "                criterion = \"squared_error\",\n",
    "                splitter = \"best\",\n",
    "                max_depth = None,\n",
    "                min_samples_split = 2,\n",
    "                min_samples_leaf = 1,\n",
    "                min_weight_fraction_leaf = 0,\n",
    "                max_features = None,\n",
    "                random_state = None,\n",
    "                max_leaf_nodes = None,\n",
    "                min_impurity_decrease = 0,\n",
    "                ccp_alpha = 0):\n",
    "                \n",
    "\n",
    "        self.model = DecisionTreeRegressor( criterion = criterion,\n",
    "                                            splitter = splitter,\n",
    "                                            max_depth = max_depth,\n",
    "                                            min_samples_split = min_samples_split,\n",
    "                                            min_samples_leaf = min_samples_leaf,\n",
    "                                            min_weight_fraction_leaf = min_weight_fraction_leaf,\n",
    "                                            max_features = max_features,\n",
    "                                            random_state = random_state,\n",
    "                                            max_leaf_nodes = max_leaf_nodes,\n",
    "                                            min_impurity_decrease = min_impurity_decrease,\n",
    "                                            ccp_alpha = ccp_alpha)\n",
    "                                            \n",
    "    def train_model(self, train_x, train_y, **train_params):\n",
    "        '''\n",
    "        *Trains the model*\n",
    "        Input: Train data x and train data y\n",
    "        Output: Linear Regression Model\n",
    "        '''\n",
    "        try:\n",
    "            trained = self.model.fit(train_x, train_y, **train_params)\n",
    "            print(\"Model trained succesfully\")\n",
    "            return trained\n",
    "        except Exception as e:\n",
    "            print(\"training unsuccessful\")    \n",
    "    \n",
    "    def predict(self,\n",
    "                test_data, \n",
    "                initial_capital = 0,\n",
    "                tech_indicator_list = [\n",
    "                        \"macd\",\n",
    "                        \"boll_ub\",\n",
    "                        \"boll_lb\",\n",
    "                        \"rsi_30\",\n",
    "                        \"cci_30\",\n",
    "                        \"dx_30\",\n",
    "                        \"close_30_sma\",\n",
    "                        \"close_60_sma\",\n",
    "                    ]):\n",
    "\n",
    "            meta_coefficient = {\"date\": [], \"weights\": []}\n",
    "            unique_trade_date = test_data.date.unique()\n",
    "            portfolio = pd.DataFrame(index=range(1), columns=unique_trade_date)\n",
    "            portfolio.loc[0, unique_trade_date[0]] = initial_capital\n",
    "\n",
    "            for i in range(len(unique_trade_date) - 1):\n",
    "                mu, sigma, tics, df_current, df_next = self._return_predict(\n",
    "                    unique_trade_date, test_data, i, tech_indicator_list)\n",
    "\n",
    "                portfolio_value = self._weight_optimization(\n",
    "                    i, unique_trade_date, meta_coefficient, mu, sigma, tics, portfolio, df_current, df_next)\n",
    "        \n",
    "            portfolio = portfolio_value\n",
    "            portfolio = portfolio.T\n",
    "            portfolio.columns = ['account_value']\n",
    "            portfolio = portfolio.reset_index()\n",
    "            portfolio.columns = ['date', 'account_value']\n",
    "\n",
    "            '''Backtest hasn't been implemented yet, hence commented.'''\n",
    "            #stats = backtest_stats(portfolio, value_col_name='account_value')\n",
    "            \n",
    "            portfolio_cumprod = (\n",
    "                portfolio.account_value.pct_change()+1).cumprod()-1\n",
    "\n",
    "            return portfolio, portfolio_cumprod, pd.DataFrame(meta_coefficient)\n",
    "            \n",
    "    def _return_predict(self, unique_trade_date, test_data, i, tech_indicator_list):\n",
    "\n",
    "        current_date = unique_trade_date[i]\n",
    "        next_date = unique_trade_date[i+1]\n",
    "\n",
    "        df_current = test_data[test_data.date ==\n",
    "                                  current_date].reset_index(drop=True)\n",
    "        df_next = test_data[test_data.date ==\n",
    "                               next_date].reset_index(drop=True)\n",
    "\n",
    "        tics = df_current['tic'].values\n",
    "        features = df_current[tech_indicator_list].values\n",
    "\n",
    "        predicted_y = self.model.predict(features)\n",
    "        mu = predicted_y\n",
    "        sigma = risk_models.sample_cov(\n",
    "            df_current.return_list[0], returns_data=True)\n",
    "\n",
    "        return mu, sigma, tics, df_current, df_next\n",
    "\n",
    "    def _weight_optimization(self, i, unique_trade_date, meta_coefficient, mu, sigma, tics, portfolio, df_current, df_next):\n",
    "\n",
    "        current_date = unique_trade_date[i]\n",
    "        predicted_y_df = pd.DataFrame(\n",
    "            {\"tic\": tics.reshape(-1,), \"predicted_y\": mu.reshape(-1,)})\n",
    "        min_weight, max_weight = 0, 1\n",
    "\n",
    "        ef = EfficientFrontier(mu, sigma)\n",
    "        weights = ef.nonconvex_objective(\n",
    "            objective_functions.sharpe_ratio,\n",
    "            objective_args=(ef.expected_returns, ef.cov_matrix),\n",
    "            weights_sum_to_one=True,\n",
    "            constraints=[\n",
    "                # greater than min_weight\n",
    "                {\"type\": \"ineq\", \"fun\": lambda w: w - min_weight},\n",
    "                # less than max_weight\n",
    "                {\"type\": \"ineq\", \"fun\": lambda w: max_weight - w},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        weight_df = {\"tic\": [], \"weight\": []}\n",
    "        meta_coefficient[\"date\"] += [current_date]\n",
    "\n",
    "        for item in weights:\n",
    "            weight_df['tic'] += [item]\n",
    "            weight_df['weight'] += [weights[item]]\n",
    "\n",
    "        weight_df = pd.DataFrame(weight_df).merge(predicted_y_df, on=['tic'])\n",
    "        meta_coefficient[\"weights\"] += [weight_df]\n",
    "        cap = portfolio.iloc[0, i]\n",
    "        # current cash invested for each stock\n",
    "        current_cash = [element * cap for element in list(weights.values())]\n",
    "        # current held shares\n",
    "        current_shares = list(np.array(current_cash) / np.array(df_current.close))\n",
    "        # next time period price\n",
    "        next_price = np.array(df_next.close)\n",
    "        portfolio.iloc[0, i+1] = np.dot(current_shares, next_price)\n",
    "\n",
    "        return portfolio \n",
    "    \n",
    "    def save_model(self,  file_name):\n",
    "        with open(file_name, 'wb') as files:\n",
    "            pickle.dump(self.model, files)\n",
    "        print(\"Model saved succesfully.\")\n",
    "\n",
    "\n",
    "    def load_model(self, file_name):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            self.model = pickle.load(f)\n",
    "        print(\"Model loaded succesfully.\")\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVRAgent(ConventionalAgent):\n",
    "    \n",
    "    def __init__(self,\n",
    "                kernel = 'rbf',\n",
    "                degree = 3,\n",
    "                gamma = 'scale',\n",
    "                coef0 = 0,\n",
    "                tol = 0.001,\n",
    "                C = 1,\n",
    "                epsilon = 0.1,\n",
    "                shrinking = True,\n",
    "                cache_size = 200,\n",
    "                verbose = False,\n",
    "                max_iter = -1):  \n",
    "\n",
    "        self.model = SVR(kernel= kernel,\n",
    "                        degree= degree,\n",
    "                        gamma= gamma, \n",
    "                        coef0= coef0,\n",
    "                        tol= tol,\n",
    "                        C= C,\n",
    "                        epsilon= epsilon,\n",
    "                        shrinking= shrinking,\n",
    "                        cache_size= cache_size,\n",
    "                        verbose= verbose,\n",
    "                        max_iter= max_iter)\n",
    "\n",
    "    def train_model(self, train_x, train_y, **train_params):\n",
    "        '''\n",
    "        *Trains the model*\n",
    "        Input: Train data x and train data y\n",
    "        Output: Linear Regression Model\n",
    "        '''\n",
    "        try:\n",
    "            trained = self.model.fit(train_x, train_y.ravel(), **train_params)\n",
    "            print(\"Model trained succesfully\")\n",
    "            return trained\n",
    "        except Exception as e:\n",
    "            print(\"training unsuccessful\")\n",
    "\n",
    "    def predict(self,\n",
    "               test_data, \n",
    "               initial_capital = 0,\n",
    "               tech_indicator_list = [\n",
    "                    \"macd\",\n",
    "                    \"boll_ub\",\n",
    "                    \"boll_lb\",\n",
    "                    \"rsi_30\",\n",
    "                    \"cci_30\",\n",
    "                    \"dx_30\",\n",
    "                    \"close_30_sma\",\n",
    "                    \"close_60_sma\",\n",
    "                ]):\n",
    "\n",
    "        meta_coefficient = {\"date\": [], \"weights\": []}\n",
    "        unique_trade_date = test_data.date.unique()\n",
    "        portfolio = pd.DataFrame(index=range(1), columns=unique_trade_date)\n",
    "        portfolio.loc[0, unique_trade_date[0]] = initial_capital\n",
    "\n",
    "        for i in range(len(unique_trade_date) - 1):\n",
    "            mu, sigma, tics, df_current, df_next = self._return_predict(\n",
    "                unique_trade_date, test_data, i, tech_indicator_list)\n",
    "\n",
    "            portfolio_value = self._weight_optimization(\n",
    "                i, unique_trade_date, meta_coefficient, mu, sigma, tics, portfolio, df_current, df_next)\n",
    "    \n",
    "        portfolio = portfolio_value\n",
    "        portfolio = portfolio.T\n",
    "        portfolio.columns = ['account_value']\n",
    "        portfolio = portfolio.reset_index()\n",
    "        portfolio.columns = ['date', 'account_value']\n",
    "\n",
    "        '''Backtest hasn't been implemented yet, hence commented.'''\n",
    "        #stats = backtest_stats(portfolio, value_col_name='account_value')\n",
    "        \n",
    "        portfolio_cumprod = (\n",
    "            portfolio.account_value.pct_change()+1).cumprod()-1\n",
    "\n",
    "        return portfolio, portfolio_cumprod, pd.DataFrame(meta_coefficient)\n",
    "    \n",
    "    def _return_predict(self, unique_trade_date, test_data, i, tech_indicator_list):\n",
    "\n",
    "        current_date = unique_trade_date[i]\n",
    "        next_date = unique_trade_date[i+1]\n",
    "\n",
    "        df_current = test_data[test_data.date ==\n",
    "                                  current_date].reset_index(drop=True)\n",
    "        df_next = test_data[test_data.date ==\n",
    "                               next_date].reset_index(drop=True)\n",
    "\n",
    "        tics = df_current['tic'].values\n",
    "        features = df_current[tech_indicator_list].values\n",
    "\n",
    "        predicted_y = self.model.predict(features)\n",
    "        mu = predicted_y\n",
    "        sigma = risk_models.sample_cov(\n",
    "            df_current.return_list[0], returns_data=True)\n",
    "\n",
    "        return mu, sigma, tics, df_current, df_next\n",
    "    \n",
    "    def _weight_optimization(self, i, unique_trade_date, meta_coefficient, mu, sigma, tics, portfolio, df_current, df_next):\n",
    "\n",
    "        current_date = unique_trade_date[i]\n",
    "        predicted_y_df = pd.DataFrame(\n",
    "            {\"tic\": tics.reshape(-1,), \"predicted_y\": mu.reshape(-1,)})\n",
    "        min_weight, max_weight = 0, 1\n",
    "\n",
    "        ef = EfficientFrontier(mu, sigma)\n",
    "        weights = ef.nonconvex_objective(\n",
    "            objective_functions.sharpe_ratio,\n",
    "            objective_args=(ef.expected_returns, ef.cov_matrix),\n",
    "            weights_sum_to_one=True,\n",
    "            constraints=[\n",
    "                # greater than min_weight\n",
    "                {\"type\": \"ineq\", \"fun\": lambda w: w - min_weight},\n",
    "                # less than max_weight\n",
    "                {\"type\": \"ineq\", \"fun\": lambda w: max_weight - w},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        weight_df = {\"tic\": [], \"weight\": []}\n",
    "        meta_coefficient[\"date\"] += [current_date]\n",
    "\n",
    "        for item in weights:\n",
    "            weight_df['tic'] += [item]\n",
    "            weight_df['weight'] += [weights[item]]\n",
    "\n",
    "        weight_df = pd.DataFrame(weight_df).merge(predicted_y_df, on=['tic'])\n",
    "        meta_coefficient[\"weights\"] += [weight_df]\n",
    "        cap = portfolio.iloc[0, i]\n",
    "        # current cash invested for each stock\n",
    "        current_cash = [element * cap for element in list(weights.values())]\n",
    "        # current held shares\n",
    "        current_shares = list(np.array(current_cash) / np.array(df_current.close))\n",
    "        # next time period price\n",
    "        next_price = np.array(df_next.close)\n",
    "        portfolio.iloc[0, i+1] = np.dot(current_shares, next_price)\n",
    "\n",
    "        return portfolio \n",
    "    \n",
    "    def save_model(self,  file_name):\n",
    "        with open(file_name, 'wb') as files:\n",
    "            pickle.dump(self.model, files)\n",
    "        print(\"Model saved succesfully.\")\n",
    "\n",
    "\n",
    "    def load_model(self, file_name):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            self.model = pickle.load(f)\n",
    "        print(\"Model loaded succesfully.\")\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRAgent(ConventionalAgent):\n",
    "\n",
    "   def __init__(self,\n",
    "                fit_intercept = True,\n",
    "                copy_X = True,\n",
    "                positive = False):  \n",
    "\n",
    "        self.model = LinearRegression(fit_intercept=fit_intercept,\n",
    "                                      copy_X= copy_X,\n",
    "                                      positive= positive)\n",
    "\n",
    "   def train_model(self, train_x, train_y, **train_params):\n",
    "        '''\n",
    "        *Trains the model*\n",
    "        Input: Train data x and train data y\n",
    "        Output: Linear Regression Model\n",
    "        '''\n",
    "        try:\n",
    "            trained_reg = self.model.fit(train_x, train_y, **train_params)\n",
    "            print(\"Model trained succesfully\")\n",
    "            return trained_reg\n",
    "        except Exception as e:\n",
    "            print(\"training unsuccessful\")\n",
    "    \n",
    "   def predict(self,\n",
    "               test_data, \n",
    "               initial_capital = 0,\n",
    "               tech_indicator_list = [\n",
    "                    \"macd\",\n",
    "                    \"boll_ub\",\n",
    "                    \"boll_lb\",\n",
    "                    \"rsi_30\",\n",
    "                    \"cci_30\",\n",
    "                    \"dx_30\",\n",
    "                    \"close_30_sma\",\n",
    "                    \"close_60_sma\",\n",
    "                ]):\n",
    "\n",
    "        meta_coefficient = {\"date\": [], \"weights\": []}\n",
    "        unique_trade_date = test_data.date.unique()\n",
    "        portfolio = pd.DataFrame(index=range(1), columns=unique_trade_date)\n",
    "        portfolio.loc[0, unique_trade_date[0]] = initial_capital\n",
    "\n",
    "        for i in range(len(unique_trade_date) - 1):\n",
    "            mu, sigma, tics, df_current, df_next = self._return_predict(\n",
    "                unique_trade_date, test_data, i, tech_indicator_list)\n",
    "\n",
    "            portfolio_value = self._weight_optimization(\n",
    "                i, unique_trade_date, meta_coefficient, mu, sigma, tics, portfolio, df_current, df_next)\n",
    "    \n",
    "        portfolio = portfolio_value\n",
    "        portfolio = portfolio.T\n",
    "        portfolio.columns = ['account_value']\n",
    "        portfolio = portfolio.reset_index()\n",
    "        portfolio.columns = ['date', 'account_value']\n",
    "\n",
    "        '''Backtest hasn't been implemented yet, hence commented.'''\n",
    "        #stats = backtest_stats(portfolio, value_col_name='account_value')\n",
    "        \n",
    "        portfolio_cumprod = (\n",
    "            portfolio.account_value.pct_change()+1).cumprod()-1\n",
    "\n",
    "        return portfolio, portfolio_cumprod, pd.DataFrame(meta_coefficient)\n",
    "\n",
    "   def _return_predict(self, unique_trade_date, test_data, i, tech_indicator_list):\n",
    "\n",
    "        current_date = unique_trade_date[i]\n",
    "        next_date = unique_trade_date[i+1]\n",
    "\n",
    "        df_current = test_data[test_data.date ==\n",
    "                                  current_date].reset_index(drop=True)\n",
    "        df_next = test_data[test_data.date ==\n",
    "                               next_date].reset_index(drop=True)\n",
    "\n",
    "        tics = df_current['tic'].values\n",
    "        features = df_current[tech_indicator_list].values\n",
    "\n",
    "        predicted_y = self.model.predict(features)\n",
    "        mu = predicted_y\n",
    "        sigma = risk_models.sample_cov(\n",
    "            df_current.return_list[0], returns_data=True)\n",
    "\n",
    "        return mu, sigma, tics, df_current, df_next\n",
    "\n",
    "   def _weight_optimization(self, i, unique_trade_date, meta_coefficient, mu, sigma, tics, portfolio, df_current, df_next):\n",
    "\n",
    "        current_date = unique_trade_date[i]\n",
    "        predicted_y_df = pd.DataFrame(\n",
    "            {\"tic\": tics.reshape(-1,), \"predicted_y\": mu.reshape(-1,)})\n",
    "        min_weight, max_weight = 0, 1\n",
    "\n",
    "        ef = EfficientFrontier(mu, sigma)\n",
    "        weights = ef.nonconvex_objective(\n",
    "            objective_functions.sharpe_ratio,\n",
    "            objective_args=(ef.expected_returns, ef.cov_matrix),\n",
    "            weights_sum_to_one=True,\n",
    "            constraints=[\n",
    "                # greater than min_weight\n",
    "                {\"type\": \"ineq\", \"fun\": lambda w: w - min_weight},\n",
    "                # less than max_weight\n",
    "                {\"type\": \"ineq\", \"fun\": lambda w: max_weight - w},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        weight_df = {\"tic\": [], \"weight\": []}\n",
    "        meta_coefficient[\"date\"] += [current_date]\n",
    "\n",
    "        for item in weights:\n",
    "            weight_df['tic'] += [item]\n",
    "            weight_df['weight'] += [weights[item]]\n",
    "\n",
    "        weight_df = pd.DataFrame(weight_df).merge(predicted_y_df, on=['tic'])\n",
    "        meta_coefficient[\"weights\"] += [weight_df]\n",
    "        cap = portfolio.iloc[0, i]\n",
    "        # current cash invested for each stock\n",
    "        current_cash = [element * cap for element in list(weights.values())]\n",
    "        # current held shares\n",
    "        current_shares = list(np.array(current_cash) / np.array(df_current.close))\n",
    "        # next time period price\n",
    "        next_price = np.array(df_next.close)\n",
    "        portfolio.iloc[0, i+1] = np.dot(current_shares, next_price)\n",
    "\n",
    "        return portfolio \n",
    "\n",
    "   def save_model(self,  file_name):\n",
    "        with open(file_name, 'wb') as files:\n",
    "            pickle.dump(self.model, files)\n",
    "        print(\"Model saved succesfully.\")\n",
    "\n",
    "\n",
    "   def load_model(self, file_name):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            self.model = pickle.load(f)\n",
    "        print(\"Model loaded succesfully.\")\n",
    "        return self.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortfolioEnv(Environment):\n",
    "\n",
    "    def __init__(self,\n",
    "                 df: pd.DataFrame,  # input data\n",
    "                 stock_dim: int,  # number of unique securities in the investment universe\n",
    "                 hmax: float,  # maximum number of shares to trade\n",
    "                 initial_amount: float,  # initial cash value\n",
    "                 transaction_cost_pct: float,  # transaction cost percentage per trade\n",
    "                 reward_scaling: float,  # scaling factor for reward as training progresses\n",
    "                 state_space: int,  # the dimension of input features (state space)\n",
    "                 action_space: int,  # number of actions, which is equal to portfolio dimension\n",
    "                 tech_indicator_list: list,  # a list of technical indicator names\n",
    "                 lookback=252,  #\n",
    "                 day=0):  # an increment number to control date\n",
    "\n",
    "        self.df = df\n",
    "        self.day = day\n",
    "        self.lookback = lookback\n",
    "        self.stock_dim = stock_dim\n",
    "        self.hmax = hmax\n",
    "        self.initial_amount = initial_amount\n",
    "        self.transaction_cost_pct = transaction_cost_pct\n",
    "        self.reward_scaling = reward_scaling\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.tech_indicator_list = tech_indicator_list\n",
    "\n",
    "        # action_space normalization and shape is self.stock_dim\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(self.action_space,))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf,\n",
    "                                            shape=(self.state_space + len(self.tech_indicator_list), self.state_space))\n",
    "        \n",
    "\n",
    "        # load data from a pandas dataframe\n",
    "        \n",
    "        \n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        self.covs = self.data['cov_list'].values[0]\n",
    "        self.state =  np.append(np.array(self.covs), [self.data[tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)\n",
    "        self.terminal = False\n",
    "        #self.turbulence_threshold = turbulence_threshold\n",
    "        # initalize state: initial portfolio return + individual stock return + individual weights\n",
    "        self.portfolio_value = self.initial_amount\n",
    "\n",
    "        # memorize portfolio value each step\n",
    "        self.asset_memory = [self.initial_amount]\n",
    "        # memorize portfolio return each step\n",
    "        self.portfolio_return_memory = [0]\n",
    "        self.actions_memory = [[1 / self.stock_dim] * self.stock_dim]\n",
    "        self.date_memory=[self.data.date.unique()[0]]\n",
    "\n",
    "    def reset(self):\n",
    "        self.asset_memory = [self.initial_amount]\n",
    "        self.day = 0\n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        # load states\n",
    "        self.covs = self.data['cov_list'].values[0]\n",
    "        self.state =  np.append(np.array(self.covs), [self.data[tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)\n",
    "                \n",
    "        self.portfolio_value = self.initial_amount\n",
    "        self.terminal = False \n",
    "        self.portfolio_return_memory = [0]\n",
    "        self.actions_memory=[[1/self.stock_dim] * self.stock_dim]\n",
    "        self.date_memory=[self.data.date.unique()[0]] \n",
    "        return self.state\n",
    "\n",
    "    def step(self, actions):\n",
    "        self.terminal = self.day >= len(self.df.index.unique()) - 1\n",
    "        if self.terminal:\n",
    "            df = pd.DataFrame(self.portfolio_return_memory)\n",
    "            df.columns = ['daily_return']\n",
    "            print(\"=================================\")\n",
    "            print(\"begin_total_asset:{}\".format(self.asset_memory[0]))\n",
    "            print(\"end_total_asset:{}\".format(self.portfolio_value))\n",
    "\n",
    "            df_daily_return = pd.DataFrame(self.portfolio_return_memory)\n",
    "            df_daily_return.columns = ['daily_return']\n",
    "            if df_daily_return['daily_return'].std() != 0:\n",
    "                sharpe = (252 ** 0.5) * df_daily_return['daily_return'].mean() / \\\n",
    "                         df_daily_return['daily_return'].std()\n",
    "                print(\"Sharpe: \", sharpe)\n",
    "            print(\"=================================\")\n",
    "\n",
    "            return self.state, self.reward, self.terminal, {}\n",
    "\n",
    "        else:\n",
    "            weights = Environment.softmax_normalization(actions)\n",
    "            self.actions_memory.append(weights)\n",
    "            transaction_fee = self.transaction_cost_pct * self.asset_memory[-1] * sum([abs(a_i - b_i) for a_i, b_i in zip(self.actions_memory[-1], self.actions_memory[-2])]) #transaction_fee\n",
    "            last_day_memory = self.data\n",
    "            # load next state\n",
    "            self.day += 1\n",
    "            self.data = self.df.loc[self.day, :]\n",
    "            self.covs = self.data['cov_list'].values[0]\n",
    "            self.state =  np.append(np.array(self.covs), [self.data[tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)         \n",
    "            portfolio_return = sum(((self.data.close.values / last_day_memory.close.values) - 1) * weights)\n",
    "            # update portfolio value\n",
    "            new_portfolio_value = self.portfolio_value * (1 + portfolio_return) - transaction_fee #transaction_fee\n",
    "            self.portfolio_value = new_portfolio_value\n",
    "\n",
    "            # save into memory\n",
    "            self.portfolio_return_memory.append(portfolio_return)\n",
    "            self.date_memory.append(self.data[\"date\"].unique()[0])\n",
    "            self.asset_memory.append(new_portfolio_value)\n",
    "\n",
    "            # the reward is the new portfolio value or end portfolo value\n",
    "            self.reward = new_portfolio_value\n",
    "\n",
    "        return self.state, self.reward, self.terminal, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        return self.state\n",
    "\n",
    "    def save_asset_memory(self):\n",
    "        date_list = self.date_memory\n",
    "        portfolio_return = self.portfolio_return_memory\n",
    "        df_account_value = pd.DataFrame({'date': date_list, 'daily_return': portfolio_return})\n",
    "        return df_account_value\n",
    "\n",
    "    def save_action_memory(self):\n",
    "        # date and close price length must match actions length\n",
    "        date_list = self.date_memory\n",
    "        df_date = pd.DataFrame(date_list)\n",
    "        df_date.columns = ['date']\n",
    "\n",
    "        action_list = self.actions_memory\n",
    "        df_actions = pd.DataFrame(action_list)\n",
    "        df_actions.columns = self.data.tic.values\n",
    "        df_actions.index = df_date.date\n",
    "        # df_actions = pd.DataFrame({'date':date_list,'actions':action_list})\n",
    "        return df_actions\n",
    "\n",
    "    def get_env(self):\n",
    "        e = DummyVecEnv([lambda: self])\n",
    "        obs = e.reset()\n",
    "        return e, obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C(RLAgent):\n",
    "\n",
    "    def __init__(self,\n",
    "                 policy: \"MlpPolicy\",\n",
    "                 env: None,\n",
    "                 learning_rate: float = 7e-4,\n",
    "                 n_steps: int = 5,\n",
    "                 gamma: float = 0.99,\n",
    "                 gae_lambda: float = 1.0,\n",
    "                 ent_coef: float = 0.0,\n",
    "                 vf_coef: float = 0.5,\n",
    "                 max_grad_norm: float = 0.5,\n",
    "                 rms_prop_eps: float = 1e-5,\n",
    "                 use_rms_prop: bool = True,\n",
    "                 use_sde: bool = False,\n",
    "                 sde_sample_freq: int = -1,\n",
    "                 normalize_advantage: bool = False,\n",
    "                 tensorboard_log: Optional[str] = None,\n",
    "                 create_eval_env: bool = False,\n",
    "                 policy_kwargs: Optional[Dict[str, Any]] = None,\n",
    "                 verbose: int = 0,\n",
    "                 seed: Optional[int] = None,\n",
    "                 device: Union[th.device, str] = \"auto\",\n",
    "                 _init_setup_model: bool = True):\n",
    "\n",
    "        self.env = env\n",
    "\n",
    "        self.model = sb_A2C(policy = policy,\n",
    "                            env=self.env,\n",
    "                            learning_rate = learning_rate,\n",
    "                            n_steps = n_steps,\n",
    "                            gamma = gamma,\n",
    "                            gae_lambda= gae_lambda,\n",
    "                            ent_coef = ent_coef,\n",
    "                            vf_coef = vf_coef,\n",
    "                            max_grad_norm = max_grad_norm,\n",
    "                            rms_prop_eps= rms_prop_eps,\n",
    "                            use_rms_prop= use_rms_prop,\n",
    "                            use_sde= use_sde,\n",
    "                            sde_sample_freq= sde_sample_freq,\n",
    "                            normalize_advantage= normalize_advantage,\n",
    "                            tensorboard_log=tensorboard_log,  \n",
    "                            create_eval_env= create_eval_env,\n",
    "                            policy_kwargs=policy_kwargs,\n",
    "                            verbose=verbose,\n",
    "                            seed=seed,\n",
    "                            device= device,\n",
    "                            _init_setup_model = _init_setup_model)\n",
    "\n",
    "    def train_model(self, **train_params):\n",
    "        self.model = self.model.learn(**train_params)\n",
    "        return self.model\n",
    "\n",
    "    def predict(self, environment, **test_params):\n",
    "\n",
    "        env_test, obs_test = environment.get_env()\n",
    "        \"\"\"make a prediction\"\"\"\n",
    "        account_memory = []\n",
    "        actions_memory = []\n",
    "\n",
    "        env_test.reset()\n",
    "        for i in range(len(environment.df.index.unique())):\n",
    "            action, _states = self.model.predict(obs_test, **test_params)\n",
    "            obs_test, rewards, dones, info = env_test.step(action)\n",
    "            if i == (len(environment.df.index.unique()) - 2):\n",
    "                account_memory = env_test.env_method(method_name=\"save_asset_memory\")\n",
    "                actions_memory = env_test.env_method(method_name=\"save_action_memory\")\n",
    "            if dones[0]:\n",
    "                print(\"hit end!\")\n",
    "                break\n",
    "\n",
    "        return account_memory[0], actions_memory[0]\n",
    "\n",
    "    def save_model(self, path):\n",
    "        self.model.save(path)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        self.model = self.model.load(path)\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO(RLAgent):\n",
    "    def __init__(self,\n",
    "                policy: \"MlpPolicy\",\n",
    "                env: None,\n",
    "                learning_rate:  3e-4,\n",
    "                n_steps: int = 2048,\n",
    "                batch_size: int = 64,\n",
    "                n_epochs: int = 10,\n",
    "                gamma: float = 0.99,\n",
    "                gae_lambda: float = 0.95,\n",
    "                clip_range: Union[float, Schedule] = 0.2,\n",
    "                clip_range_vf: Union[None, float, Schedule] = None,\n",
    "                normalize_advantage: bool = True,\n",
    "                ent_coef: float = 0.0,\n",
    "                vf_coef: float = 0.5,\n",
    "                max_grad_norm: float = 0.5,\n",
    "                use_sde: bool = False,\n",
    "                sde_sample_freq: int = -1,\n",
    "                target_kl: Optional[float] = None,\n",
    "                tensorboard_log: Optional[str] = None,\n",
    "                create_eval_env: bool = False,\n",
    "                policy_kwargs: Optional[Dict[str, Any]] = None,\n",
    "                verbose: int = 0,\n",
    "                seed: Optional[int] = None,\n",
    "                device: Union[th.device, str] = \"auto\",\n",
    "                _init_setup_model: bool = True):\n",
    "\n",
    "        self.env = env\n",
    "        self.model = sb_PPO(policy = policy,\n",
    "                            env=self.env,\n",
    "                            learning_rate = learning_rate,\n",
    "                            n_steps = n_steps,\n",
    "                            gamma = gamma,\n",
    "                            batch_size = batch_size,\n",
    "                            n_epochs = n_epochs,\n",
    "                            gae_lambda=gae_lambda,\n",
    "                            clip_range = clip_range,\n",
    "                            clip_range_vf = clip_range_vf,\n",
    "                            normalize_advantage=normalize_advantage,\n",
    "                            ent_coef=ent_coef,\n",
    "                            vf_coef=vf_coef,\n",
    "                            max_grad_norm=max_grad_norm,\n",
    "                            use_sde=use_sde,\n",
    "                            sde_sample_freq=sde_sample_freq,\n",
    "                            target_kl=target_kl,\n",
    "                            tensorboard_log=tensorboard_log,\n",
    "                            create_eval_env=create_eval_env,\n",
    "                            policy_kwargs=policy_kwargs,\n",
    "                            verbose=verbose,\n",
    "                            seed=seed,\n",
    "                            device=device,\n",
    "                            _init_setup_model = _init_setup_model)\n",
    "    \n",
    "    def train_model(self, **train_params):\n",
    "        self.model = self.model.learn(**train_params)\n",
    "        return self.model\n",
    "    \n",
    "    def predict(self, environment, **test_params):\n",
    "        env_test, obs_test = environment.get_env()\n",
    "        \"\"\"make a prediction\"\"\"\n",
    "        account_memory = []\n",
    "        actions_memory = []\n",
    "\n",
    "        env_test.reset()\n",
    "        for i in range(len(environment.df.index.unique())):\n",
    "            action, _states = self.model.predict(obs_test, **test_params)\n",
    "            obs_test, rewards, dones, info = env_test.step(action)\n",
    "            if i == (len(environment.df.index.unique()) - 2):\n",
    "                account_memory = env_test.env_method(method_name=\"save_asset_memory\")\n",
    "                actions_memory = env_test.env_method(method_name=\"save_action_memory\")\n",
    "            if dones[0]:\n",
    "                print(\"hit end!\")\n",
    "                break\n",
    "\n",
    "        return account_memory[0], actions_memory[0]\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        self.model = self.model.load(path)\n",
    "        return self.model\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        self.model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG(RLAgent):\n",
    "    def __init__(self,\n",
    "                policy: \"MlpPolicy\",\n",
    "                env: None,\n",
    "                learning_rate : 1e-3,\n",
    "                buffer_size: 1_000_000,  # 1e6\n",
    "                learning_starts: 100,\n",
    "                batch_size:  100,\n",
    "                tau:  0.005,\n",
    "                gamma:  0.99,\n",
    "                train_freq:  1,\n",
    "                gradient_steps: int = -1,\n",
    "                action_noise: Optional[ActionNoise] = None,\n",
    "                replay_buffer_class: Optional[ReplayBuffer] = None,\n",
    "                replay_buffer_kwargs: Optional[Dict[str, Any]] = None,\n",
    "                optimize_memory_usage: bool = False,\n",
    "                tensorboard_log: Optional[str] = None,\n",
    "                create_eval_env: bool = False,\n",
    "                policy_kwargs: Optional[Dict[str, Any]] = None,\n",
    "                verbose: int = 0,\n",
    "                seed: Optional[int] = None,\n",
    "                device: Union[th.device, str] = \"auto\",\n",
    "                _init_setup_model: bool = True):\n",
    "                \n",
    "        self.env = env\n",
    "    \n",
    "        self.model = sb_DDPG(policy = policy,\n",
    "                            env=self.env,\n",
    "                            learning_rate = learning_rate,\n",
    "                            buffer_size = buffer_size,\n",
    "                            learning_starts= learning_starts,\n",
    "                            batch_size = batch_size,\n",
    "                            tau = tau,\n",
    "                            gamma= gamma,\n",
    "                            train_freq = train_freq,\n",
    "                            gradient_steps = gradient_steps,\n",
    "                            action_noise= action_noise,\n",
    "                            replay_buffer_class= replay_buffer_class,\n",
    "                            replay_buffer_kwargs= replay_buffer_kwargs,\n",
    "                            optimize_memory_usage=optimize_memory_usage,\n",
    "                            tensorboard_log=tensorboard_log,\n",
    "                            create_eval_env=create_eval_env,\n",
    "                            policy_kwargs=policy_kwargs,\n",
    "                            verbose=verbose,\n",
    "                            seed=seed,\n",
    "                            device=device,\n",
    "                            _init_setup_model = _init_setup_model)\n",
    "\n",
    "    def train_model(self, **train_params):\n",
    "        self.model = self.model.learn(**train_params)\n",
    "        return self.model\n",
    "\n",
    "    def predict(self, environment, **test_params):\n",
    "        env_test, obs_test = environment.get_env()\n",
    "        \"\"\"make a prediction\"\"\"\n",
    "        account_memory = []\n",
    "        actions_memory = []\n",
    "\n",
    "        env_test.reset()\n",
    "        for i in range(len(environment.df.index.unique())):\n",
    "            action, _states = self.model.predict(obs_test, **test_params)\n",
    "            obs_test, rewards, dones, info = env_test.step(action)\n",
    "            if i == (len(environment.df.index.unique()) - 2):\n",
    "                account_memory = env_test.env_method(method_name=\"save_asset_memory\")\n",
    "                actions_memory = env_test.env_method(method_name=\"save_action_memory\")\n",
    "            if dones[0]:\n",
    "                print(\"hit end!\")\n",
    "                break\n",
    "\n",
    "        return account_memory[0], actions_memory[0]\n",
    "        \n",
    "    def load_model(self, path):\n",
    "        self.model = self.model.load(path)\n",
    "        return self.model\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        self.model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3(RLAgent):\n",
    " \n",
    "    def __init__(self,\n",
    "                policy: \"MlpPolicy\",\n",
    "                env: None,\n",
    "                learning_rate: float =  1e-3,\n",
    "                buffer_size: int = 1_000_000,  # 1e6\n",
    "                learning_starts: int = 100,\n",
    "                batch_size: int = 100,\n",
    "                tau: float = 0.005,\n",
    "                gamma: float = 0.99,\n",
    "                train_freq: int = 1,\n",
    "                gradient_steps: int = -1,\n",
    "                action_noise: Optional[ActionNoise] = None,\n",
    "                replay_buffer_class: Optional[ReplayBuffer] = None,\n",
    "                replay_buffer_kwargs: Optional[Dict[str, Any]] = None,\n",
    "                optimize_memory_usage: bool = False,\n",
    "                tensorboard_log: Optional[str] = None,\n",
    "                create_eval_env: bool = False,\n",
    "                policy_kwargs: Optional[Dict[str, Any]] = None,\n",
    "                verbose: int = 0,\n",
    "                seed: Optional[int] = None,\n",
    "                device: Union[th.device, str] = \"auto\",\n",
    "                _init_setup_model: bool = True):\n",
    "                \n",
    "        self.env = env\n",
    "    \n",
    "        self.model = sb_TD3(policy = policy,\n",
    "                            env=self.env,\n",
    "                            learning_rate = learning_rate,\n",
    "                            buffer_size = buffer_size,\n",
    "                            learning_starts= learning_starts,\n",
    "                            batch_size = batch_size,\n",
    "                            tau = tau,\n",
    "                            gamma= gamma,\n",
    "                            train_freq = train_freq,\n",
    "                            gradient_steps = gradient_steps,\n",
    "                            action_noise= action_noise,\n",
    "                            replay_buffer_class= replay_buffer_class,\n",
    "                            replay_buffer_kwargs= replay_buffer_kwargs,\n",
    "                            optimize_memory_usage=optimize_memory_usage,\n",
    "                            tensorboard_log=tensorboard_log,\n",
    "                            create_eval_env=create_eval_env,\n",
    "                            policy_kwargs=policy_kwargs,\n",
    "                            verbose=verbose,\n",
    "                            seed=seed,\n",
    "                            device=device,\n",
    "                            _init_setup_model = _init_setup_model)\n",
    "\n",
    "    def train_model(self, **train_params):\n",
    "        self.model = self.model.learn(**train_params)\n",
    "        return self.model\n",
    "\n",
    "    def predict(self, environment, **test_params):\n",
    "        env_test, obs_test = environment.get_env()\n",
    "        \"\"\"make a prediction\"\"\"\n",
    "        account_memory = []\n",
    "        actions_memory = []\n",
    "\n",
    "        env_test.reset()\n",
    "        for i in range(len(environment.df.index.unique())):\n",
    "            action, _states = self.model.predict(obs_test, **test_params)\n",
    "            obs_test, rewards, dones, info = env_test.step(action)\n",
    "            if i == (len(environment.df.index.unique()) - 2):\n",
    "                account_memory = env_test.env_method(method_name=\"save_asset_memory\")\n",
    "                actions_memory = env_test.env_method(method_name=\"save_action_memory\")\n",
    "            if dones[0]:\n",
    "                print(\"hit end!\")\n",
    "                break\n",
    "\n",
    "        return account_memory[0], actions_memory[0]\n",
    "        \n",
    "    def load_model(self, path):\n",
    "        self.model = self.model.load(path)\n",
    "        return self.model\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        self.model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gather user parameters\n",
    "with open(\"../user_params.yaml\", \"r\") as stream:\n",
    "    try:\n",
    "        user_params = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = user_params[\"TICKERS\"]\n",
    "env_kwargs = user_params[\"ENV_PARAMS\"]\n",
    "train_params = user_params[\"TRAIN_PARAMS\"]\n",
    "policy_params = user_params[\"POLICY_PARAMS\"]\n",
    "test_params = user_params[\"TEST_PARAMS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 3: Downloading from Yahoo.........\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Shape of DataFrame:  (12924, 8)\n",
      "        date       open       high        low      close     volume   tic  day\n",
      "0 2008-12-31   3.070357   3.133571   3.047857   2.606278  607541200  AAPL    2\n",
      "1 2008-12-31  41.590000  43.049999  41.500000  32.005898    5443100    BA    2\n",
      "2 2008-12-31  43.700001  45.099998  43.700001  30.628826    6277400   CAT    2\n",
      "3 2008-12-31  72.900002  74.629997  72.900002  43.314426    9964300   CVX    2\n",
      "4 2009-01-02   3.067143   3.251429   3.041429   2.771174  746015200  AAPL    4\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTest 3: Downloading from Yahoo.........\")\n",
    "downloaded_df = DataDownloader(start_date='2009-01-01',\n",
    "                                end_date='2021-10-31',\n",
    "                                ticker_list= tickers).download_from_yahoo()\n",
    "print(downloaded_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 4: Feature engineer.........\n",
      "Successfully added technical indicators\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Shape of DataFrame:  (3231, 8)\n",
      "Successfully added vix\n",
      "Successfully added turbulence index\n",
      "Successfully added covariances\n",
      "Successfully added returns\n"
     ]
    }
   ],
   "source": [
    " # PREPROCESS DATA\n",
    "print(\"\\nTest 4: Feature engineer.........\")\n",
    "data_processor = DefaultFeatureEngineer(use_default=False,\n",
    "                                        tech_indicator_list=env_kwargs[\"tech_indicator_list\"],\n",
    "                                        use_vix=True,\n",
    "                                        use_turbulence=True,\n",
    "                                        use_covar=True)\n",
    "# included technical indicators as features\n",
    "df_processed = data_processor.extend_data(downloaded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data to train and test\n",
    "splitter = TimeSeriesSplitter()\n",
    "train = splitter.get_split_data(df_processed, '2009-01-01', '2020-06-30')\n",
    "trade = splitter.get_split_data(df_processed, '2020-07-01', '2021-09-02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PortfolioEnv(df=train, **env_kwargs)\n",
    "env_train, _ = env.get_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo = TD3(env=env_train, **policy_params[\"TD3_PARAMS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.td3.td3.TD3 at 0x1561bc5e0>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo.train_model(**train_params[\"TD3_PARAMS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test = PortfolioEnv(df=trade, **env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:1468535.9864424637\n",
      "Sharpe:  1.3708750992495065\n",
      "=================================\n",
      "hit end!\n"
     ]
    }
   ],
   "source": [
    "df_daily_return_ppo, df_actions_ppo = ppo.predict(environment=env_test, **test_params[\"TD3_PARAMS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1ee38ef4a5a9feb55287fd749643f13d043cb0a7addaab2a9c224cbe137c0062"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
