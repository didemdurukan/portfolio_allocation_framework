{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/doganparlak/Desktop/Master_2.2/Master_Project/uniFi_github/uniFi/AgentLayer', '/usr/local/Cellar/python@3.8/3.8.8_1/Frameworks/Python.framework/Versions/3.8/lib/python38.zip', '/usr/local/Cellar/python@3.8/3.8.8_1/Frameworks/Python.framework/Versions/3.8/lib/python3.8', '/usr/local/Cellar/python@3.8/3.8.8_1/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload', '', '/usr/local/lib/python3.8/site-packages', '/usr/local/lib/python3.8/site-packages/selenium-3.141.0-py3.8.egg', '/usr/local/lib/python3.8/site-packages/urllib3-1.26.4-py3.8.egg', '/Users/doganparlak/Library/Python/3.8/lib/python/site-packages', '/Users/doganparlak/Desktop/Master_2.2/Master_Project/uniFi_github/uniFi', '/Users/doganparlak/Desktop/Master_2.2/Master_Project/uniFi_github/uniFi', '/Users/doganparlak/Desktop/Master_2.2/Master_Project/uniFi_github/uniFi', '/Users/doganparlak/Desktop/Master_2.2/Master_Project/uniFi_github/uniFi']\n"
     ]
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import A2C as sb_A2C\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import gym\n",
    "from datetime import datetime\n",
    "from sys import path\n",
    "from os.path import dirname as dir\n",
    "\n",
    "path.append(dir(path[0]))\n",
    "print(path)\n",
    "#__package__ = \"examples\"\n",
    "\n",
    "from FinancialEnvLayer.datacollector import CustomDatasetImporter\n",
    "from FinancialEnvLayer.datacollector import DataDownloader\n",
    "from FinancialEnvLayer.dataprocessor import FeatureEngineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(gym.Env, ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def step(self, action):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def render(self, mode=\"human\"):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_env(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax_normalization(actions):\n",
    "        numerator = np.exp(actions)\n",
    "        denominator = np.sum(np.exp(actions))\n",
    "        softmax_output = numerator / denominator\n",
    "        return softmax_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_model():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save_model():\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def load_model():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConventionalAgent(Agent, ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_model():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save_model():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_model():\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _return_predict():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _weight_optimization():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLAgent(Agent, ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_model():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict():\n",
    "        \n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save_model():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_model():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortfolioEnv(Environment):\n",
    "\n",
    "    def __init__(self,\n",
    "                 df: pd.DataFrame,  # input data\n",
    "                 port_dim: int,  # number of unique securities in the investment universe\n",
    "                 hmax: float,  # maximum number of shares to trade\n",
    "                 initial_amount: float,  # initial cash value\n",
    "                 transaction_cost_pct: float,  # transaction cost percentage per trade\n",
    "                 reward_scaling: float,  # scaling factor for reward as training progresses\n",
    "                 state_space: int,  # the dimension of input features (state space)\n",
    "                 action_space: int,  # number of actions, which is equal to portfolio dimension\n",
    "                 tech_indicator_list: list,  # a list of technical indicator names\n",
    "                 turbulence_threshold=None,  # a threshold to control risk aversion\n",
    "                 lookback=252,  #\n",
    "                 day=0):  # an increment number to control date\n",
    "\n",
    "        self.df = df\n",
    "        self.day = day\n",
    "        self.lookback = lookback\n",
    "        self.port_dim = port_dim\n",
    "        self.hmax = hmax\n",
    "        self.initial_amount = initial_amount\n",
    "        self.transaction_cost_pct = transaction_cost_pct\n",
    "        self.reward_scaling = reward_scaling\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.tech_indicator_list = tech_indicator_list\n",
    "\n",
    "        # action_space normalization and shape is self.stock_dim\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(self.action_space,))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf,\n",
    "                                            shape=(self.state_space + len(self.tech_indicator_list), self.state_space))\n",
    "        \n",
    "\n",
    "        # load data from a pandas dataframe\n",
    "        print(self.df.head(1))\n",
    "        self.data = self.df.loc[self.day, :]\n",
    "        print(self.data.head(1))\n",
    "        self.covs = self.data['cov_list'][0]\n",
    "        #print(self.data[tech])\n",
    "        self.state = np.append(np.array(self.covs), [self.data[tech] for tech in self.tech_indicator_list],\n",
    "                            axis=0)\n",
    "        self.terminal = False\n",
    "        #self.turbulence_threshold = turbulence_threshold\n",
    "        # initalize state: initial portfolio return + individual stock return + individual weights\n",
    "        self.portfolio_value = self.initial_amount\n",
    "\n",
    "        # memorize portfolio value each step\n",
    "        self.asset_memory = [self.initial_amount]\n",
    "        # memorize portfolio return each step\n",
    "        self.portfolio_return_memory = [0]\n",
    "        self.actions_memory = [[1 / self.port_dim] * self.port_dim]\n",
    "        self.date_memory = [self.data[\"date\"]]\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def step(self, actions):\n",
    "        self.terminal = self.day >= len(self.df.index.unique()) - 1\n",
    "\n",
    "        if self.terminal:\n",
    "            df = pd.DataFrame(self.portfolio_return_memory)\n",
    "            df.columns = ['daily_return']\n",
    "            plt.plot(df.daily_return.cumsum(), 'r')\n",
    "            plt.savefig('results/cumulative_reward.png')\n",
    "            plt.close()\n",
    "\n",
    "            plt.plot(self.portfolio_return_memory, 'r')\n",
    "            plt.savefig('results/rewards.png')\n",
    "            plt.close()\n",
    "\n",
    "            print(\"=================================\")\n",
    "            print(\"begin_total_asset:{}\".format(self.asset_memory[0]))\n",
    "            print(\"end_total_asset:{}\".format(self.portfolio_value))\n",
    "\n",
    "            df_daily_return = pd.DataFrame(self.portfolio_return_memory)\n",
    "            df_daily_return.columns = ['daily_return']\n",
    "            if df_daily_return['daily_return'].std() != 0:\n",
    "                sharpe = (252 ** 0.5) * df_daily_return['daily_return'].mean() / \\\n",
    "                         df_daily_return['daily_return'].std()\n",
    "                print(\"Sharpe: \", sharpe)\n",
    "            print(\"=================================\")\n",
    "\n",
    "            return self.state, self.reward, self.terminal, {}\n",
    "\n",
    "        else:\n",
    "            weights = Environment.softmax_normalization(actions)\n",
    "            self.actions_memory.append(weights)\n",
    "            last_day_memory = self.data\n",
    "\n",
    "            # load next state\n",
    "            self.day += 1\n",
    "            self.data = self.df.loc[self.day, :]\n",
    "            self.covs = self.data['cov_list'][0]\n",
    "            self.state = np.append(np.array(self.covs), [self.data[tech] for tech in self.tech_indicator_list],\n",
    "                                   axis=0)\n",
    "            portfolio_return = sum(((self.data.close.values / last_day_memory.close.values) - 1) * weights)\n",
    "            log_portfolio_return = np.log(sum((self.data.close.values / last_day_memory.close.values) * weights))\n",
    "            # update portfolio value\n",
    "            new_portfolio_value = self.portfolio_value * (1 + portfolio_return)\n",
    "            self.portfolio_value = new_portfolio_value\n",
    "\n",
    "            # save into memory\n",
    "            self.portfolio_return_memory.append(portfolio_return)\n",
    "            self.date_memory.append(self.data[\"date\"].unique()[0])\n",
    "            self.asset_memory.append(new_portfolio_value)\n",
    "\n",
    "            # the reward is the new portfolio value or end portfolo value\n",
    "            self.reward = new_portfolio_value\n",
    "\n",
    "        return self.state, self.reward, self.terminal, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        return self.state\n",
    "\n",
    "    def save_asset_memory(self):\n",
    "        date_list = self.date_memory\n",
    "        portfolio_return = self.portfolio_return_memory\n",
    "        # print(len(date_list))\n",
    "        # print(len(asset_list))\n",
    "        df_account_value = pd.DataFrame({'date': date_list, 'daily_return': portfolio_return})\n",
    "        return df_account_value\n",
    "\n",
    "    def save_action_memory(self):\n",
    "        # date and close price length must match actions length\n",
    "        date_list = self.date_memory\n",
    "        df_date = pd.DataFrame(date_list)\n",
    "        df_date.columns = ['date']\n",
    "\n",
    "        action_list = self.actions_memory\n",
    "        df_actions = pd.DataFrame(action_list)\n",
    "        df_actions.columns = self.data.tic.values\n",
    "        df_actions.index = df_date.date\n",
    "        # df_actions = pd.DataFrame({'date':date_list,'actions':action_list})\n",
    "        return df_actions\n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def get_env(self):\n",
    "        e = DummyVecEnv([lambda: self])\n",
    "        obs = e.reset()\n",
    "        return e, obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C(RLAgent):\n",
    "\n",
    "    def __init__(self,\n",
    "                 policy=\"MlpPolicy\",\n",
    "                 env=None,\n",
    "                 learning_rate: float = 7e-4,\n",
    "                 n_steps: int = 5,\n",
    "                 gamma: float = 0.99,\n",
    "                 gae_lambda: float = 1.0,\n",
    "                 ent_coef: float = 0.0,\n",
    "                 vf_coef: float = 0.5,\n",
    "                 max_grad_norm: float = 0.5,\n",
    "                 rms_prop_eps: float = 1e-5,\n",
    "                 use_rms_prop: bool = True,\n",
    "                 use_sde: bool = False,\n",
    "                 sde_sample_freq: int = -1,\n",
    "                 normalize_advantage: bool = False,\n",
    "                 tensorboard_log=None,\n",
    "                 create_eval_env: bool = False,\n",
    "                 policy_kwargs=None,\n",
    "                 verbose: int = 0,\n",
    "                 seed=None,\n",
    "                 device=\"auto\",\n",
    "                 _init_setup_model: bool = True):\n",
    "\n",
    "        self.env = env\n",
    "        # self.model = A2C(model_params[\"policy\"], model_params[\"environment\"], model_params[\"verbose\"])\n",
    "        self.model = sb_A2C(policy=policy,\n",
    "                            env=self.env,\n",
    "                            tensorboard_log=tensorboard_log,\n",
    "                            verbose=verbose,\n",
    "                            policy_kwargs=policy_kwargs,\n",
    "                            seed=seed,\n",
    "                            **model_kwargs)\n",
    "\n",
    "    def train_model(self, **train_params):\n",
    "        self.model = self.model.learn(total_timesteps=train_params[\"total_timesteps\"],\n",
    "                                      log_interval=train_params[\"log_interval\"])\n",
    "        return self.model\n",
    "\n",
    "    def predict(self, **test_params):\n",
    "\n",
    "        test_env, test_obs = test_params[\"environment\"].environment()\n",
    "        \"\"\"make a prediction\"\"\"\n",
    "        account_memory = []\n",
    "        actions_memory = []\n",
    "\n",
    "        test_env.reset()\n",
    "        for i in range(len(test_params[\"environment\"].df.index.unique())):\n",
    "            action, _states = self.model.predict(test_obs, deterministic=test_params[\"deterministic\"])\n",
    "            test_obs, rewards, dones, info = test_env.step(action)\n",
    "            if i == (len(test_params[\"environment\"].df.index.unique()) - 2):\n",
    "                account_memory = test_env.env_method(method_name=\"save_asset_memory\")\n",
    "                actions_memory = test_env.env_method(method_name=\"save_action_memory\")\n",
    "            if dones[0]:\n",
    "                print(\"hit end!\")\n",
    "                break\n",
    "\n",
    "        return account_memory[0], actions_memory[0]\n",
    "\n",
    "    def save_model(self, path):\n",
    "        self.model.save(path)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        self.model = self.model.load(path)\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TECHNICAL_INDICATORS_LIST = [\n",
    "    \"macd\",\n",
    "    \"boll_ub\",\n",
    "    \"boll_lb\",\n",
    "    \"rsi_30\",\n",
    "    \"cci_30\",\n",
    "    \"dx_30\",\n",
    "    \"close_30_sma\",\n",
    "    \"close_60_sma\",\n",
    "]\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100, #maximum number of shares to trade\n",
    "    \"initial_amount\": 1000000, # initial cash\n",
    "    \"transaction_cost_pct\": 0.001, # transaction cost percentage\n",
    "    \"state_space\": 29, # number of unique stocks \n",
    "    \"stock_dim\": 29, # number of unique stocks\n",
    "    \"tech_indicator_list\": TECHNICAL_INDICATORS_LIST, # technical indicators\n",
    "    \"action_space\": 29, # number of stocks in training data\n",
    "    \"reward_scaling\": 1e-1  #hyperparameter\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 3: Downloading from Yahoo.........\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Shape of DataFrame:  (10354, 8)\n",
      "         date       open       high        low      close     volume   tic  \\\n",
      "0  2008-12-31   3.070357   3.133571   3.047857   2.610107  607541200  AAPL   \n",
      "1  2008-12-31  50.740002  51.689999  49.910000  51.279999    7792200  AMZN   \n",
      "2  2008-12-31  82.239998  86.150002  81.120003  69.224129   14894100    GS   \n",
      "3  2009-01-02   3.067143   3.251429   3.041429   2.775245  746015200  AAPL   \n",
      "4  2009-01-02  51.349998  54.529999  51.070000  54.360001    7296400  AMZN   \n",
      "\n",
      "   day  \n",
      "0    2  \n",
      "1    2  \n",
      "2    2  \n",
      "3    4  \n",
      "4    4  \n"
     ]
    }
   ],
   "source": [
    "#Gather user parameters\n",
    "with open(\"../user_params.yaml\", \"r\") as stream:\n",
    "    try:\n",
    "        user_params = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "tickers = user_params[\"tickers\"]\n",
    "env_kwargs = user_params[\"env_params\"]\n",
    "\n",
    "print(\"\\nTest 3: Downloading from Yahoo.........\")\n",
    "downloaded_df = DataDownloader.download_data(start_date='2009-01-01',\n",
    "                                                end_date='2021-10-31',\n",
    "                                                ticker_list=tickers)\n",
    "print(downloaded_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 4: Feature engineer.........\n",
      "Successfully added technical indicators\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Shape of DataFrame:  (3231, 8)\n",
      "Successfully added vix\n",
      "Successfully added turbulence index\n",
      "Successfully added user defined features\n",
      "Successfully added covariances\n",
      "        date   tic        open        high         low       close  \\\n",
      "0 2009-12-31  AAPL    7.611786    7.619643    7.520000    6.444381   \n",
      "1 2009-12-31  AMZN  137.089996  137.279999  134.520004  134.520004   \n",
      "2 2009-12-31    GS  167.289993  170.130005  166.929993  140.212418   \n",
      "3 2010-01-04  AAPL    7.622500    7.660714    7.585000    6.544687   \n",
      "4 2010-01-04  AMZN  136.250000  136.610001  133.139999  133.899994   \n",
      "\n",
      "        volume  day      macd     boll_ub  ...     rsi_30      cci_30  \\\n",
      "0  352410800.0  3.0  0.105383    6.541181  ...  60.410575  155.820993   \n",
      "1    4523000.0  3.0  2.339104  142.484494  ...  58.842982   24.419618   \n",
      "2    6401800.0  3.0 -1.015380  140.049410  ...  49.863584   51.380987   \n",
      "3  493729600.0  0.0  0.120074    6.609204  ...  62.133712  168.777373   \n",
      "4    7599900.0  0.0  2.047087  141.460445  ...  58.237355    3.864207   \n",
      "\n",
      "       dx_30  close_30_sma  close_60_sma        vix  turbulence  daily_return  \\\n",
      "0  31.312031      6.114613      6.057069  21.680000         0.0     -0.953448   \n",
      "1  15.469698    134.175334    123.438833  21.680000         0.0     19.874000   \n",
      "2   6.433155    138.207426    143.715650  21.680000         0.0      0.042316   \n",
      "3  33.760635      6.122819      6.069180  20.040001         0.0     -0.953323   \n",
      "4  10.171696    134.262334    124.104333  20.040001         0.0     19.459343   \n",
      "\n",
      "                                            cov_list  \\\n",
      "0  [[0.0004566258236396403, 0.0002820014596461389...   \n",
      "1  [[0.0004566258236396403, 0.0002820014596461389...   \n",
      "2  [[0.0004566258236396403, 0.0002820014596461389...   \n",
      "3  [[0.000443058653221541, 0.0002683376743984121,...   \n",
      "4  [[0.000443058653221541, 0.0002683376743984121,...   \n",
      "\n",
      "                                         return_list  \n",
      "0  tic             AAPL      AMZN        GS\n",
      "date ...  \n",
      "1  tic             AAPL      AMZN        GS\n",
      "date ...  \n",
      "2  tic             AAPL      AMZN        GS\n",
      "date ...  \n",
      "3  tic             AAPL      AMZN        GS\n",
      "date ...  \n",
      "4  tic             AAPL      AMZN        GS\n",
      "date ...  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTest 4: Feature engineer.........\")\n",
    "df_processed = FeatureEngineer.add_features(df=downloaded_df,\n",
    "                                            use_default=True,\n",
    "                                            tech_indicator_list=TECHNICAL_INDICATORS_LIST,\n",
    "                                            use_vix=True,\n",
    "                                            use_turbulence=True,\n",
    "                                            user_defined_feature=True)  # included technical indicators as features\n",
    "\n",
    "print(df_processed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date   tic      open      high   low     close       volume  day  \\\n",
      "0 2009-12-31  AAPL  7.611786  7.619643  7.52  6.444381  352410800.0  3.0   \n",
      "\n",
      "       macd   boll_ub  ...     rsi_30      cci_30      dx_30  close_30_sma  \\\n",
      "0  0.105383  6.541181  ...  60.410575  155.820993  31.312031      6.114613   \n",
      "\n",
      "   close_60_sma    vix  turbulence  daily_return  \\\n",
      "0      6.057069  21.68         0.0     -0.953448   \n",
      "\n",
      "                                            cov_list  \\\n",
      "0  [[0.0004566258236396403, 0.0002820014596461389...   \n",
      "\n",
      "                                         return_list  \n",
      "0  tic             AAPL      AMZN        GS\n",
      "date ...  \n",
      "\n",
      "[1 rows x 21 columns]\n",
      "date    2009-12-31 00:00:00\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "env = PortfolioEnv(df=df_processed, **env_kwargs) #train parametresi training data olucak (dataframe) daha koyulmadi \n",
    "env_train, _ = env.get_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_train.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2c_params = {\"policy\" : \"MlpPolicy\", \n",
    "                \"environment\" : env_train,\n",
    "                \"verbose\" : 1}\n",
    "\n",
    "train_params = {\"total_timesteps\": 25000,\n",
    "                \"log_interval\": 100}\n",
    "\n",
    "test_params = {\"environment\" : env_train,\n",
    "               \"deterministic\" : True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The algorithm only supports (<class 'gym.spaces.box.Box'>, <class 'gym.spaces.discrete.Discrete'>, <class 'gym.spaces.multi_discrete.MultiDiscrete'>, <class 'gym.spaces.multi_binary.MultiBinary'>) as action spaces but None was provided",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/doganparlak/Desktop/Master_2.2/Master_Project/uniFi_github/uniFi/AgentLayer/model_structure.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/doganparlak/Desktop/Master_2.2/Master_Project/uniFi_github/uniFi/AgentLayer/model_structure.ipynb#ch0000014?line=0'>1</a>\u001b[0m \u001b[39m#object creation\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/doganparlak/Desktop/Master_2.2/Master_Project/uniFi_github/uniFi/AgentLayer/model_structure.ipynb#ch0000014?line=1'>2</a>\u001b[0m a2c \u001b[39m=\u001b[39m A2C(env \u001b[39m=\u001b[39;49m env_train, policy \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mMlpPolicy\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py:79\u001b[0m, in \u001b[0;36mA2C.__init__\u001b[0;34m(self, policy, env, learning_rate, n_steps, gamma, gae_lambda, ent_coef, vf_coef, max_grad_norm, rms_prop_eps, use_rms_prop, use_sde, sde_sample_freq, normalize_advantage, tensorboard_log, create_eval_env, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=53'>54</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=54'>55</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=55'>56</a>\u001b[0m     policy: Union[\u001b[39mstr\u001b[39m, Type[ActorCriticPolicy]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=75'>76</a>\u001b[0m     _init_setup_model: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=76'>77</a>\u001b[0m ):\n\u001b[0;32m---> <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=78'>79</a>\u001b[0m     \u001b[39msuper\u001b[39;49m(A2C, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=79'>80</a>\u001b[0m         policy,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=80'>81</a>\u001b[0m         env,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=81'>82</a>\u001b[0m         learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=82'>83</a>\u001b[0m         n_steps\u001b[39m=\u001b[39;49mn_steps,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=83'>84</a>\u001b[0m         gamma\u001b[39m=\u001b[39;49mgamma,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=84'>85</a>\u001b[0m         gae_lambda\u001b[39m=\u001b[39;49mgae_lambda,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=85'>86</a>\u001b[0m         ent_coef\u001b[39m=\u001b[39;49ment_coef,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=86'>87</a>\u001b[0m         vf_coef\u001b[39m=\u001b[39;49mvf_coef,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=87'>88</a>\u001b[0m         max_grad_norm\u001b[39m=\u001b[39;49mmax_grad_norm,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=88'>89</a>\u001b[0m         use_sde\u001b[39m=\u001b[39;49muse_sde,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=89'>90</a>\u001b[0m         sde_sample_freq\u001b[39m=\u001b[39;49msde_sample_freq,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=90'>91</a>\u001b[0m         tensorboard_log\u001b[39m=\u001b[39;49mtensorboard_log,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=91'>92</a>\u001b[0m         policy_kwargs\u001b[39m=\u001b[39;49mpolicy_kwargs,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=92'>93</a>\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=93'>94</a>\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=94'>95</a>\u001b[0m         create_eval_env\u001b[39m=\u001b[39;49mcreate_eval_env,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=95'>96</a>\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=96'>97</a>\u001b[0m         _init_setup_model\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=97'>98</a>\u001b[0m         supported_action_spaces\u001b[39m=\u001b[39;49m(\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=98'>99</a>\u001b[0m             spaces\u001b[39m.\u001b[39;49mBox,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=99'>100</a>\u001b[0m             spaces\u001b[39m.\u001b[39;49mDiscrete,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=100'>101</a>\u001b[0m             spaces\u001b[39m.\u001b[39;49mMultiDiscrete,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=101'>102</a>\u001b[0m             spaces\u001b[39m.\u001b[39;49mMultiBinary,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=102'>103</a>\u001b[0m         ),\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=103'>104</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=105'>106</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize_advantage \u001b[39m=\u001b[39m normalize_advantage\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=107'>108</a>\u001b[0m     \u001b[39m# Update optimizer inside the policy if we want to use RMSProp\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=108'>109</a>\u001b[0m     \u001b[39m# (original implementation) rather than Adam\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py:77\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.__init__\u001b[0;34m(self, policy, env, learning_rate, n_steps, gamma, gae_lambda, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, policy_base, tensorboard_log, create_eval_env, monitor_wrapper, policy_kwargs, verbose, seed, device, _init_setup_model, supported_action_spaces)\u001b[0m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=51'>52</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=52'>53</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=53'>54</a>\u001b[0m     policy: Union[\u001b[39mstr\u001b[39m, Type[ActorCriticPolicy]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=73'>74</a>\u001b[0m     supported_action_spaces: Optional[Tuple[gym\u001b[39m.\u001b[39mspaces\u001b[39m.\u001b[39mSpace, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=74'>75</a>\u001b[0m ):\n\u001b[0;32m---> <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=76'>77</a>\u001b[0m     \u001b[39msuper\u001b[39;49m(OnPolicyAlgorithm, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=77'>78</a>\u001b[0m         policy\u001b[39m=\u001b[39;49mpolicy,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=78'>79</a>\u001b[0m         env\u001b[39m=\u001b[39;49menv,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=79'>80</a>\u001b[0m         policy_base\u001b[39m=\u001b[39;49mpolicy_base,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=80'>81</a>\u001b[0m         learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=81'>82</a>\u001b[0m         policy_kwargs\u001b[39m=\u001b[39;49mpolicy_kwargs,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=82'>83</a>\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=83'>84</a>\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=84'>85</a>\u001b[0m         use_sde\u001b[39m=\u001b[39;49muse_sde,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=85'>86</a>\u001b[0m         sde_sample_freq\u001b[39m=\u001b[39;49msde_sample_freq,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=86'>87</a>\u001b[0m         create_eval_env\u001b[39m=\u001b[39;49mcreate_eval_env,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=87'>88</a>\u001b[0m         support_multi_env\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=88'>89</a>\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=89'>90</a>\u001b[0m         tensorboard_log\u001b[39m=\u001b[39;49mtensorboard_log,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=90'>91</a>\u001b[0m         supported_action_spaces\u001b[39m=\u001b[39;49msupported_action_spaces,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=91'>92</a>\u001b[0m     )\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=93'>94</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_steps \u001b[39m=\u001b[39m n_steps\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=94'>95</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma \u001b[39m=\u001b[39m gamma\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/stable_baselines3/common/base_class.py:171\u001b[0m, in \u001b[0;36mBaseAlgorithm.__init__\u001b[0;34m(self, policy, env, policy_base, learning_rate, policy_kwargs, tensorboard_log, verbose, device, support_multi_env, create_eval_env, monitor_wrapper, seed, use_sde, sde_sample_freq, supported_action_spaces)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/base_class.py?line=167'>168</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39m=\u001b[39m env\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/base_class.py?line=169'>170</a>\u001b[0m \u001b[39mif\u001b[39;00m supported_action_spaces \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/base_class.py?line=170'>171</a>\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space, supported_action_spaces), (\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/base_class.py?line=171'>172</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe algorithm only supports \u001b[39m\u001b[39m{\u001b[39;00msupported_action_spaces\u001b[39m}\u001b[39;00m\u001b[39m as action spaces \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/base_class.py?line=172'>173</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m}\u001b[39;00m\u001b[39m was provided\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/base_class.py?line=173'>174</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/base_class.py?line=175'>176</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m support_multi_env \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_envs \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/base_class.py?line=176'>177</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/base_class.py?line=177'>178</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mError: the model does not support multiple envs; it requires \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39ma single vectorized environment.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/base_class.py?line=178'>179</a>\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: The algorithm only supports (<class 'gym.spaces.box.Box'>, <class 'gym.spaces.discrete.Discrete'>, <class 'gym.spaces.multi_discrete.MultiDiscrete'>, <class 'gym.spaces.multi_binary.MultiBinary'>) as action spaces but None was provided"
     ]
    }
   ],
   "source": [
    "#object creation\n",
    "a2c = A2C(env = env_train, policy = \"MlpPolicy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "a2c.train_model(train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting\n",
    "a2c.predict(test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving \n",
    "a2c.save_model(\"a2c_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading\n",
    "loaded_a2c_model = a2c.load_model(\"a2c_model\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1ee38ef4a5a9feb55287fd749643f13d043cb0a7addaab2a9c224cbe137c0062"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
