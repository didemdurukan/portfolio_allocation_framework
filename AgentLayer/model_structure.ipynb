{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/doganparlak/Desktop/Master_2.2/Master_Project/uniFi_github/uniFi/AgentLayer', '/usr/local/Cellar/python@3.8/3.8.8_1/Frameworks/Python.framework/Versions/3.8/lib/python38.zip', '/usr/local/Cellar/python@3.8/3.8.8_1/Frameworks/Python.framework/Versions/3.8/lib/python3.8', '/usr/local/Cellar/python@3.8/3.8.8_1/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload', '', '/usr/local/lib/python3.8/site-packages', '/usr/local/lib/python3.8/site-packages/selenium-3.141.0-py3.8.egg', '/usr/local/lib/python3.8/site-packages/urllib3-1.26.4-py3.8.egg', '/Users/doganparlak/Desktop/Master_2.2/Master_Project/uniFi_github/uniFi']\n"
     ]
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import A2C as sb_A2C\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import gym\n",
    "from datetime import datetime\n",
    "from sys import path\n",
    "from os.path import dirname as dir\n",
    "\n",
    "path.append(dir(path[0]))\n",
    "print(path)\n",
    "#__package__ = \"examples\"\n",
    "\n",
    "from FinancialEnvLayer.datacollector import CustomDatasetImporter\n",
    "from FinancialEnvLayer.datacollector import DataDownloader\n",
    "from FinancialEnvLayer.dataprocessor import FeatureEngineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(gym.Env, ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def step(self, action):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def render(self, mode=\"human\"):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_env(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax_normalization(actions):\n",
    "        numerator = np.exp(actions)\n",
    "        denominator = np.sum(np.exp(actions))\n",
    "        softmax_output = numerator / denominator\n",
    "        return softmax_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_model():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save_model():\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def load_model():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConventionalAgent(Agent, ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_model():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save_model():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_model():\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _return_predict():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _weight_optimization():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLAgent(Agent, ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_model():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict():\n",
    "        \n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save_model():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_model():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortfolioEnv(Environment):\n",
    "\n",
    "    def __init__(self,\n",
    "                 df: pd.DataFrame,  # input data\n",
    "                 port_dim: int,  # number of unique securities in the investment universe\n",
    "                 hmax: float,  # maximum number of shares to trade\n",
    "                 initial_amount: float,  # initial cash value\n",
    "                 transaction_cost_pct: float,  # transaction cost percentage per trade\n",
    "                 reward_scaling: float,  # scaling factor for reward as training progresses\n",
    "                 state_space: int,  # the dimension of input features (state space)\n",
    "                 action_space: int,  # number of actions, which is equal to portfolio dimension\n",
    "                 tech_indicator_list: list,  # a list of technical indicator names\n",
    "                 turbulence_threshold=None,  # a threshold to control risk aversion\n",
    "                 lookback=252,  #\n",
    "                 day=0):  # an increment number to control date\n",
    "\n",
    "        self.df = df\n",
    "        self.day = day\n",
    "        self.lookback = lookback\n",
    "        self.port_dim = port_dim\n",
    "        self.hmax = hmax\n",
    "        self.initial_amount = initial_amount\n",
    "        self.transaction_cost_pct = transaction_cost_pct\n",
    "        self.reward_scaling = reward_scaling\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.tech_indicator_list = tech_indicator_list\n",
    "\n",
    "        # action_space normalization and shape is self.stock_dim\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(self.action_space,))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf,\n",
    "                                            shape=(self.state_space + len(self.tech_indicator_list), self.state_space))\n",
    "        \n",
    "\n",
    "        # load data from a pandas dataframe\n",
    "        self.data = self.df.loc[self.day, :]\n",
    "        self.covs = self.data['cov_list'][0]\n",
    "        self.state = np.append(np.array(self.covs), [self.data[tech] for tech in self.tech_indicator_list], axis=0)\n",
    "        self.terminal = False\n",
    "        #self.turbulence_threshold = turbulence_threshold\n",
    "        # initalize state: initial portfolio return + individual stock return + individual weights\n",
    "        self.portfolio_value = self.initial_amount\n",
    "\n",
    "        # memorize portfolio value each step\n",
    "        self.asset_memory = [self.initial_amount]\n",
    "        # memorize portfolio return each step\n",
    "        self.portfolio_return_memory = [0]\n",
    "        self.actions_memory = [[1 / self.port_dim] * self.port_dim]\n",
    "        self.date_memory = [self.data[\"date\"]]\n",
    "\n",
    "    def reset(self):\n",
    "        print(\"----RESET---\")\n",
    "        self.asset_memory = [self.initial_amount]\n",
    "        self.day = 0\n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        # load states\n",
    "        self.covs = self.data['cov_list'].values[0]\n",
    "        self.state =  np.append(np.array(self.covs), [self.data[tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)\n",
    "        self.portfolio_value = self.initial_amount\n",
    "        #self.cost = 0\n",
    "        #self.trades = 0\n",
    "        self.terminal = False \n",
    "        self.portfolio_return_memory = [0]\n",
    "        self.actions_memory=[[1/self.stock_dim]*self.stock_dim]\n",
    "        self.date_memory=[self.data.date.unique()[0]] \n",
    "        return self.state\n",
    "\n",
    "    def step(self, actions):\n",
    "        self.terminal = self.day >= len(self.df.index.unique()) - 1\n",
    "\n",
    "        if self.terminal:\n",
    "            df = pd.DataFrame(self.portfolio_return_memory)\n",
    "            df.columns = ['daily_return']\n",
    "            plt.plot(df.daily_return.cumsum(), 'r')\n",
    "            plt.savefig('results/cumulative_reward.png')\n",
    "            plt.close()\n",
    "\n",
    "            plt.plot(self.portfolio_return_memory, 'r')\n",
    "            plt.savefig('results/rewards.png')\n",
    "            plt.close()\n",
    "\n",
    "            print(\"=================================\")\n",
    "            print(\"begin_total_asset:{}\".format(self.asset_memory[0]))\n",
    "            print(\"end_total_asset:{}\".format(self.portfolio_value))\n",
    "\n",
    "            df_daily_return = pd.DataFrame(self.portfolio_return_memory)\n",
    "            df_daily_return.columns = ['daily_return']\n",
    "            if df_daily_return['daily_return'].std() != 0:\n",
    "                sharpe = (252 ** 0.5) * df_daily_return['daily_return'].mean() / \\\n",
    "                         df_daily_return['daily_return'].std()\n",
    "                print(\"Sharpe: \", sharpe)\n",
    "            print(\"=================================\")\n",
    "\n",
    "            return self.state, self.reward, self.terminal, {}\n",
    "\n",
    "        else:\n",
    "            weights = Environment.softmax_normalization(actions)\n",
    "            self.actions_memory.append(weights)\n",
    "            last_day_memory = self.data\n",
    "\n",
    "            # load next state\n",
    "            self.day += 1\n",
    "            self.data = self.df.loc[self.day, :]\n",
    "            self.covs = self.data['cov_list'][0]\n",
    "            self.state = np.append(np.array(self.covs), [self.data[tech] for tech in self.tech_indicator_list],\n",
    "                                   axis=0)\n",
    "            portfolio_return = sum(((self.data.close.values / last_day_memory.close.values) - 1) * weights)\n",
    "            log_portfolio_return = np.log(sum((self.data.close.values / last_day_memory.close.values) * weights))\n",
    "            # update portfolio value\n",
    "            new_portfolio_value = self.portfolio_value * (1 + portfolio_return)\n",
    "            self.portfolio_value = new_portfolio_value\n",
    "\n",
    "            # save into memory\n",
    "            self.portfolio_return_memory.append(portfolio_return)\n",
    "            self.date_memory.append(self.data[\"date\"].unique()[0])\n",
    "            self.asset_memory.append(new_portfolio_value)\n",
    "\n",
    "            # the reward is the new portfolio value or end portfolo value\n",
    "            self.reward = new_portfolio_value\n",
    "\n",
    "        return self.state, self.reward, self.terminal, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        return self.state\n",
    "\n",
    "    def save_asset_memory(self):\n",
    "        date_list = self.date_memory\n",
    "        portfolio_return = self.portfolio_return_memory\n",
    "        # print(len(date_list))\n",
    "        # print(len(asset_list))\n",
    "        df_account_value = pd.DataFrame({'date': date_list, 'daily_return': portfolio_return})\n",
    "        return df_account_value\n",
    "\n",
    "    def save_action_memory(self):\n",
    "        # date and close price length must match actions length\n",
    "        date_list = self.date_memory\n",
    "        df_date = pd.DataFrame(date_list)\n",
    "        df_date.columns = ['date']\n",
    "\n",
    "        action_list = self.actions_memory\n",
    "        df_actions = pd.DataFrame(action_list)\n",
    "        df_actions.columns = self.data.tic.values\n",
    "        df_actions.index = df_date.date\n",
    "        # df_actions = pd.DataFrame({'date':date_list,'actions':action_list})\n",
    "        return df_actions\n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def get_env(self):\n",
    "        e = DummyVecEnv([lambda: self])\n",
    "        obs = e.reset()\n",
    "        return e, obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C(RLAgent):\n",
    "\n",
    "    def __init__(self,\n",
    "                 policy= \"MlpPolicy\",\n",
    "                 env= None,\n",
    "                 learning_rate: float = 7e-4,\n",
    "                 n_steps: int = 5,\n",
    "                 gamma: float = 0.99,\n",
    "                 gae_lambda: float = 1.0,\n",
    "                 ent_coef: float = 0.0,\n",
    "                 vf_coef: float = 0.5,\n",
    "                 max_grad_norm: float = 0.5,\n",
    "                 rms_prop_eps: float = 1e-5,\n",
    "                 use_rms_prop: bool = True,\n",
    "                 use_sde: bool = False,\n",
    "                 sde_sample_freq: int = -1,\n",
    "                 normalize_advantage: bool = False,\n",
    "                 tensorboard_log=None,\n",
    "                 create_eval_env: bool = False,\n",
    "                 policy_kwargs=None,\n",
    "                 verbose: int = 0,\n",
    "                 seed=None,\n",
    "                 device=\"auto\",\n",
    "                 _init_setup_model: bool = True):\n",
    "\n",
    "        self.env = env\n",
    "        # self.model = A2C(model_params[\"policy\"], model_params[\"environment\"], model_params[\"verbose\"])\n",
    "        self.model = sb_A2C(policy = policy,\n",
    "                            env=self.env,\n",
    "                            learning_rate = learning_rate,\n",
    "                            n_steps = n_steps,\n",
    "                            gamma = gamma,\n",
    "                            gae_lambda= gae_lambda,\n",
    "                            ent_coef = ent_coef,\n",
    "                            vf_coef = vf_coef,\n",
    "                            max_grad_norm = max_grad_norm,\n",
    "                            rms_prop_eps= rms_prop_eps,\n",
    "                            use_rms_prop= use_rms_prop,\n",
    "                            use_sde= use_sde,\n",
    "                            sde_sample_freq= sde_sample_freq,\n",
    "                            normalize_advantage= normalize_advantage,\n",
    "                            tensorboard_log=tensorboard_log,  \n",
    "                            create_eval_env= create_eval_env,\n",
    "                            policy_kwargs=policy_kwargs,\n",
    "                            verbose=verbose,\n",
    "                            seed=seed,\n",
    "                            device= device,\n",
    "                            _init_setup_model = _init_setup_model)\n",
    "\n",
    "    def train_model(self, **train_params):\n",
    "        self.model = self.model.learn(**train_params)\n",
    "        return self.model\n",
    "\n",
    "    def predict(self, **test_params):\n",
    "\n",
    "        test_env, test_obs = test_params[\"environment\"].environment()\n",
    "        \"\"\"make a prediction\"\"\"\n",
    "        account_memory = []\n",
    "        actions_memory = []\n",
    "\n",
    "        test_env.reset()\n",
    "        for i in range(len(test_params[\"environment\"].df.index.unique())):\n",
    "            action, _states = self.model.predict(test_obs, deterministic=test_params[\"deterministic\"])\n",
    "            test_obs, rewards, dones, info = test_env.step(action)\n",
    "            if i == (len(test_params[\"environment\"].df.index.unique()) - 2):\n",
    "                account_memory = test_env.env_method(method_name=\"save_asset_memory\")\n",
    "                actions_memory = test_env.env_method(method_name=\"save_action_memory\")\n",
    "            if dones[0]:\n",
    "                print(\"hit end!\")\n",
    "                break\n",
    "\n",
    "        return account_memory[0], actions_memory[0]\n",
    "\n",
    "    def save_model(self, path):\n",
    "        self.model.save(path)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        self.model = self.model.load(path)\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gather user parameters\n",
    "with open(\"../user_params.yaml\", \"r\") as stream:\n",
    "    try:\n",
    "        user_params = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "tickers = user_params[\"tickers\"]\n",
    "env_kwargs = user_params[\"env_params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 3: Downloading from Yahoo.........\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Shape of DataFrame:  (10354, 8)\n",
      "         date       open       high        low      close     volume   tic  \\\n",
      "0  2008-12-31   3.070357   3.133571   3.047857   2.610107  607541200  AAPL   \n",
      "1  2008-12-31  50.740002  51.689999  49.910000  51.279999    7792200  AMZN   \n",
      "2  2008-12-31  82.239998  86.150002  81.120003  69.224167   14894100    GS   \n",
      "3  2009-01-02   3.067143   3.251429   3.041429   2.775246  746015200  AAPL   \n",
      "4  2009-01-02  51.349998  54.529999  51.070000  54.360001    7296400  AMZN   \n",
      "\n",
      "   day  \n",
      "0    2  \n",
      "1    2  \n",
      "2    2  \n",
      "3    4  \n",
      "4    4  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTest 3: Downloading from Yahoo.........\")\n",
    "downloaded_df = DataDownloader.download_data(start_date='2009-01-01',\n",
    "                                                end_date='2021-10-31',\n",
    "                                                ticker_list=tickers)\n",
    "print(downloaded_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 4: Feature engineer.........\n",
      "Successfully added technical indicators\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Shape of DataFrame:  (3231, 8)\n",
      "Successfully added vix\n",
      "Successfully added turbulence index\n",
      "Successfully added user defined features\n",
      "Successfully added covariances\n",
      "        date   tic        open        high         low       close  \\\n",
      "0 2009-12-31  AAPL    7.611786    7.619643    7.520000    6.444381   \n",
      "1 2009-12-31  AMZN  137.089996  137.279999  134.520004  134.520004   \n",
      "2 2009-12-31    GS  167.289993  170.130005  166.929993  140.212402   \n",
      "3 2010-01-04  AAPL    7.622500    7.660714    7.585000    6.544688   \n",
      "4 2010-01-04  AMZN  136.250000  136.610001  133.139999  133.899994   \n",
      "\n",
      "        volume  day      macd     boll_ub  ...     rsi_30      cci_30  \\\n",
      "0  352410800.0  3.0  0.105383    6.541181  ...  60.410597  155.820973   \n",
      "1    4523000.0  3.0  2.339104  142.484494  ...  58.842982   24.419618   \n",
      "2    6401800.0  3.0 -1.015380  140.049394  ...  49.863571   51.380800   \n",
      "3  493729600.0  0.0  0.120074    6.609204  ...  62.133760  168.777540   \n",
      "4    7599900.0  0.0  2.047087  141.460445  ...  58.237355    3.864207   \n",
      "\n",
      "       dx_30  close_30_sma  close_60_sma        vix  turbulence  daily_return  \\\n",
      "0  31.312031      6.114613      6.057069  21.680000         0.0     -0.953448   \n",
      "1  15.469698    134.175334    123.438833  21.680000         0.0     19.874000   \n",
      "2   6.433155    138.207428    143.715650  21.680000         0.0      0.042316   \n",
      "3  33.760635      6.122819      6.069180  20.040001         0.0     -0.953323   \n",
      "4  10.171696    134.262334    124.104333  20.040001         0.0     19.459339   \n",
      "\n",
      "                                            cov_list  \\\n",
      "0  [[0.00045662662859292545, 0.000282001376518939...   \n",
      "1  [[0.00045662662859292545, 0.000282001376518939...   \n",
      "2  [[0.00045662662859292545, 0.000282001376518939...   \n",
      "3  [[0.00044305943863221653, 0.000268337564501212...   \n",
      "4  [[0.00044305943863221653, 0.000268337564501212...   \n",
      "\n",
      "                                         return_list  \n",
      "0  tic             AAPL      AMZN        GS\n",
      "date ...  \n",
      "1  tic             AAPL      AMZN        GS\n",
      "date ...  \n",
      "2  tic             AAPL      AMZN        GS\n",
      "date ...  \n",
      "3  tic             AAPL      AMZN        GS\n",
      "date ...  \n",
      "4  tic             AAPL      AMZN        GS\n",
      "date ...  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTest 4: Feature engineer.........\")\n",
    "df_processed = FeatureEngineer.add_features(df= downloaded_df,\n",
    "                                            use_default= True,\n",
    "                                            tech_indicator_list= env_kwargs[\"tech_indicator_list\"],\n",
    "                                            use_vix=True,\n",
    "                                            use_turbulence=True,\n",
    "                                            user_defined_feature=True)  # included technical indicators as features\n",
    "\n",
    "print(df_processed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PortfolioEnv(df=df_processed, **env_kwargs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_train, _ = env.get_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([0. 0. 0.], [1. 1. 1.], (3,), float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_train.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = user_params[\"train_params\"]\n",
    "policy_params = user_params[\"policy_params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A2C_PARAMS': {'total_timesteps': 1000,\n",
       "  'callback': None,\n",
       "  'log_interval': 100,\n",
       "  'eval_env': None,\n",
       "  'eval_freq': -1,\n",
       "  'n_eval_episodes': 5,\n",
       "  'tb_log_name': 'A2C',\n",
       "  'eval_log_path': None,\n",
       "  'reset_num_timesteps': True}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#object creation\n",
    "a2c = A2C(env = env_train, **policy_params[\"A2C_PARAMS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected parameter loc (Tensor of shape (1, 3)) of distribution Normal(loc: torch.Size([1, 3]), scale: torch.Size([1, 3])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan, nan]])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/doganparlak/Desktop/Master_2.2/Master_Project/uniFi_github/uniFi/AgentLayer/model_structure.ipynb Cell 17'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/doganparlak/Desktop/Master_2.2/Master_Project/uniFi_github/uniFi/AgentLayer/model_structure.ipynb#ch0000014?line=0'>1</a>\u001b[0m \u001b[39m#training\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/doganparlak/Desktop/Master_2.2/Master_Project/uniFi_github/uniFi/AgentLayer/model_structure.ipynb#ch0000014?line=1'>2</a>\u001b[0m a2c\u001b[39m.\u001b[39;49mtrain_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtrain_params[\u001b[39m\"\u001b[39;49m\u001b[39mA2C_PARAMS\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "\u001b[1;32m/Users/doganparlak/Desktop/Master_2.2/Master_Project/uniFi_github/uniFi/AgentLayer/model_structure.ipynb Cell 7'\u001b[0m in \u001b[0;36mA2C.train_model\u001b[0;34m(self, **train_params)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/doganparlak/Desktop/Master_2.2/Master_Project/uniFi_github/uniFi/AgentLayer/model_structure.ipynb#ch0000006?line=49'>50</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_model\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrain_params):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/doganparlak/Desktop/Master_2.2/Master_Project/uniFi_github/uniFi/AgentLayer/model_structure.ipynb#ch0000006?line=50'>51</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mlearn(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtrain_params)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/doganparlak/Desktop/Master_2.2/Master_Project/uniFi_github/uniFi/AgentLayer/model_structure.ipynb#ch0000006?line=51'>52</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py:191\u001b[0m, in \u001b[0;36mA2C.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=177'>178</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=178'>179</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=179'>180</a>\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=187'>188</a>\u001b[0m     reset_num_timesteps: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=188'>189</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mA2C\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=190'>191</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(A2C, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=191'>192</a>\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=192'>193</a>\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=193'>194</a>\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=194'>195</a>\u001b[0m         eval_env\u001b[39m=\u001b[39;49meval_env,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=195'>196</a>\u001b[0m         eval_freq\u001b[39m=\u001b[39;49meval_freq,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=196'>197</a>\u001b[0m         n_eval_episodes\u001b[39m=\u001b[39;49mn_eval_episodes,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=197'>198</a>\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=198'>199</a>\u001b[0m         eval_log_path\u001b[39m=\u001b[39;49meval_log_path,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=199'>200</a>\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py?line=200'>201</a>\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py:250\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=245'>246</a>\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=247'>248</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=249'>250</a>\u001b[0m     continue_training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, callback, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrollout_buffer, n_rollout_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_steps)\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=251'>252</a>\u001b[0m     \u001b[39mif\u001b[39;00m continue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=252'>253</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py:169\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=165'>166</a>\u001b[0m \u001b[39mwith\u001b[39;00m th\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=166'>167</a>\u001b[0m     \u001b[39m# Convert to pytorch tensor or to TensorDict\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=167'>168</a>\u001b[0m     obs_tensor \u001b[39m=\u001b[39m obs_as_tensor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=168'>169</a>\u001b[0m     actions, values, log_probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy(obs_tensor)\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=169'>170</a>\u001b[0m actions \u001b[39m=\u001b[39m actions\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=171'>172</a>\u001b[0m \u001b[39m# Rescale and perform action\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/stable_baselines3/common/policies.py:592\u001b[0m, in \u001b[0;36mActorCriticPolicy.forward\u001b[0;34m(self, obs, deterministic)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/policies.py?line=589'>590</a>\u001b[0m \u001b[39m# Evaluate the values for the given observations\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/policies.py?line=590'>591</a>\u001b[0m values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue_net(latent_vf)\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/policies.py?line=591'>592</a>\u001b[0m distribution \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_action_dist_from_latent(latent_pi)\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/policies.py?line=592'>593</a>\u001b[0m actions \u001b[39m=\u001b[39m distribution\u001b[39m.\u001b[39mget_actions(deterministic\u001b[39m=\u001b[39mdeterministic)\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/policies.py?line=593'>594</a>\u001b[0m log_prob \u001b[39m=\u001b[39m distribution\u001b[39m.\u001b[39mlog_prob(actions)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/stable_baselines3/common/policies.py:607\u001b[0m, in \u001b[0;36mActorCriticPolicy._get_action_dist_from_latent\u001b[0;34m(self, latent_pi)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/policies.py?line=603'>604</a>\u001b[0m mean_actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_net(latent_pi)\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/policies.py?line=605'>606</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_dist, DiagGaussianDistribution):\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/policies.py?line=606'>607</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_dist\u001b[39m.\u001b[39;49mproba_distribution(mean_actions, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlog_std)\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/policies.py?line=607'>608</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_dist, CategoricalDistribution):\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/policies.py?line=608'>609</a>\u001b[0m     \u001b[39m# Here mean_actions are the logits before the softmax\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/policies.py?line=609'>610</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_dist\u001b[39m.\u001b[39mproba_distribution(action_logits\u001b[39m=\u001b[39mmean_actions)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/stable_baselines3/common/distributions.py:152\u001b[0m, in \u001b[0;36mDiagGaussianDistribution.proba_distribution\u001b[0;34m(self, mean_actions, log_std)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/distributions.py?line=143'>144</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/distributions.py?line=144'>145</a>\u001b[0m \u001b[39mCreate the distribution given its parameters (mean, std)\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/distributions.py?line=145'>146</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/distributions.py?line=148'>149</a>\u001b[0m \u001b[39m:return:\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/distributions.py?line=149'>150</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/distributions.py?line=150'>151</a>\u001b[0m action_std \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mones_like(mean_actions) \u001b[39m*\u001b[39m log_std\u001b[39m.\u001b[39mexp()\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/distributions.py?line=151'>152</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribution \u001b[39m=\u001b[39m Normal(mean_actions, action_std)\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/site-packages/stable_baselines3/common/distributions.py?line=152'>153</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/distributions/normal.py:50\u001b[0m, in \u001b[0;36mNormal.__init__\u001b[0;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/torch/distributions/normal.py?line=47'>48</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/torch/distributions/normal.py?line=48'>49</a>\u001b[0m     batch_shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloc\u001b[39m.\u001b[39msize()\n\u001b[0;32m---> <a href='file:///usr/local/lib/python3.8/site-packages/torch/distributions/normal.py?line=49'>50</a>\u001b[0m \u001b[39msuper\u001b[39;49m(Normal, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(batch_shape, validate_args\u001b[39m=\u001b[39;49mvalidate_args)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/distributions/distribution.py:55\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/torch/distributions/distribution.py?line=52'>53</a>\u001b[0m valid \u001b[39m=\u001b[39m constraint\u001b[39m.\u001b[39mcheck(value)\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/torch/distributions/distribution.py?line=53'>54</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m valid\u001b[39m.\u001b[39mall():\n\u001b[0;32m---> <a href='file:///usr/local/lib/python3.8/site-packages/torch/distributions/distribution.py?line=54'>55</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/torch/distributions/distribution.py?line=55'>56</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected parameter \u001b[39m\u001b[39m{\u001b[39;00mparam\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/torch/distributions/distribution.py?line=56'>57</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(value)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m of shape \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(value\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/torch/distributions/distribution.py?line=57'>58</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mof distribution \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/torch/distributions/distribution.py?line=58'>59</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mto satisfy the constraint \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(constraint)\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/torch/distributions/distribution.py?line=59'>60</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut found invalid values:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/torch/distributions/distribution.py?line=60'>61</a>\u001b[0m     )\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/torch/distributions/distribution.py?line=61'>62</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m constraint\u001b[39m.\u001b[39mcheck(\u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, param))\u001b[39m.\u001b[39mall():\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/site-packages/torch/distributions/distribution.py?line=62'>63</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe parameter \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m has invalid values\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(param))\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter loc (Tensor of shape (1, 3)) of distribution Normal(loc: torch.Size([1, 3]), scale: torch.Size([1, 3])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan, nan]])"
     ]
    }
   ],
   "source": [
    "#training\n",
    "a2c.train_model(**train_params[\"A2C_PARAMS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting\n",
    "a2c.predict(test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving \n",
    "a2c.save_model(\"a2c_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading\n",
    "loaded_a2c_model = a2c.load_model(\"a2c_model\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1ee38ef4a5a9feb55287fd749643f13d043cb0a7addaab2a9c224cbe137c0062"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
