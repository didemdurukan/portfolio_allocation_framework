{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/doganparlak/Desktop/Master_2.2/Master_Project/uniFi_github/uniFi/AgentLayer', '/usr/local/Cellar/python@3.8/3.8.8_1/Frameworks/Python.framework/Versions/3.8/lib/python38.zip', '/usr/local/Cellar/python@3.8/3.8.8_1/Frameworks/Python.framework/Versions/3.8/lib/python3.8', '/usr/local/Cellar/python@3.8/3.8.8_1/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload', '', '/Users/doganparlak/Library/Python/3.8/lib/python/site-packages', '/usr/local/lib/python3.8/site-packages', '/usr/local/lib/python3.8/site-packages/selenium-3.141.0-py3.8.egg', '/usr/local/lib/python3.8/site-packages/urllib3-1.26.4-py3.8.egg', '/Users/doganparlak/Desktop/Master_2.2/Master_Project/uniFi_github/uniFi']\n"
     ]
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import A2C as sb_A2C\n",
    "from stable_baselines3 import PPO as sb_PPO\n",
    "from stable_baselines3 import DDPG as sb_DDPG\n",
    "from stable_baselines3 import TD3 as sb_TD3\n",
    "from stable_baselines3.common.noise import ActionNoise\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "from stable_baselines3.common.type_aliases import Schedule\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch as th\n",
    "import yaml\n",
    "import gym\n",
    "from datetime import datetime\n",
    "from sys import path\n",
    "from os.path import dirname as dir\n",
    "from typing import Any, Dict, Optional, Tuple, Type, Union\n",
    "path.append(dir(path[0]))\n",
    "print(path)\n",
    "#__package__ = \"examples\"\n",
    "\n",
    "from FinancialDataLayer.DataCollection.DataDownloader import DataDownloader\n",
    "from FinancialDataLayer.DataProcessing.DefaultFeatureEngineer import DefaultFeatureEngineer\n",
    "from AgentLayer.DataSplitter.TimeSeriesSplitter import TimeSeriesSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pypfopt\n",
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "from pypfopt import risk_models\n",
    "from pypfopt import EfficientFrontier\n",
    "from pypfopt import risk_models\n",
    "from pypfopt import expected_returns\n",
    "from pypfopt import objective_functions\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(gym.Env, ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def step(self, action):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def render(self, mode=\"human\"):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_env(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax_normalization(actions):\n",
    "        numerator = np.exp(actions)\n",
    "        denominator = np.sum(np.exp(actions))\n",
    "        softmax_output = numerator / denominator\n",
    "        return softmax_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_model():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save_model():\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def load_model():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConventionalAgent(Agent, ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_model():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save_model():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_model():\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _return_predict():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _weight_optimization():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLAgent(Agent, ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_model():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict():\n",
    "        \n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save_model():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_model():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionAgent(ConventionalAgent):\n",
    "\n",
    "   def __init__(self,\n",
    "                fit_intercept = True,\n",
    "                copy_X = True,\n",
    "                positive = False):  \n",
    "\n",
    "        self.model = LinearRegression(fit_intercept=fit_intercept,\n",
    "                                      copy_X= copy_X,\n",
    "                                      positive= positive)\n",
    "\n",
    "   def train_model(self, train_x, train_y, **train_params):\n",
    "        '''\n",
    "        *Trains the model*\n",
    "        Input: Train data x and train data y\n",
    "        Output: Linear Regression Model\n",
    "        '''\n",
    "        try:\n",
    "            trained_reg = self.model.fit(train_x, train_y, **train_params)\n",
    "            print(\"Model trained succesfully\")\n",
    "            return trained_reg\n",
    "        except Exception as e:\n",
    "            print(\"training unsuccessful\")\n",
    "    \n",
    "   def predict(self,\n",
    "               test_data, \n",
    "               initial_capital = 0,\n",
    "               tech_indicator_list = [\n",
    "                    \"macd\",\n",
    "                    \"boll_ub\",\n",
    "                    \"boll_lb\",\n",
    "                    \"rsi_30\",\n",
    "                    \"cci_30\",\n",
    "                    \"dx_30\",\n",
    "                    \"close_30_sma\",\n",
    "                    \"close_60_sma\",\n",
    "                ]):\n",
    "\n",
    "        meta_coefficient = {\"date\": [], \"weights\": []}\n",
    "        unique_trade_date = test_data.date.unique()\n",
    "        portfolio = pd.DataFrame(index=range(1), columns=unique_trade_date)\n",
    "        portfolio.loc[0, unique_trade_date[0]] = initial_capital\n",
    "\n",
    "        for i in range(len(unique_trade_date) - 1):\n",
    "            mu, sigma, tics, df_current, df_next = self._return_predict(\n",
    "                unique_trade_date, test_data, i, tech_indicator_list)\n",
    "\n",
    "            portfolio_value = self._weight_optimization(\n",
    "                i, unique_trade_date, meta_coefficient, mu, sigma, tics, portfolio, df_current, df_next)\n",
    "    \n",
    "        portfolio = portfolio_value\n",
    "        portfolio = portfolio.T\n",
    "        portfolio.columns = ['account_value']\n",
    "        portfolio = portfolio.reset_index()\n",
    "        portfolio.columns = ['date', 'account_value']\n",
    "\n",
    "        '''Backtest hasn't been implemented yet, hence commented.'''\n",
    "        #stats = backtest_stats(portfolio, value_col_name='account_value')\n",
    "        \n",
    "        portfolio_cumprod = (\n",
    "            portfolio.account_value.pct_change()+1).cumprod()-1\n",
    "\n",
    "        return portfolio, portfolio_cumprod, pd.DataFrame(meta_coefficient)\n",
    "\n",
    "   def _return_predict(self, unique_trade_date, test_data, i, tech_indicator_list):\n",
    "\n",
    "        current_date = unique_trade_date[i]\n",
    "        next_date = unique_trade_date[i+1]\n",
    "\n",
    "        df_current = test_data[test_data.date ==\n",
    "                                  current_date].reset_index(drop=True)\n",
    "        df_next = test_data[test_data.date ==\n",
    "                               next_date].reset_index(drop=True)\n",
    "\n",
    "        tics = df_current['tic'].values\n",
    "        features = df_current[tech_indicator_list].values\n",
    "\n",
    "        predicted_y = self.model.predict(features)\n",
    "        mu = predicted_y\n",
    "        sigma = risk_models.sample_cov(\n",
    "            df_current.return_list[0], returns_data=True)\n",
    "\n",
    "        return mu, sigma, tics, df_current, df_next\n",
    "\n",
    "   def _weight_optimization(self, i, unique_trade_date, meta_coefficient, mu, sigma, tics, portfolio, df_current, df_next):\n",
    "\n",
    "        current_date = unique_trade_date[i]\n",
    "        predicted_y_df = pd.DataFrame(\n",
    "            {\"tic\": tics.reshape(-1,), \"predicted_y\": mu.reshape(-1,)})\n",
    "        min_weight, max_weight = 0, 1\n",
    "\n",
    "        ef = EfficientFrontier(mu, sigma)\n",
    "        weights = ef.nonconvex_objective(\n",
    "            objective_functions.sharpe_ratio,\n",
    "            objective_args=(ef.expected_returns, ef.cov_matrix),\n",
    "            weights_sum_to_one=True,\n",
    "            constraints=[\n",
    "                # greater than min_weight\n",
    "                {\"type\": \"ineq\", \"fun\": lambda w: w - min_weight},\n",
    "                # less than max_weight\n",
    "                {\"type\": \"ineq\", \"fun\": lambda w: max_weight - w},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        weight_df = {\"tic\": [], \"weight\": []}\n",
    "        meta_coefficient[\"date\"] += [current_date]\n",
    "\n",
    "        for item in weights:\n",
    "            weight_df['tic'] += [item]\n",
    "            weight_df['weight'] += [weights[item]]\n",
    "\n",
    "        weight_df = pd.DataFrame(weight_df).merge(predicted_y_df, on=['tic'])\n",
    "        meta_coefficient[\"weights\"] += [weight_df]\n",
    "        cap = portfolio.iloc[0, i]\n",
    "        # current cash invested for each stock\n",
    "        current_cash = [element * cap for element in list(weights.values())]\n",
    "        # current held shares\n",
    "        current_shares = list(np.array(current_cash) / np.array(df_current.close))\n",
    "        # next time period price\n",
    "        next_price = np.array(df_next.close)\n",
    "        portfolio.iloc[0, i+1] = np.dot(current_shares, next_price)\n",
    "\n",
    "        return portfolio \n",
    "\n",
    "   def save_model(self,  file_name):\n",
    "        with open(file_name, 'wb') as files:\n",
    "            pickle.dump(self.model, files)\n",
    "        print(\"Model saved succesfully.\")\n",
    "\n",
    "\n",
    "   def load_model(self, file_name):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            self.model = pickle.load(f)\n",
    "        print(\"Model loaded succesfully.\")\n",
    "        return self.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortfolioEnv(Environment):\n",
    "\n",
    "    def __init__(self,\n",
    "                 df: pd.DataFrame,  # input data\n",
    "                 stock_dim: int,  # number of unique securities in the investment universe\n",
    "                 hmax: float,  # maximum number of shares to trade\n",
    "                 initial_amount: float,  # initial cash value\n",
    "                 transaction_cost_pct: float,  # transaction cost percentage per trade\n",
    "                 reward_scaling: float,  # scaling factor for reward as training progresses\n",
    "                 state_space: int,  # the dimension of input features (state space)\n",
    "                 action_space: int,  # number of actions, which is equal to portfolio dimension\n",
    "                 tech_indicator_list: list,  # a list of technical indicator names\n",
    "                 turbulence_threshold=None,  # a threshold to control risk aversion\n",
    "                 lookback=252,  #\n",
    "                 day=0):  # an increment number to control date\n",
    "\n",
    "        self.df = df\n",
    "        self.day = day\n",
    "        self.lookback = lookback\n",
    "        self.stock_dim = stock_dim\n",
    "        self.hmax = hmax\n",
    "        self.initial_amount = initial_amount\n",
    "        self.transaction_cost_pct = transaction_cost_pct\n",
    "        self.reward_scaling = reward_scaling\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.tech_indicator_list = tech_indicator_list\n",
    "\n",
    "        # action_space normalization and shape is self.stock_dim\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(self.action_space,))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf,\n",
    "                                            shape=(self.state_space + len(self.tech_indicator_list), self.state_space))\n",
    "        \n",
    "\n",
    "        # load data from a pandas dataframe\n",
    "        \n",
    "        ##FINRL APPROACH\n",
    "        #self.df.set_index(\"date\", drop = True, inplace=True)\n",
    "        \n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        self.covs = self.data['cov_list'].values[0]\n",
    "        self.state =  np.append(np.array(self.covs), [self.data[tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)\n",
    "        self.terminal = False\n",
    "        #self.turbulence_threshold = turbulence_threshold\n",
    "        # initalize state: initial portfolio return + individual stock return + individual weights\n",
    "        self.portfolio_value = self.initial_amount\n",
    "\n",
    "        # memorize portfolio value each step\n",
    "        self.asset_memory = [self.initial_amount]\n",
    "        # memorize portfolio return each step\n",
    "        self.portfolio_return_memory = [0]\n",
    "        self.actions_memory = [[1 / self.stock_dim] * self.stock_dim]\n",
    "        self.date_memory=[self.data.date.unique()[0]]\n",
    "\n",
    "    def reset(self):\n",
    "        self.asset_memory = [self.initial_amount]\n",
    "        self.day = 0\n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        # load states\n",
    "        self.covs = self.data['cov_list'].values[0]\n",
    "        self.state =  np.append(np.array(self.covs), [self.data[tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)\n",
    "                \n",
    "        self.portfolio_value = self.initial_amount\n",
    "        #self.cost = 0\n",
    "        #self.trades = 0\n",
    "        self.terminal = False \n",
    "        self.portfolio_return_memory = [0]\n",
    "        self.actions_memory=[[1/self.stock_dim] * self.stock_dim]\n",
    "        self.date_memory=[self.data.date.unique()[0]] \n",
    "        return self.state\n",
    "\n",
    "    def step(self, actions):\n",
    "        self.terminal = self.day >= len(self.df.index.unique()) - 1\n",
    "        if self.terminal:\n",
    "            df = pd.DataFrame(self.portfolio_return_memory)\n",
    "            print(df)\n",
    "            df.columns = ['daily_return']\n",
    "            plt.plot(df.daily_return.cumsum(), 'r')\n",
    "           # plt.savefig('results/cumulative_reward.png')\n",
    "            plt.close()\n",
    "\n",
    "            plt.plot(self.portfolio_return_memory, 'r')\n",
    "           #plt.savefig('results/rewards.png')\n",
    "            plt.close()\n",
    "\n",
    "            print(\"=================================\")\n",
    "            print(\"begin_total_asset:{}\".format(self.asset_memory[0]))\n",
    "            print(\"end_total_asset:{}\".format(self.portfolio_value))\n",
    "\n",
    "            df_daily_return = pd.DataFrame(self.portfolio_return_memory)\n",
    "            df_daily_return.columns = ['daily_return']\n",
    "            if df_daily_return['daily_return'].std() != 0:\n",
    "                sharpe = (252 ** 0.5) * df_daily_return['daily_return'].mean() / \\\n",
    "                         df_daily_return['daily_return'].std()\n",
    "                print(\"Sharpe: \", sharpe)\n",
    "            print(\"=================================\")\n",
    "\n",
    "            return self.state, self.reward, self.terminal, {}\n",
    "\n",
    "        else:\n",
    "            weights = Environment.softmax_normalization(actions)\n",
    "            self.actions_memory.append(weights)\n",
    "            last_day_memory = self.data\n",
    "\n",
    "            # load next state\n",
    "            self.day += 1\n",
    "            self.data = self.df.loc[self.day, :]\n",
    "            self.covs = self.data['cov_list'].values[0]\n",
    "            self.state =  np.append(np.array(self.covs), [self.data[tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)         \n",
    "            portfolio_return = sum(((self.data.close.values / last_day_memory.close.values) - 1) * weights)\n",
    "            log_portfolio_return = np.log(sum((self.data.close.values / last_day_memory.close.values) * weights))\n",
    "            # update portfolio value\n",
    "            new_portfolio_value = self.portfolio_value * (1 + portfolio_return)\n",
    "            self.portfolio_value = new_portfolio_value\n",
    "\n",
    "            # save into memory\n",
    "            self.portfolio_return_memory.append(portfolio_return)\n",
    "            self.date_memory.append(self.data[\"date\"].unique()[0])\n",
    "            self.asset_memory.append(new_portfolio_value)\n",
    "\n",
    "            # the reward is the new portfolio value or end portfolo value\n",
    "            self.reward = new_portfolio_value\n",
    "\n",
    "        return self.state, self.reward, self.terminal, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        return self.state\n",
    "\n",
    "    def save_asset_memory(self):\n",
    "        date_list = self.date_memory\n",
    "        portfolio_return = self.portfolio_return_memory\n",
    "        # print(len(date_list))\n",
    "        # print(len(asset_list))\n",
    "        df_account_value = pd.DataFrame({'date': date_list, 'daily_return': portfolio_return})\n",
    "        return df_account_value\n",
    "\n",
    "    def save_action_memory(self):\n",
    "        # date and close price length must match actions length\n",
    "        date_list = self.date_memory\n",
    "        df_date = pd.DataFrame(date_list)\n",
    "        df_date.columns = ['date']\n",
    "\n",
    "        action_list = self.actions_memory\n",
    "        df_actions = pd.DataFrame(action_list)\n",
    "        df_actions.columns = self.data.tic.values\n",
    "        df_actions.index = df_date.date\n",
    "        # df_actions = pd.DataFrame({'date':date_list,'actions':action_list})\n",
    "        return df_actions\n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def get_env(self):\n",
    "        e = DummyVecEnv([lambda: self])\n",
    "        obs = e.reset()\n",
    "        return e, obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C(RLAgent):\n",
    "\n",
    "    def __init__(self,\n",
    "                 policy: \"MlpPolicy\",\n",
    "                 env: None,\n",
    "                 learning_rate: float = 7e-4,\n",
    "                 n_steps: int = 5,\n",
    "                 gamma: float = 0.99,\n",
    "                 gae_lambda: float = 1.0,\n",
    "                 ent_coef: float = 0.0,\n",
    "                 vf_coef: float = 0.5,\n",
    "                 max_grad_norm: float = 0.5,\n",
    "                 rms_prop_eps: float = 1e-5,\n",
    "                 use_rms_prop: bool = True,\n",
    "                 use_sde: bool = False,\n",
    "                 sde_sample_freq: int = -1,\n",
    "                 normalize_advantage: bool = False,\n",
    "                 tensorboard_log: Optional[str] = None,\n",
    "                 create_eval_env: bool = False,\n",
    "                 policy_kwargs: Optional[Dict[str, Any]] = None,\n",
    "                 verbose: int = 0,\n",
    "                 seed: Optional[int] = None,\n",
    "                 device: Union[th.device, str] = \"auto\",\n",
    "                 _init_setup_model: bool = True):\n",
    "\n",
    "        self.env = env\n",
    "\n",
    "        self.model = sb_A2C(policy = policy,\n",
    "                            env=self.env,\n",
    "                            learning_rate = learning_rate,\n",
    "                            n_steps = n_steps,\n",
    "                            gamma = gamma,\n",
    "                            gae_lambda= gae_lambda,\n",
    "                            ent_coef = ent_coef,\n",
    "                            vf_coef = vf_coef,\n",
    "                            max_grad_norm = max_grad_norm,\n",
    "                            rms_prop_eps= rms_prop_eps,\n",
    "                            use_rms_prop= use_rms_prop,\n",
    "                            use_sde= use_sde,\n",
    "                            sde_sample_freq= sde_sample_freq,\n",
    "                            normalize_advantage= normalize_advantage,\n",
    "                            tensorboard_log=tensorboard_log,  \n",
    "                            create_eval_env= create_eval_env,\n",
    "                            policy_kwargs=policy_kwargs,\n",
    "                            verbose=verbose,\n",
    "                            seed=seed,\n",
    "                            device= device,\n",
    "                            _init_setup_model = _init_setup_model)\n",
    "\n",
    "    def train_model(self, **train_params):\n",
    "        self.model = self.model.learn(**train_params)\n",
    "        return self.model\n",
    "\n",
    "    def predict(self, environment, **test_params):\n",
    "\n",
    "        env_test, obs_test = environment.get_env()\n",
    "        \"\"\"make a prediction\"\"\"\n",
    "        account_memory = []\n",
    "        actions_memory = []\n",
    "\n",
    "        env_test.reset()\n",
    "        for i in range(len(environment.df.index.unique())):\n",
    "            action, _states = self.model.predict(obs_test, **test_params)\n",
    "            obs_test, rewards, dones, info = env_test.step(action)\n",
    "            if i == (len(environment.df.index.unique()) - 2):\n",
    "                account_memory = env_test.env_method(method_name=\"save_asset_memory\")\n",
    "                actions_memory = env_test.env_method(method_name=\"save_action_memory\")\n",
    "            if dones[0]:\n",
    "                print(\"hit end!\")\n",
    "                break\n",
    "\n",
    "        return account_memory[0], actions_memory[0]\n",
    "\n",
    "    def save_model(self, path):\n",
    "        self.model.save(path)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        self.model = self.model.load(path)\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO(RLAgent):\n",
    "    def __init__(self,\n",
    "                policy: \"MlpPolicy\",\n",
    "                env: None,\n",
    "                learning_rate:  3e-4,\n",
    "                n_steps: int = 2048,\n",
    "                batch_size: int = 64,\n",
    "                n_epochs: int = 10,\n",
    "                gamma: float = 0.99,\n",
    "                gae_lambda: float = 0.95,\n",
    "                clip_range: Union[float, Schedule] = 0.2,\n",
    "                clip_range_vf: Union[None, float, Schedule] = None,\n",
    "                normalize_advantage: bool = True,\n",
    "                ent_coef: float = 0.0,\n",
    "                vf_coef: float = 0.5,\n",
    "                max_grad_norm: float = 0.5,\n",
    "                use_sde: bool = False,\n",
    "                sde_sample_freq: int = -1,\n",
    "                target_kl: Optional[float] = None,\n",
    "                tensorboard_log: Optional[str] = None,\n",
    "                create_eval_env: bool = False,\n",
    "                policy_kwargs: Optional[Dict[str, Any]] = None,\n",
    "                verbose: int = 0,\n",
    "                seed: Optional[int] = None,\n",
    "                device: Union[th.device, str] = \"auto\",\n",
    "                _init_setup_model: bool = True):\n",
    "\n",
    "        self.env = env\n",
    "        self.model = sb_PPO(policy = policy,\n",
    "                            env=self.env,\n",
    "                            learning_rate = learning_rate,\n",
    "                            n_steps = n_steps,\n",
    "                            gamma = gamma,\n",
    "                            batch_size = batch_size,\n",
    "                            n_epochs = n_epochs,\n",
    "                            gae_lambda=gae_lambda,\n",
    "                            clip_range = clip_range,\n",
    "                            clip_range_vf = clip_range_vf,\n",
    "                            normalize_advantage=normalize_advantage,\n",
    "                            ent_coef=ent_coef,\n",
    "                            vf_coef=vf_coef,\n",
    "                            max_grad_norm=max_grad_norm,\n",
    "                            use_sde=use_sde,\n",
    "                            sde_sample_freq=sde_sample_freq,\n",
    "                            target_kl=target_kl,\n",
    "                            tensorboard_log=tensorboard_log,\n",
    "                            create_eval_env=create_eval_env,\n",
    "                            policy_kwargs=policy_kwargs,\n",
    "                            verbose=verbose,\n",
    "                            seed=seed,\n",
    "                            device=device,\n",
    "                            _init_setup_model = _init_setup_model)\n",
    "    \n",
    "    def train_model(self, **train_params):\n",
    "        self.model = self.model.learn(**train_params)\n",
    "        return self.model\n",
    "    \n",
    "    def predict(self, environment, **test_params):\n",
    "        env_test, obs_test = environment.get_env()\n",
    "        \"\"\"make a prediction\"\"\"\n",
    "        account_memory = []\n",
    "        actions_memory = []\n",
    "\n",
    "        env_test.reset()\n",
    "        for i in range(len(environment.df.index.unique())):\n",
    "            action, _states = self.model.predict(obs_test, **test_params)\n",
    "            obs_test, rewards, dones, info = env_test.step(action)\n",
    "            if i == (len(environment.df.index.unique()) - 2):\n",
    "                account_memory = env_test.env_method(method_name=\"save_asset_memory\")\n",
    "                actions_memory = env_test.env_method(method_name=\"save_action_memory\")\n",
    "            if dones[0]:\n",
    "                print(\"hit end!\")\n",
    "                break\n",
    "\n",
    "        return account_memory[0], actions_memory[0]\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        self.model = self.model.load(path)\n",
    "        return self.model\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        self.model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG(RLAgent):\n",
    "    def __init__(self,\n",
    "                policy: \"MlpPolicy\",\n",
    "                env: None,\n",
    "                learning_rate : 1e-3,\n",
    "                buffer_size: 1_000_000,  # 1e6\n",
    "                learning_starts: 100,\n",
    "                batch_size:  100,\n",
    "                tau:  0.005,\n",
    "                gamma:  0.99,\n",
    "                train_freq:  1,\n",
    "                gradient_steps: int = -1,\n",
    "                action_noise: Optional[ActionNoise] = None,\n",
    "                replay_buffer_class: Optional[ReplayBuffer] = None,\n",
    "                replay_buffer_kwargs: Optional[Dict[str, Any]] = None,\n",
    "                optimize_memory_usage: bool = False,\n",
    "                tensorboard_log: Optional[str] = None,\n",
    "                create_eval_env: bool = False,\n",
    "                policy_kwargs: Optional[Dict[str, Any]] = None,\n",
    "                verbose: int = 0,\n",
    "                seed: Optional[int] = None,\n",
    "                device: Union[th.device, str] = \"auto\",\n",
    "                _init_setup_model: bool = True):\n",
    "                \n",
    "        self.env = env\n",
    "    \n",
    "        self.model = sb_DDPG(policy = policy,\n",
    "                            env=self.env,\n",
    "                            learning_rate = learning_rate,\n",
    "                            buffer_size = buffer_size,\n",
    "                            learning_starts= learning_starts,\n",
    "                            batch_size = batch_size,\n",
    "                            tau = tau,\n",
    "                            gamma= gamma,\n",
    "                            train_freq = train_freq,\n",
    "                            gradient_steps = gradient_steps,\n",
    "                            action_noise= action_noise,\n",
    "                            replay_buffer_class= replay_buffer_class,\n",
    "                            replay_buffer_kwargs= replay_buffer_kwargs,\n",
    "                            optimize_memory_usage=optimize_memory_usage,\n",
    "                            tensorboard_log=tensorboard_log,\n",
    "                            create_eval_env=create_eval_env,\n",
    "                            policy_kwargs=policy_kwargs,\n",
    "                            verbose=verbose,\n",
    "                            seed=seed,\n",
    "                            device=device,\n",
    "                            _init_setup_model = _init_setup_model)\n",
    "\n",
    "    def train_model(self, **train_params):\n",
    "        self.model = self.model.learn(**train_params)\n",
    "        return self.model\n",
    "\n",
    "    def predict(self, environment, **test_params):\n",
    "        env_test, obs_test = environment.get_env()\n",
    "        \"\"\"make a prediction\"\"\"\n",
    "        account_memory = []\n",
    "        actions_memory = []\n",
    "\n",
    "        env_test.reset()\n",
    "        for i in range(len(environment.df.index.unique())):\n",
    "            action, _states = self.model.predict(obs_test, **test_params)\n",
    "            obs_test, rewards, dones, info = env_test.step(action)\n",
    "            if i == (len(environment.df.index.unique()) - 2):\n",
    "                account_memory = env_test.env_method(method_name=\"save_asset_memory\")\n",
    "                actions_memory = env_test.env_method(method_name=\"save_action_memory\")\n",
    "            if dones[0]:\n",
    "                print(\"hit end!\")\n",
    "                break\n",
    "\n",
    "        return account_memory[0], actions_memory[0]\n",
    "        \n",
    "    def load_model(self, path):\n",
    "        self.model = self.model.load(path)\n",
    "        return self.model\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        self.model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3(RLAgent):\n",
    " \n",
    "    def __init__(self,\n",
    "                policy: \"MlpPolicy\",\n",
    "                env: None,\n",
    "                learning_rate: float =  1e-3,\n",
    "                buffer_size: int = 1_000_000,  # 1e6\n",
    "                learning_starts: int = 100,\n",
    "                batch_size: int = 100,\n",
    "                tau: float = 0.005,\n",
    "                gamma: float = 0.99,\n",
    "                train_freq: int = 1,\n",
    "                gradient_steps: int = -1,\n",
    "                action_noise: Optional[ActionNoise] = None,\n",
    "                replay_buffer_class: Optional[ReplayBuffer] = None,\n",
    "                replay_buffer_kwargs: Optional[Dict[str, Any]] = None,\n",
    "                optimize_memory_usage: bool = False,\n",
    "                tensorboard_log: Optional[str] = None,\n",
    "                create_eval_env: bool = False,\n",
    "                policy_kwargs: Optional[Dict[str, Any]] = None,\n",
    "                verbose: int = 0,\n",
    "                seed: Optional[int] = None,\n",
    "                device: Union[th.device, str] = \"auto\",\n",
    "                _init_setup_model: bool = True):\n",
    "                \n",
    "        self.env = env\n",
    "    \n",
    "        self.model = sb_TD3(policy = policy,\n",
    "                            env=self.env,\n",
    "                            learning_rate = learning_rate,\n",
    "                            buffer_size = buffer_size,\n",
    "                            learning_starts= learning_starts,\n",
    "                            batch_size = batch_size,\n",
    "                            tau = tau,\n",
    "                            gamma= gamma,\n",
    "                            train_freq = train_freq,\n",
    "                            gradient_steps = gradient_steps,\n",
    "                            action_noise= action_noise,\n",
    "                            replay_buffer_class= replay_buffer_class,\n",
    "                            replay_buffer_kwargs= replay_buffer_kwargs,\n",
    "                            optimize_memory_usage=optimize_memory_usage,\n",
    "                            tensorboard_log=tensorboard_log,\n",
    "                            create_eval_env=create_eval_env,\n",
    "                            policy_kwargs=policy_kwargs,\n",
    "                            verbose=verbose,\n",
    "                            seed=seed,\n",
    "                            device=device,\n",
    "                            _init_setup_model = _init_setup_model)\n",
    "\n",
    "    def train_model(self, **train_params):\n",
    "        self.model = self.model.learn(**train_params)\n",
    "        return self.model\n",
    "\n",
    "    def predict(self, environment, **test_params):\n",
    "        env_test, obs_test = environment.get_env()\n",
    "        \"\"\"make a prediction\"\"\"\n",
    "        account_memory = []\n",
    "        actions_memory = []\n",
    "\n",
    "        env_test.reset()\n",
    "        for i in range(len(environment.df.index.unique())):\n",
    "            action, _states = self.model.predict(obs_test, **test_params)\n",
    "            obs_test, rewards, dones, info = env_test.step(action)\n",
    "            if i == (len(environment.df.index.unique()) - 2):\n",
    "                account_memory = env_test.env_method(method_name=\"save_asset_memory\")\n",
    "                actions_memory = env_test.env_method(method_name=\"save_action_memory\")\n",
    "            if dones[0]:\n",
    "                print(\"hit end!\")\n",
    "                break\n",
    "\n",
    "        return account_memory[0], actions_memory[0]\n",
    "        \n",
    "    def load_model(self, path):\n",
    "        self.model = self.model.load(path)\n",
    "        return self.model\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        self.model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gather user parameters\n",
    "with open(\"../user_params.yaml\", \"r\") as stream:\n",
    "    try:\n",
    "        user_params = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = user_params[\"tickers\"]\n",
    "env_kwargs = user_params[\"env_params\"]\n",
    "train_params = user_params[\"train_params\"]\n",
    "policy_params = user_params[\"policy_params\"]\n",
    "test_params = user_params[\"test_params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 3: Downloading from Yahoo.........\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Shape of DataFrame:  (12924, 8)\n",
      "        date       open       high        low      close     volume   tic  day\n",
      "0 2008-12-31   3.070357   3.133571   3.047857   2.606278  607541200  AAPL    2\n",
      "1 2008-12-31  41.590000  43.049999  41.500000  32.005875    5443100    BA    2\n",
      "2 2008-12-31  43.700001  45.099998  43.700001  30.628834    6277400   CAT    2\n",
      "3 2008-12-31  72.900002  74.629997  72.900002  43.314430    9964300   CVX    2\n",
      "4 2009-01-02   3.067143   3.251429   3.041429   2.771173  746015200  AAPL    4\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTest 3: Downloading from Yahoo.........\")\n",
    "downloaded_df = DataDownloader(start_date='2009-01-01',\n",
    "                                end_date='2021-10-31',\n",
    "                                ticker_list= tickers).download_from_yahoo()\n",
    "print(downloaded_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 4: Feature engineer.........\n",
      "\n",
      "Test 4: Feature engineer.........\n",
      "Successfully added technical indicators\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Shape of DataFrame:  (3231, 8)\n",
      "Successfully added vix\n",
      "Successfully added turbulence index\n",
      "Successfully added covariances\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTest 4: Feature engineer.........\")\n",
    " # PREPROCESS DATA\n",
    "print(\"\\nTest 4: Feature engineer.........\")\n",
    "data_processor = DefaultFeatureEngineer(use_default=False,\n",
    "                                        tech_indicator_list=env_kwargs[\"tech_indicator_list\"],\n",
    "                                        use_vix=True,\n",
    "                                        use_turbulence=True,\n",
    "                                        use_covar=True)\n",
    "# included technical indicators as features\n",
    "df_processed = data_processor.extend_data(downloaded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data to train and test\n",
    "splitter = TimeSeriesSplitter()\n",
    "train = splitter.get_split_data(df_processed, '2009-01-01', '2020-06-30')\n",
    "trade = splitter.get_split_data(df_processed, '2020-07-01', '2021-09-02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tic</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>vix</th>\n",
       "      <th>turbulence</th>\n",
       "      <th>cov_list</th>\n",
       "      <th>return_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-12-31</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>7.611786</td>\n",
       "      <td>7.619643</td>\n",
       "      <td>7.520000</td>\n",
       "      <td>6.434926</td>\n",
       "      <td>352410800.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.105229</td>\n",
       "      <td>6.531585</td>\n",
       "      <td>5.633487</td>\n",
       "      <td>60.410603</td>\n",
       "      <td>155.827601</td>\n",
       "      <td>31.312031</td>\n",
       "      <td>6.105642</td>\n",
       "      <td>6.048183</td>\n",
       "      <td>21.680000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[[0.0004566276633955281, 0.0002589254805581765...</td>\n",
       "      <td>tic             AAPL        BA       CAT      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-12-31</td>\n",
       "      <td>BA</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>55.220001</td>\n",
       "      <td>54.049999</td>\n",
       "      <td>42.180107</td>\n",
       "      <td>2189400.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.448061</td>\n",
       "      <td>43.940989</td>\n",
       "      <td>41.779000</td>\n",
       "      <td>54.118669</td>\n",
       "      <td>17.483202</td>\n",
       "      <td>4.580979</td>\n",
       "      <td>42.211020</td>\n",
       "      <td>40.690112</td>\n",
       "      <td>21.680000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[[0.0004566276633955281, 0.0002589254805581765...</td>\n",
       "      <td>tic             AAPL        BA       CAT      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-12-31</td>\n",
       "      <td>CAT</td>\n",
       "      <td>57.599998</td>\n",
       "      <td>57.959999</td>\n",
       "      <td>56.990002</td>\n",
       "      <td>40.802944</td>\n",
       "      <td>3859700.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.043858</td>\n",
       "      <td>42.300238</td>\n",
       "      <td>40.350230</td>\n",
       "      <td>51.872620</td>\n",
       "      <td>-74.366621</td>\n",
       "      <td>6.791854</td>\n",
       "      <td>41.526064</td>\n",
       "      <td>41.025309</td>\n",
       "      <td>21.680000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[[0.0004566276633955281, 0.0002589254805581765...</td>\n",
       "      <td>tic             AAPL        BA       CAT      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-12-31</td>\n",
       "      <td>CVX</td>\n",
       "      <td>77.720001</td>\n",
       "      <td>77.779999</td>\n",
       "      <td>76.930000</td>\n",
       "      <td>46.806007</td>\n",
       "      <td>4246600.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.009374</td>\n",
       "      <td>47.587707</td>\n",
       "      <td>46.552030</td>\n",
       "      <td>52.439821</td>\n",
       "      <td>-69.789981</td>\n",
       "      <td>6.763381</td>\n",
       "      <td>47.276572</td>\n",
       "      <td>46.664711</td>\n",
       "      <td>21.680000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[[0.0004566276633955281, 0.0002589254805581765...</td>\n",
       "      <td>tic             AAPL        BA       CAT      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>7.622500</td>\n",
       "      <td>7.660714</td>\n",
       "      <td>7.585000</td>\n",
       "      <td>6.535085</td>\n",
       "      <td>493729600.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.119897</td>\n",
       "      <td>6.599508</td>\n",
       "      <td>5.619095</td>\n",
       "      <td>62.133748</td>\n",
       "      <td>168.776894</td>\n",
       "      <td>33.760635</td>\n",
       "      <td>6.113836</td>\n",
       "      <td>6.060275</td>\n",
       "      <td>20.040001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[[0.0004430606263241792, 0.0002466281001534800...</td>\n",
       "      <td>tic             AAPL        BA       CAT      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2639</th>\n",
       "      <td>2020-06-26</td>\n",
       "      <td>CVX</td>\n",
       "      <td>88.779999</td>\n",
       "      <td>88.830002</td>\n",
       "      <td>86.180000</td>\n",
       "      <td>78.338341</td>\n",
       "      <td>13766000.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.752703</td>\n",
       "      <td>93.112452</td>\n",
       "      <td>76.463880</td>\n",
       "      <td>46.927254</td>\n",
       "      <td>-141.407484</td>\n",
       "      <td>18.869071</td>\n",
       "      <td>84.164341</td>\n",
       "      <td>80.878847</td>\n",
       "      <td>34.730000</td>\n",
       "      <td>1.877620</td>\n",
       "      <td>[[0.0006521345919568532, 0.0006959979141852898...</td>\n",
       "      <td>tic             AAPL        BA       CAT      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640</th>\n",
       "      <td>2020-06-29</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>88.312500</td>\n",
       "      <td>90.542503</td>\n",
       "      <td>87.820000</td>\n",
       "      <td>89.329300</td>\n",
       "      <td>130646000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.088531</td>\n",
       "      <td>92.294060</td>\n",
       "      <td>78.598776</td>\n",
       "      <td>62.399312</td>\n",
       "      <td>92.968114</td>\n",
       "      <td>23.584901</td>\n",
       "      <td>83.104047</td>\n",
       "      <td>76.785888</td>\n",
       "      <td>31.780001</td>\n",
       "      <td>12.311246</td>\n",
       "      <td>[[0.000653226497504472, 0.0007078381745854392,...</td>\n",
       "      <td>tic             AAPL        BA       CAT      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640</th>\n",
       "      <td>2020-06-29</td>\n",
       "      <td>BA</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>194.500000</td>\n",
       "      <td>176.270004</td>\n",
       "      <td>194.490005</td>\n",
       "      <td>78499900.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.740439</td>\n",
       "      <td>223.696927</td>\n",
       "      <td>154.242075</td>\n",
       "      <td>53.814440</td>\n",
       "      <td>45.363542</td>\n",
       "      <td>14.268369</td>\n",
       "      <td>173.214335</td>\n",
       "      <td>154.108667</td>\n",
       "      <td>31.780001</td>\n",
       "      <td>12.311246</td>\n",
       "      <td>[[0.000653226497504472, 0.0007078381745854392,...</td>\n",
       "      <td>tic             AAPL        BA       CAT      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640</th>\n",
       "      <td>2020-06-29</td>\n",
       "      <td>CAT</td>\n",
       "      <td>123.720001</td>\n",
       "      <td>126.040001</td>\n",
       "      <td>123.279999</td>\n",
       "      <td>119.876373</td>\n",
       "      <td>2798700.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.323069</td>\n",
       "      <td>130.677985</td>\n",
       "      <td>113.093641</td>\n",
       "      <td>52.455044</td>\n",
       "      <td>13.227455</td>\n",
       "      <td>5.712842</td>\n",
       "      <td>118.600580</td>\n",
       "      <td>113.334042</td>\n",
       "      <td>31.780001</td>\n",
       "      <td>12.311246</td>\n",
       "      <td>[[0.000653226497504472, 0.0007078381745854392,...</td>\n",
       "      <td>tic             AAPL        BA       CAT      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640</th>\n",
       "      <td>2020-06-29</td>\n",
       "      <td>CVX</td>\n",
       "      <td>86.910004</td>\n",
       "      <td>88.570000</td>\n",
       "      <td>86.599998</td>\n",
       "      <td>79.434692</td>\n",
       "      <td>7022200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.920217</td>\n",
       "      <td>93.216085</td>\n",
       "      <td>75.896343</td>\n",
       "      <td>47.804784</td>\n",
       "      <td>-127.197543</td>\n",
       "      <td>18.869071</td>\n",
       "      <td>84.158301</td>\n",
       "      <td>81.069896</td>\n",
       "      <td>31.780001</td>\n",
       "      <td>12.311246</td>\n",
       "      <td>[[0.000653226497504472, 0.0007078381745854392,...</td>\n",
       "      <td>tic             AAPL        BA       CAT      ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10564 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date   tic        open        high         low       close  \\\n",
       "0    2009-12-31  AAPL    7.611786    7.619643    7.520000    6.434926   \n",
       "0    2009-12-31    BA   55.000000   55.220001   54.049999   42.180107   \n",
       "0    2009-12-31   CAT   57.599998   57.959999   56.990002   40.802944   \n",
       "0    2009-12-31   CVX   77.720001   77.779999   76.930000   46.806007   \n",
       "1    2010-01-04  AAPL    7.622500    7.660714    7.585000    6.535085   \n",
       "...         ...   ...         ...         ...         ...         ...   \n",
       "2639 2020-06-26   CVX   88.779999   88.830002   86.180000   78.338341   \n",
       "2640 2020-06-29  AAPL   88.312500   90.542503   87.820000   89.329300   \n",
       "2640 2020-06-29    BA  181.000000  194.500000  176.270004  194.490005   \n",
       "2640 2020-06-29   CAT  123.720001  126.040001  123.279999  119.876373   \n",
       "2640 2020-06-29   CVX   86.910004   88.570000   86.599998   79.434692   \n",
       "\n",
       "           volume  day      macd     boll_ub     boll_lb     rsi_30  \\\n",
       "0     352410800.0  3.0  0.105229    6.531585    5.633487  60.410603   \n",
       "0       2189400.0  3.0  0.448061   43.940989   41.779000  54.118669   \n",
       "0       3859700.0  3.0  0.043858   42.300238   40.350230  51.872620   \n",
       "0       4246600.0  3.0  0.009374   47.587707   46.552030  52.439821   \n",
       "1     493729600.0  0.0  0.119897    6.599508    5.619095  62.133748   \n",
       "...           ...  ...       ...         ...         ...        ...   \n",
       "2639   13766000.0  4.0 -0.752703   93.112452   76.463880  46.927254   \n",
       "2640  130646000.0  0.0  3.088531   92.294060   78.598776  62.399312   \n",
       "2640   78499900.0  0.0  6.740439  223.696927  154.242075  53.814440   \n",
       "2640    2798700.0  0.0  1.323069  130.677985  113.093641  52.455044   \n",
       "2640    7022200.0  0.0 -0.920217   93.216085   75.896343  47.804784   \n",
       "\n",
       "          cci_30      dx_30  close_30_sma  close_60_sma        vix  \\\n",
       "0     155.827601  31.312031      6.105642      6.048183  21.680000   \n",
       "0      17.483202   4.580979     42.211020     40.690112  21.680000   \n",
       "0     -74.366621   6.791854     41.526064     41.025309  21.680000   \n",
       "0     -69.789981   6.763381     47.276572     46.664711  21.680000   \n",
       "1     168.776894  33.760635      6.113836      6.060275  20.040001   \n",
       "...          ...        ...           ...           ...        ...   \n",
       "2639 -141.407484  18.869071     84.164341     80.878847  34.730000   \n",
       "2640   92.968114  23.584901     83.104047     76.785888  31.780001   \n",
       "2640   45.363542  14.268369    173.214335    154.108667  31.780001   \n",
       "2640   13.227455   5.712842    118.600580    113.334042  31.780001   \n",
       "2640 -127.197543  18.869071     84.158301     81.069896  31.780001   \n",
       "\n",
       "      turbulence                                           cov_list  \\\n",
       "0       0.000000  [[0.0004566276633955281, 0.0002589254805581765...   \n",
       "0       0.000000  [[0.0004566276633955281, 0.0002589254805581765...   \n",
       "0       0.000000  [[0.0004566276633955281, 0.0002589254805581765...   \n",
       "0       0.000000  [[0.0004566276633955281, 0.0002589254805581765...   \n",
       "1       0.000000  [[0.0004430606263241792, 0.0002466281001534800...   \n",
       "...          ...                                                ...   \n",
       "2639    1.877620  [[0.0006521345919568532, 0.0006959979141852898...   \n",
       "2640   12.311246  [[0.000653226497504472, 0.0007078381745854392,...   \n",
       "2640   12.311246  [[0.000653226497504472, 0.0007078381745854392,...   \n",
       "2640   12.311246  [[0.000653226497504472, 0.0007078381745854392,...   \n",
       "2640   12.311246  [[0.000653226497504472, 0.0007078381745854392,...   \n",
       "\n",
       "                                            return_list  \n",
       "0     tic             AAPL        BA       CAT      ...  \n",
       "0     tic             AAPL        BA       CAT      ...  \n",
       "0     tic             AAPL        BA       CAT      ...  \n",
       "0     tic             AAPL        BA       CAT      ...  \n",
       "1     tic             AAPL        BA       CAT      ...  \n",
       "...                                                 ...  \n",
       "2639  tic             AAPL        BA       CAT      ...  \n",
       "2640  tic             AAPL        BA       CAT      ...  \n",
       "2640  tic             AAPL        BA       CAT      ...  \n",
       "2640  tic             AAPL        BA       CAT      ...  \n",
       "2640  tic             AAPL        BA       CAT      ...  \n",
       "\n",
       "[10564 rows x 20 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = data_processor.prepare_ml_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01556493],\n",
       "       [0.03787167],\n",
       "       [0.02737315],\n",
       "       ...,\n",
       "       [0.1439916 ],\n",
       "       [0.02336786],\n",
       "       [0.01399508]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegressionAgent(**policy_params[\"LR_PARAMS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained succesfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.train_model(train_x, train_y, **train_params[\"LR_PARAMS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio, portfolio_cumprod, meta_coefficient = lr.predict(trade, **test_params[\"LR_PARAMS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved succesfully.\n"
     ]
    }
   ],
   "source": [
    "lr.save_model(\"../AgentLayer/ConventionalAgents/lr_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded succesfully.\n"
     ]
    }
   ],
   "source": [
    "lr_loaded = lr.load_model(\"../AgentLayer/ConventionalAgents/lr_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression()\n"
     ]
    }
   ],
   "source": [
    "print(lr_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1ee38ef4a5a9feb55287fd749643f13d043cb0a7addaab2a9c224cbe137c0062"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
