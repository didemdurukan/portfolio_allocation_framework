{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/doganparlak/Desktop/Master_2.2/Master_Project/uniFi_github/uniFi/AgentLayer', '/usr/local/Cellar/python@3.8/3.8.8_1/Frameworks/Python.framework/Versions/3.8/lib/python38.zip', '/usr/local/Cellar/python@3.8/3.8.8_1/Frameworks/Python.framework/Versions/3.8/lib/python3.8', '/usr/local/Cellar/python@3.8/3.8.8_1/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload', '', '/usr/local/lib/python3.8/site-packages', '/usr/local/lib/python3.8/site-packages/selenium-3.141.0-py3.8.egg', '/usr/local/lib/python3.8/site-packages/urllib3-1.26.4-py3.8.egg', '/Users/doganparlak/Desktop/Master_2.2/Master_Project/uniFi_github/uniFi', '/Users/doganparlak/Desktop/Master_2.2/Master_Project/uniFi_github/uniFi', '/Users/doganparlak/Desktop/Master_2.2/Master_Project/uniFi_github/uniFi']\n"
     ]
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import A2C as sb_A2C\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import gym\n",
    "from datetime import datetime\n",
    "from sys import path\n",
    "from os.path import dirname as dir\n",
    "\n",
    "path.append(dir(path[0]))\n",
    "print(path)\n",
    "#__package__ = \"examples\"\n",
    "\n",
    "\"\"\"\n",
    "from FinancialEnvLayer.datacollector import CustomDatasetImporter\n",
    "from FinancialEnvLayer.datacollector import DataDownloader\n",
    "from FinancialEnvLayer.dataprocessor import FeatureEngineer\n",
    "\"\"\"\n",
    "\n",
    "from FinancialDataLayer.DataCollection.DataDownloader import DataDownloader\n",
    "from FinancialDataLayer.DataProcessing.DefaultFeatureEngineer import DefaultFeatureEngineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(gym.Env, ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def step(self, action):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def render(self, mode=\"human\"):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_env(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax_normalization(actions):\n",
    "        numerator = np.exp(actions)\n",
    "        denominator = np.sum(np.exp(actions))\n",
    "        softmax_output = numerator / denominator\n",
    "        return softmax_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_model():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save_model():\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def load_model():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConventionalAgent(Agent, ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_model():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save_model():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_model():\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _return_predict():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _weight_optimization():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLAgent(Agent, ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_model():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict():\n",
    "        \n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save_model():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_model():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortfolioEnv(Environment):\n",
    "\n",
    "    def __init__(self,\n",
    "                 df: pd.DataFrame,  # input data\n",
    "                 stock_dim: int,  # number of unique securities in the investment universe\n",
    "                 hmax: float,  # maximum number of shares to trade\n",
    "                 initial_amount: float,  # initial cash value\n",
    "                 transaction_cost_pct: float,  # transaction cost percentage per trade\n",
    "                 reward_scaling: float,  # scaling factor for reward as training progresses\n",
    "                 state_space: int,  # the dimension of input features (state space)\n",
    "                 action_space: int,  # number of actions, which is equal to portfolio dimension\n",
    "                 tech_indicator_list: list,  # a list of technical indicator names\n",
    "                 turbulence_threshold=None,  # a threshold to control risk aversion\n",
    "                 lookback=252,  #\n",
    "                 day=0):  # an increment number to control date\n",
    "\n",
    "        self.df = df\n",
    "        self.day = day\n",
    "        self.lookback = lookback\n",
    "        self.stock_dim = stock_dim\n",
    "        self.hmax = hmax\n",
    "        self.initial_amount = initial_amount\n",
    "        self.transaction_cost_pct = transaction_cost_pct\n",
    "        self.reward_scaling = reward_scaling\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.tech_indicator_list = tech_indicator_list\n",
    "\n",
    "        # action_space normalization and shape is self.stock_dim\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(self.action_space,))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf,\n",
    "                                            shape=(self.state_space + len(self.tech_indicator_list), self.state_space))\n",
    "        \n",
    "\n",
    "        # load data from a pandas dataframe\n",
    "        \n",
    "        ##FINRL APPROACH\n",
    "        #self.df.set_index(\"date\", drop = True, inplace=True)\n",
    "        \n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        self.covs = self.data['cov_list'].values[0]\n",
    "        self.state =  np.append(np.array(self.covs), [self.data[tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)\n",
    "        self.terminal = False\n",
    "        #self.turbulence_threshold = turbulence_threshold\n",
    "        # initalize state: initial portfolio return + individual stock return + individual weights\n",
    "        self.portfolio_value = self.initial_amount\n",
    "\n",
    "        # memorize portfolio value each step\n",
    "        self.asset_memory = [self.initial_amount]\n",
    "        # memorize portfolio return each step\n",
    "        self.portfolio_return_memory = [0]\n",
    "        self.actions_memory = [[1 / self.stock_dim] * self.stock_dim]\n",
    "        self.date_memory = [self.data[\"date\"]]\n",
    "\n",
    "    def reset(self):\n",
    "        print(\"----RESET---\")\n",
    "        self.asset_memory = [self.initial_amount]\n",
    "        self.day = 0\n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        # load states\n",
    "        self.covs = self.data['cov_list'].values[0]\n",
    "        self.state =  np.append(np.array(self.covs), [self.data[tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)\n",
    "                \n",
    "        self.portfolio_value = self.initial_amount\n",
    "        #self.cost = 0\n",
    "        #self.trades = 0\n",
    "        self.terminal = False \n",
    "        self.portfolio_return_memory = [0]\n",
    "        self.actions_memory=[[1/self.stock_dim] * self.stock_dim]\n",
    "        self.date_memory=[self.data[\"date\"]] \n",
    "        return self.state\n",
    "\n",
    "    def step(self, actions):\n",
    "        print(\"----STEP---\")\n",
    "        self.terminal = self.day >= len(self.df.index.unique()) - 1\n",
    "        if self.terminal:\n",
    "            df = pd.DataFrame(self.portfolio_return_memory)\n",
    "            df.columns = ['daily_return']\n",
    "            plt.plot(df.daily_return.cumsum(), 'r')\n",
    "            plt.savefig('results/cumulative_reward.png')\n",
    "            plt.close()\n",
    "\n",
    "            plt.plot(self.portfolio_return_memory, 'r')\n",
    "            plt.savefig('results/rewards.png')\n",
    "            plt.close()\n",
    "\n",
    "            print(\"=================================\")\n",
    "            print(\"begin_total_asset:{}\".format(self.asset_memory[0]))\n",
    "            print(\"end_total_asset:{}\".format(self.portfolio_value))\n",
    "\n",
    "            df_daily_return = pd.DataFrame(self.portfolio_return_memory)\n",
    "            df_daily_return.columns = ['daily_return']\n",
    "            if df_daily_return['daily_return'].std() != 0:\n",
    "                sharpe = (252 ** 0.5) * df_daily_return['daily_return'].mean() / \\\n",
    "                         df_daily_return['daily_return'].std()\n",
    "                print(\"Sharpe: \", sharpe)\n",
    "            print(\"=================================\")\n",
    "\n",
    "            return self.state, self.reward, self.terminal, {}\n",
    "\n",
    "        else:\n",
    "            weights = Environment.softmax_normalization(actions)\n",
    "            self.actions_memory.append(weights)\n",
    "            last_day_memory = self.data\n",
    "\n",
    "            # load next state\n",
    "            self.day += 1\n",
    "            self.data = self.df.loc[self.day, :]\n",
    "            self.covs = self.data['cov_list'].values[0]\n",
    "            self.state =  np.append(np.array(self.covs), [self.data[tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)         \n",
    "            portfolio_return = sum(((self.data.close.values / last_day_memory.close.values) - 1) * weights)\n",
    "            log_portfolio_return = np.log(sum((self.data.close.values / last_day_memory.close.values) * weights))\n",
    "            # update portfolio value\n",
    "            new_portfolio_value = self.portfolio_value * (1 + portfolio_return)\n",
    "            self.portfolio_value = new_portfolio_value\n",
    "\n",
    "            # save into memory\n",
    "            self.portfolio_return_memory.append(portfolio_return)\n",
    "            self.date_memory.append(self.data[\"date\"].unique()[0])\n",
    "            self.asset_memory.append(new_portfolio_value)\n",
    "\n",
    "            # the reward is the new portfolio value or end portfolo value\n",
    "            self.reward = new_portfolio_value\n",
    "\n",
    "        return self.state, self.reward, self.terminal, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(\"---RENDER---\")\n",
    "        return self.state\n",
    "\n",
    "    def save_asset_memory(self):\n",
    "        print(\"---ASSET_MEMORY---\")\n",
    "        date_list = self.date_memory\n",
    "        portfolio_return = self.portfolio_return_memory\n",
    "        # print(len(date_list))\n",
    "        # print(len(asset_list))\n",
    "        df_account_value = pd.DataFrame({'date': date_list, 'daily_return': portfolio_return})\n",
    "        return df_account_value\n",
    "\n",
    "    def save_action_memory(self):\n",
    "        print(\"---ACTION_MEMORY---\")\n",
    "        # date and close price length must match actions length\n",
    "        date_list = self.date_memory\n",
    "        df_date = pd.DataFrame(date_list)\n",
    "        df_date.columns = ['date']\n",
    "\n",
    "        action_list = self.actions_memory\n",
    "        df_actions = pd.DataFrame(action_list)\n",
    "        df_actions.columns = self.data.tic.values\n",
    "        df_actions.index = df_date.date\n",
    "        # df_actions = pd.DataFrame({'date':date_list,'actions':action_list})\n",
    "        return df_actions\n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def get_env(self):\n",
    "        e = DummyVecEnv([lambda: self])\n",
    "        obs = e.reset()\n",
    "        return e, obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C(RLAgent):\n",
    "\n",
    "    def __init__(self,\n",
    "                 policy= \"MlpPolicy\",\n",
    "                 env= None,\n",
    "                 learning_rate: float = 7e-4,\n",
    "                 n_steps: int = 5,\n",
    "                 gamma: float = 0.99,\n",
    "                 gae_lambda: float = 1.0,\n",
    "                 ent_coef: float = 0.0,\n",
    "                 vf_coef: float = 0.5,\n",
    "                 max_grad_norm: float = 0.5,\n",
    "                 rms_prop_eps: float = 1e-5,\n",
    "                 use_rms_prop: bool = True,\n",
    "                 use_sde: bool = False,\n",
    "                 sde_sample_freq: int = -1,\n",
    "                 normalize_advantage: bool = False,\n",
    "                 tensorboard_log=None,\n",
    "                 create_eval_env: bool = False,\n",
    "                 policy_kwargs=None,\n",
    "                 verbose: int = 0,\n",
    "                 seed=None,\n",
    "                 device=\"auto\",\n",
    "                 _init_setup_model: bool = True):\n",
    "\n",
    "        self.env = env\n",
    "        # self.model = A2C(model_params[\"policy\"], model_params[\"environment\"], model_params[\"verbose\"])\n",
    "        self.model = sb_A2C(policy = policy,\n",
    "                            env=self.env,\n",
    "                            learning_rate = learning_rate,\n",
    "                            n_steps = n_steps,\n",
    "                            gamma = gamma,\n",
    "                            gae_lambda= gae_lambda,\n",
    "                            ent_coef = ent_coef,\n",
    "                            vf_coef = vf_coef,\n",
    "                            max_grad_norm = max_grad_norm,\n",
    "                            rms_prop_eps= rms_prop_eps,\n",
    "                            use_rms_prop= use_rms_prop,\n",
    "                            use_sde= use_sde,\n",
    "                            sde_sample_freq= sde_sample_freq,\n",
    "                            normalize_advantage= normalize_advantage,\n",
    "                            tensorboard_log=tensorboard_log,  \n",
    "                            create_eval_env= create_eval_env,\n",
    "                            policy_kwargs=policy_kwargs,\n",
    "                            verbose=verbose,\n",
    "                            seed=seed,\n",
    "                            device= device,\n",
    "                            _init_setup_model = _init_setup_model)\n",
    "\n",
    "    def train_model(self, **train_params):\n",
    "        self.model = self.model.learn(**train_params)\n",
    "        return self.model\n",
    "\n",
    "    def predict(self, **test_params):\n",
    "\n",
    "        test_env, test_obs = test_params[\"environment\"].environment()\n",
    "        \"\"\"make a prediction\"\"\"\n",
    "        account_memory = []\n",
    "        actions_memory = []\n",
    "\n",
    "        test_env.reset()\n",
    "        for i in range(len(test_params[\"environment\"].df.index.unique())):\n",
    "            action, _states = self.model.predict(test_obs, deterministic=test_params[\"deterministic\"])\n",
    "            test_obs, rewards, dones, info = test_env.step(action)\n",
    "            if i == (len(test_params[\"environment\"].df.index.unique()) - 2):\n",
    "                account_memory = test_env.env_method(method_name=\"save_asset_memory\")\n",
    "                actions_memory = test_env.env_method(method_name=\"save_action_memory\")\n",
    "            if dones[0]:\n",
    "                print(\"hit end!\")\n",
    "                break\n",
    "\n",
    "        return account_memory[0], actions_memory[0]\n",
    "\n",
    "    def save_model(self, path):\n",
    "        self.model.save(path)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        self.model = self.model.load(path)\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gather user parameters\n",
    "with open(\"../user_params.yaml\", \"r\") as stream:\n",
    "    try:\n",
    "        user_params = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "tickers = user_params[\"tickers\"]\n",
    "env_kwargs = user_params[\"env_params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 3: Downloading from Yahoo.........\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Shape of DataFrame:  (12924, 8)\n",
      "        date       open       high        low      close     volume   tic  day\n",
      "0 2008-12-31   3.070357   3.133571   3.047857   2.606277  607541200  AAPL    2\n",
      "1 2008-12-31  41.590000  43.049999  41.500000  32.005894    5443100    BA    2\n",
      "2 2008-12-31  43.700001  45.099998  43.700001  30.628820    6277400   CAT    2\n",
      "3 2008-12-31  72.900002  74.629997  72.900002  43.670765    9964300   CVX    2\n",
      "4 2009-01-02   3.067143   3.251429   3.041429   2.771174  746015200  AAPL    4\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTest 3: Downloading from Yahoo.........\")\n",
    "downloaded_df = DataDownloader(start_date='2009-01-01',\n",
    "                                end_date='2021-10-31',\n",
    "                                ticker_list= tickers).download_from_yahoo()\n",
    "\"\"\"\n",
    "downloaded_df = DataDownloader.download_data(start_date='2009-01-01',\n",
    "                                                end_date='2021-10-31',\n",
    "                                             ticker_list=tickers)\n",
    "\"\"\"\n",
    "print(downloaded_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 4: Feature engineer.........\n",
      "Successfully added technical indicators\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Shape of DataFrame:  (3231, 8)\n",
      "Successfully added vix\n",
      "Successfully added turbulence index\n",
      "Successfully added covariances\n",
      "        date   tic       open       high        low      close       volume  \\\n",
      "0 2009-12-31  AAPL   7.611786   7.619643   7.520000   6.434926  352410800.0   \n",
      "0 2009-12-31    BA  55.000000  55.220001  54.049999  42.180115    2189400.0   \n",
      "0 2009-12-31   CAT  57.599998  57.959999  56.990002  40.802937    3859700.0   \n",
      "0 2009-12-31   CVX  77.720001  77.779999  76.930000  47.191086    4246600.0   \n",
      "1 2010-01-04  AAPL   7.622500   7.660714   7.585000   6.535085  493729600.0   \n",
      "\n",
      "   day      macd    boll_ub    boll_lb     rsi_30      cci_30      dx_30  \\\n",
      "0  3.0  0.105229   6.531585   5.633488  60.410545  155.827743  31.312031   \n",
      "0  3.0  0.448061  43.940988  41.779003  54.118696   17.483345   4.580979   \n",
      "0  3.0  0.043859  42.300240  40.350231  51.872588  -74.367093   6.791854   \n",
      "0  3.0  0.009454  47.979190  46.935004  52.439951  -69.819439   6.763381   \n",
      "1  0.0  0.119897   6.599508   5.619095  62.133676  168.776935  33.760635   \n",
      "\n",
      "   close_30_sma  close_60_sma        vix  turbulence  \\\n",
      "0      6.105642      6.048183  21.680000         0.0   \n",
      "0     42.211020     40.690111  21.680000         0.0   \n",
      "0     41.526065     41.025309  21.680000         0.0   \n",
      "0     47.665498     47.048606  21.680000         0.0   \n",
      "1      6.113836      6.060275  20.040001         0.0   \n",
      "\n",
      "                                            cov_list  \\\n",
      "0  [[0.00045662743203731134, 0.000258924822323675...   \n",
      "0  [[0.00045662743203731134, 0.000258924822323675...   \n",
      "0  [[0.00045662743203731134, 0.000258924822323675...   \n",
      "0  [[0.00045662743203731134, 0.000258924822323675...   \n",
      "1  [[0.0004430601192065793, 0.0002466274840833986...   \n",
      "\n",
      "                                         return_list  \n",
      "0  tic             AAPL        BA       CAT      ...  \n",
      "0  tic             AAPL        BA       CAT      ...  \n",
      "0  tic             AAPL        BA       CAT      ...  \n",
      "0  tic             AAPL        BA       CAT      ...  \n",
      "1  tic             AAPL        BA       CAT      ...  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTest 4: Feature engineer.........\")\n",
    "\n",
    "df_processed = DefaultFeatureEngineer( use_default= False,\n",
    "                                       tech_indicator_list= env_kwargs[\"tech_indicator_list\"],\n",
    "                                       use_vix=True,\n",
    "                                       use_turbulence=True,\n",
    "                                       use_covar=True).extend_data(downloaded_df)  # included technical indicators as features\n",
    "\n",
    "print(df_processed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PortfolioEnv(df=df_processed, **env_kwargs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----RESET---\n"
     ]
    }
   ],
   "source": [
    "env_train, _ = env.get_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([0. 0. 0. 0.], [1. 1. 1. 1.], (4,), float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_train.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = user_params[\"train_params\"]\n",
    "policy_params = user_params[\"policy_params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A2C_PARAMS': {'total_timesteps': 1000,\n",
       "  'callback': None,\n",
       "  'log_interval': 100,\n",
       "  'eval_env': None,\n",
       "  'eval_freq': -1,\n",
       "  'n_eval_episodes': 5,\n",
       "  'tb_log_name': 'A2C',\n",
       "  'eval_log_path': None,\n",
       "  'reset_num_timesteps': True}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#object creation\n",
    "a2c = A2C(env = env_train, **policy_params[\"A2C_PARAMS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----RESET---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n",
      "----STEP---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x1655bcfd0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training\n",
    "a2c.train_model(**train_params[\"A2C_PARAMS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting\n",
    "a2c.predict(test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving \n",
    "a2c.save_model(\"a2c_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading\n",
    "loaded_a2c_model = a2c.load_model(\"a2c_model\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1ee38ef4a5a9feb55287fd749643f13d043cb0a7addaab2a9c224cbe137c0062"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
